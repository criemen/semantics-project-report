% !TEX root = main.tex
\begin{abstract}
    \lipsum[1-2][1-20]
\end{abstract}
\tableofcontents

\newpage
\section{Introduction}
\subsection{Static Analysis}
\subsection{Description of CodeQL}
% Quick history Overview

\subsection{Architecture Outline}
% TODO check again
In this section, we will give a short overview of the CodeQL architecture, as well as where our
project fits in. We describe the extraction process where source code is turned into a relational database,
as well as the QL language and the libraries supporting the queries.

In order to analyze source code, it first needs to be processed into a database representation.
For each language an \emph{extractor} parses the source code and outputs a database file.
The extractor is language specific and usually hooks into a (possibly modified) compiler for that language.
The output of the extractor is a relational database in a language-specific \emph{database schema}.
This database schema evolves along with the language, as well as extractor capabilities.
For example, the Java database schema changed when preliminary support for the Java 13 yield statement was
added to the extractor.

The \emph{database engine} allows users to query the database file produced by the extractor.
Queries are written in the QL language.
Both the database engine and the QL language are agnostic to the language the analyzed source code 
is written in.
This allows developers at GitHub with relative easy to add support for new languages to the CodeQL ecosystem.
In theory, QL queries are also independent of the language the analyzed source code is written.
However, in practice queries are always tied intimately to the language of the analyzed source code,
as they depend on the language-specific database schema.
Furthermore, queries can't gloss over language-specific semantics either {---} one example of many would be
that signed integer overflow needs to be treated differently in Java than in C/C++.

For each language, a set of \emph{libraries} supports the development of queries.
These libraries are implemented in QL.
The libraries implement features such as taint tracking, dataflow analysis or control flow graphs.
For every supported language by the CodeQL analyzers, developers need to re-implement all of these libraries from
scratch, as components can't be shared across languages.
The only exception is the dataflow library, which is separated into a language-agnostic and 
language-specific part.
This language-agnostic part of the dataflow library is shared between the libraries for Java, C/C++ and C\#.

Using those libraries, developers at GitHub write \emph{queries} to detect bugs in the analyzed program.
For example, one query detects when variable dereference in Java might be null.
Another query detects if user input ends up in a database query, without the user input being 
sanitized properly for use in such a query.
Queries often rely on the supporting libraries to do the heavy lifting, e.g.\ the SQL injection detection 
is built on top of the dataflow library.


% TODO
The queries are accessible to researchers in academia and all open-source projects free of charge 
via the \href{https://LGTM.com}{LGTM.com} online platform and the CodeQL CLI utility . % TODO font for LGTM
Bugs can be reported via the issue tracker and the project takes pull requests.


All queries as well as their supporting libraries are open-source and published 
on GitHub\footnote{\url{https://github.com/Semmle/ql}}.
The extractors and the query engine are available only in compiled form under a 
very restrictive license\footnote{\url{https://github.com/github/codeql-cli-binaries}}.

Our project outside the course scope contributes improvements to the dataflow library,
both the shared and the language-specific part. We only implemented the language-specific 
functionality, and an GitHub employee provided stubs for C/C++ and C\#.
The individual features are discussed in the next chapters.

% Every language has its own extractor, which parses source code into the language-specif%ic database scheme.
% TODO? This database scheme evolves together with the language and extractor, 
%The queries and the supporting libraries are all specific to the language being analyzed,
%only the evaluator

\subsection{Philosophy}
Using static analysis for bug finding is lacking in the theoretical foundation - 
analyses are usually neither sound nor complete.
This means that a query can report \emph{false positives}, problems flagged by the query that 
are not bugs at all.
Furthermore, queries can also not report instances of a bug which are present in the code,
that instance is then called a \emph{false negative}.
Obviously both false positives and negatives are not desirable, but static analyzers go a long way
to reduce false positives as a means to reduce noise.
If false positives are reduced, almost every problem flagged by an analysis is a real bug.
This makes the static analyzer a very useful tool.
% TODO reword
% TODO general thing

\cite{qlpaper}
\subsection{Evaluation Model}
Every predicate is computed bottoms-up, so when a predicate is encountered, its
tuple set is computed.
Recursive predicates are computed as fixed-point iterations, which means that the database engine 
first adds the tuple sets arising from the base cases, and then from the recursive cases, until
no more tuples can be added. (TODO improve).
Mutual recursions are TODO.

Another useful feature of QL is that it includes syntactic sugar to compute the reflexive-transitive
and the transitive closure of predicates.
For example, a predicate \texttt{getSucc} on control flow nodes gives the set of all reachable
nodes by invoking \texttt{getSucc*}.

\subsection{QL And Relational Database TODO title}
At the heart of QL lives a relational database, and QL is designed to query a relational database.
Every predicate, the basic notion of a BLA, can be regarded as a tuple.

\newpage
\section{Description of Dataflow Library}
Designing a dataflow library which works in theory is a task that is not too difficult.
However, designing a dataflow library which performs well on projects with millions of lines of code is 
considerably more difficult.
In this section, we will describe the dataflow library for the CodeQL platform % TODO check marketing material, is this correct?
for analyzing programs in Java.
We will describe the version without our contributions which are outlined in the next sections.

As the dataflow library is used for bug finding, it is neither sound nor complete.
% TODO finish sentence
In general, the dataflow library reports \emph{possible} dataflow, i.e.\ as soon
as there is a path in the control flow graph TODO TODO flow is reported.
On the other hand, sometimes dataflow paths which do exist are not reported by the 
dataflow library because finding them would be prohibitive performance-wise.
These limitations will be explained in detail later. 

The dataflow library tracks only the flow of exact values.
For example, dataflow of a string is interrupted when \texttt{toUpperCase()} is called.
However, using functions like \texttt{toUpperCase()} does not prevent XSS or SQL injection attacks.
To find such paths that do not interrupt flow when the content of a variable still depends 
heavily on the input data, the \texttt{TaintTracking} library exists.
It is built onto the dataflow library and provides a generic interface to enable
finding of bug classes XSS, SQL injections or directory traversals.
While the taint tracking library is probably the biggest user of the dataflow library,
it is not related to the implementation of the dataflow library.
Thus, it is not discussed here further.
However, all improvements made to the dataflow library directly benefit taint tracking.

\subsection{Interface}
Before we delve into the internals of the dataflow library, we will describe the API 
of the dataflow library users will encounter. %, as well as general design principles.

\subsubsection*{Nodes}
The main concept used throughout the dataflow library is that of a \texttt{Node}.
Every element in the program holding data is a node.
The type \texttt{Node} has several subtypes, corresponding to different kinds of nodes.
The most important subclass of \texttt{Node} is \texttt{ExprNode}, 
as every expression in the program corresponds to a \texttt{Node}.
Another important subclass is \texttt{ParameterNode}.
Both the implicit \texttt{this} parameter for methods and all explicit parameters
given to methods are tracked via these nodes.
Other subtypes exist as well, but are less important to the bigger picture.
These subtypes correspond to instance creation, implicit reads of the \texttt{this} parameter,
the desugaring of vararg method calls and other technical elements that need to be 
represented in the dataflow graph.

One peculiarity of the \texttt{Node} class is that the dataflow graph
is stored in separate predicates, not on the nodes making up the graph itself.
This means that the graph is stored in an adjacency matrix, where the adjacency matrix is encoded in the tuple 
set of a predicate.
Different kinds of dataflow graphs can be represented by different predicates.
% TODO elaborate

\subsubsection*{Configuration}
The \texttt{Configuration} is the main class a user interacts with.
A configuration defines which nodes are sources and sinks of dataflow.
Furthermore, a configuration can also specify additional steps which yield flow.
In the example, a configuration could add that a call to 
\texttt{toUpperCase()} on a string still provides dataflow.
On the other hand, configurations can also specify nodes which block flow.
In the example, this could be sanitizer methods which for example strip HTML tags.
This enables the user to filter out paths which are known to be safe.

The most important predicate on the \texttt{Configuration} class 
is the \texttt{hasFlow(Node source, Node sink)} predicate.
With that predicate users can query the dataflow relation.
It holds whenever there is dataflow starting at the given node \texttt{source} 
(which has to be a source) which flows to the given node \texttt{sink} (which has to be a sink).
Note that this interface does \textbf{not} provide information about specific
dataflow paths.
It also limits dataflow queries to dataflow between the (previously defined) sources and sinks.
Is design limitation is intentionally restrictive, because it allows performance optimizations
that make it feasible to compute whole-program dataflow.

\subsection{General Design}

% jumpstep+localstep

In theory, implementing a dataflow algorithm in a logic programming language is very simple.
Given the \texttt{Node} concept discussed above, one only needs to provide a relation
\texttt{step(Node node1, Node node2)}.
This step relation holds if \texttt{node1} has dataflow in one step to \texttt{node2}
(local and global steps!).
Then, the transitive closure of \texttt{step} has all possible dataflow paths in the program.
These dataflow paths then can be filtered so that only paths which begin in a source and 
end in a sink are considered.

In practice however, this example wouldn't perform well at all.
First, note that the number of \texttt{Node}s grows roughly linear in the size of
the program.
Second, the transitive closure of the \texttt{step} relation enumerates all dataflow
paths in the program from one node to another.
It is defined on the Cartesian product of \texttt{Node} with itself, which is roughly quadratic 
in the size of the program.

\subsection{Description of the algorithm}
% configuration: source, sink, additional steps
% localflowstep+jumpstep

% simpleLocalFlowStep (Util): Follows SSA adjacent uses and def-first-use.
%   Data also flows from assignments to asignees (Check word??)
%   interestingly enough, also through casts

% start of considered nodes: nodeCandFwd1(node, config) -  simple local flow 
% node is reachable via dataflow from the source,
% including local flow, field stores, reads, flows into callables via parameters
% and flow out of callables via modified parameters (takes VD into account)
% doesn't take call contexts into account

% then nodeCand1 filters nodeCandFwd1 by just keeping all nodes which are
% reachable from a sink (through the same steps as before)

% nodeCand2: very simple call contexts, TODO find out exact differences to Cand1
% nodeCand:=nodeCand2 excluding call contexts

% bigstep relation: localFlowBigStep

% flowCandFwd0 something something accesspathfront, starts again with flooding
% from the source with all nodeCand2's
% flowCand0 filters flowCandFwd from the sink backwards to the source

% flowCandFwd = flowCandFwd0 AND CastingNode compatible type check

% flowCand = flowCand0 AND flowCandFwd
% flowCand0 uses flowCand and flowCandFwd, so mutual recursion, goes backwards from sink

% TODO: Sections about casts


% what is a node?
% user interface
\newpage
\section{Call Sensitivity}
\subsection{Motivation}
TODO: as discussed above, the dataflow library reports 

% TODO intro
Programmers write functions in their code which change their behavior based on 
a Boolean parameter.
Then on the call sites, sometimes a Boolean literal is used to determine the behavior
of the subroutine.
For example in the Apache Hadoop project, almost 10000 instances of this
pattern occurred\footnote{\url{https://lgtm.com/query/139379883091955/}}.
In pseudo-code, the pattern looks like this:
% TODO better
\begin{minted}{java}
        fun A(bool param) {
            if(param) {
                // do X
            }
            else {
                // do Y
            }
        }

        fun B() {
            A(false);
            A(true);
        }
\end{minted}
Previously, the dataflow library didn't take these into account.


\subsection{Description of Implementation}
The implementation consists of N (TODO) different parts.
First, there is the predicate \texttt{isUnreachableInCall(Node n, DataFlowCall call)} which
computes the set of all nodes in the dataflow graph which are unreachable
TODO more
Second, 
% TODO mention virtual dispatch
\subsection{Correctness}
The correctness of the implementation is based on the observation that constants can be
inlined.
% TODO more

\subsection{Examples of code where detection is now improved}
\subsection{Future improvements}
The analysis described in this pattern can be further generalized, as
this pattern applies to more than just functions that take a Boolean parameter.
Boolean types are generalized by enumerated types, providing (usually) more than two options.
In fact, using an enum-typed variable over a Boolean variable can be the better from a software engineering perspective
even when just encoding a choice of two options.
Choices such as left and right are better represented 
by enum constants \texttt{LEFT} and \texttt{RIGHT}, rather than the 
non-descriptive Boolean constants \texttt{true} and \texttt{false}.

Looking again at the Apache Hadoop project, we see almost 1800
function calls\footnote{\url{https://lgtm.com/query/1789970986252448931/}} with enum constants as parameter.
In all these instances the enum parameter influences control flow in the function.

However, adding support for enum constants was not in the scope of the project, so we didn't implement it.
Furthermore, our architecture is designed to be easily extended.
Thus adding support for enum constant would be a mere software engineering effort
with no significant academic interest.


\newpage
\section{Path Sensitivity}
\subsection{Motivation}
\subsection{Description of Implementation}
\subsection{Sketch of Correctness Proof}




%%% Local Variables:
%%% TeX-master: "main"
%%% TeX-engine: xetex
%%% End:
