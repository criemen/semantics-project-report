% !TEX root = main.tex
\begin{abstract}
    \lipsum[1-2][1-20]
\end{abstract}
\tableofcontents

\newpage
\section{Introduction}
\subsection{Static Analysis}
\subsection{Description of CodeQL}
% Quick history Overview

\subsection{Architecture Outline}
% TODO check again
% overview paragraph is not very good/precise
In this section, we will give a short overview of the CodeQL architecture, as well as where our
project fits in. We describe the extraction process where source code is turned into a relational database,
as well as the QL language and the libraries supporting the queries.

In order to analyze source code, it first needs to be processed into a database representation.
For each language an \emph{extractor} parses the source code and outputs a database file.
The extractor is language specific and usually hooks into a (possibly modified) compiler for that language.
The output of the extractor is a relational database in a language-specific \emph{database schema}.
This database schema evolves along with the language, as well as extractor capabilities.
For example, the Java database schema changed when preliminary support for the Java 13 yield statement was
added to the extractor.

The \emph{database engine} allows users to query the database file produced by the extractor.
Queries are written in the QL language.
Both the database engine and the QL language are agnostic to the language the analyzed source code 
is written in.
This allows developers at GitHub with relative easy to add support for new languages to the CodeQL ecosystem.
In theory, QL queries are also independent of the language the analyzed source code is written.
However, in practice queries are always tied intimately to the language of the analyzed source code,
as they depend on the language-specific database schema.
Furthermore, queries can't gloss over language-specific semantics either {---} one example of many would be
that signed integer overflow needs to be treated differently in Java than in C/C++.

For each language, a set of \emph{libraries} supports the development of queries.
These libraries are implemented in QL.
The libraries implement features such as taint tracking, dataflow analysis or control flow graphs.
For every supported language by the CodeQL analyzers, developers need to re-implement all of these libraries from
scratch, as components can't be shared across languages.
The only exception is the dataflow library, which is separated into a language-agnostic and 
language-specific part.
This language-agnostic part of the dataflow library is shared between the libraries for Java, C/C++ and C\#.

Using those libraries, developers at GitHub write \emph{queries} to detect bugs in the analyzed program.
For example, one query detects when variable dereference in Java might be null.
Another query detects if user input ends up in a database query, without the user input being 
sanitized properly for use in such a query.
Queries often rely on the supporting libraries to do the heavy lifting, e.g.\ the SQL injection detection 
is built on top of the dataflow library.


% TODO
The queries are accessible to researchers in academia and all open-source projects free of charge 
via the \href{https://LGTM.com}{LGTM.com} online platform and the CodeQL CLI utility. % TODO font for LGTM
Bugs can be reported via the issue tracker and the project takes pull requests.


All queries as well as the supporting libraries are open-source and published 
on GitHub\footnote{\url{https://github.com/Semmle/ql}}.
The extractors and the query engine are available only in compiled form under a 
proprietary license\footnote{\url{https://github.com/github/codeql-cli-binaries}}.
While the license is quite restrictive,
usage of the CodeQL CLI utility for academic research is explicitly allowed.

Our project outside the course scope contributes improvements to the dataflow library,
both the shared and the language-specific part. We only implemented the language-specific 
functionality, and an GitHub employee provided stubs for C/C++ and C\#.
The individual features are discussed in the next chapters.

% Every language has its own extractor, which parses source code into the language-specif%ic database scheme.
% TODO? This database scheme evolves together with the language and extractor, 
%The queries and the supporting libraries are all specific to the language being analyzed,
%only the evaluator

\subsection{Philosophy}
Using static analysis for bug finding is lacking in the theoretical foundation {---} 
analyses are usually neither sound nor complete.
This means that a query can report \emph{false positives}, problems flagged by the query that 
are not bugs at all.
Furthermore, queries can also not report instances of a bug which are present in the code,
that instance is then called a \emph{false negative}.
Obviously both false positives and negatives are not desirable, but static analyzers go a long way
to reduce false positives as a means to reduce noise.
If false positives are reduced, almost every problem flagged by an analysis is a real bug.
This makes the static analyzer a very useful tool.
% TODO reword
% TODO general thing

\cite{qlpaper}
\subsection{Evaluation Model}
Every predicate is computed bottoms-up, so when a predicate is encountered, its
tuple set is computed.
Recursive predicates are computed as fixed-point iterations, which means that the database engine 
first adds the tuple sets arising from the base cases, and then from the recursive cases, until
no more tuples can be added. (TODO improve).
Mutual recursions are TODO.

Another useful feature of QL is that it includes syntactic sugar to compute the reflexive-transitive
and the transitive closure of predicates.
For example, a predicate \texttt{getSucc} on control flow nodes gives the set of all reachable
nodes by invoking \texttt{getSucc*}.

\subsection{QL And Relational Database TODO title}
At the heart of QL lives a relational database, and QL is designed to query a relational database.
Every predicate, the basic notion of a BLA, can be regarded as a tuple.

\newpage
\section{Description of Dataflow Library}
Designing a dataflow library which works in theory is a task that is not too difficult.
However, designing a dataflow library which performs well on projects with millions of lines of code is 
considerably more difficult.
In this section, we will describe the dataflow library for the CodeQL platform % TODO check marketing material, is this correct?
for analyzing programs in Java.
We will describe the version without our contributions which are outlined in the next sections.

As the dataflow library is used for bug finding, it is neither sound nor complete.
% TODO finish sentence
In general, the dataflow library reports \emph{possible} dataflow, i.e.\ as soon
as there is a path in the control flow graph TODO flow is reported.
On the other hand, sometimes dataflow paths which do exist are not reported by the 
dataflow library because finding them would be prohibitive performance-wise.
These limitations will be explained in detail later. 

The dataflow library tracks only the flow of exact values.
For example, dataflow of a string is interrupted when \texttt{toUpperCase()} is called.
However, using functions like \texttt{toUpperCase()} does not prevent XSS or SQL injection attacks.
To find such paths that do not interrupt flow when the content of a variable still depends 
heavily on the input data, the \texttt{TaintTracking} library exists.
It is built onto the dataflow library and provides a generic interface to enable
finding of bug classes XSS, SQL injections or directory traversals.
While the taint tracking library is probably the biggest user of the dataflow library,
it is not related to the implementation of the dataflow library.
Thus, it is not discussed here further.
However, all improvements made to the dataflow library directly benefit taint tracking.

\subsection{Interface}
Before we delve into the internals of the dataflow library, we will describe the API 
of the dataflow library users will encounter. %, as well as general design principles.

\subsubsection*{Nodes}
The main concept used throughout the dataflow library is that of a \texttt{Node}.
Every element in the program holding data is a node.
The type \texttt{Node} has several subtypes, corresponding to different kinds of nodes.
The most important subclass of \texttt{Node} is \texttt{ExprNode}, 
as every expression in the program corresponds to a \texttt{Node}.
Another important subclass is \texttt{ParameterNode}.
Both the implicit \texttt{this} parameter for methods and all explicit parameters
given to methods are tracked via these nodes.
Other subtypes exist as well, but are less important to the bigger picture.
These subtypes correspond to instance creation, implicit reads of the \texttt{this} parameter,
the desugaring of vararg method calls and other technical elements that need to be 
represented in the dataflow graph.

One peculiarity of the \texttt{Node} class is that the dataflow graph
is stored in separate predicates, not on the nodes making up the graph itself.
This means that the graph is stored in an adjacency matrix, where the adjacency matrix is encoded in the tuple 
set of a predicate.
Different kinds of dataflow graphs can be represented by different predicates.
% TODO elaborate

\subsubsection*{Configuration}
The \texttt{Configuration} is the main class a user interacts with.
A configuration defines which nodes are sources and sinks of dataflow.
Furthermore, a configuration can also specify additional steps which yield flow.
In the example, a configuration could add that a call to 
\texttt{toUpperCase()} on a string still provides dataflow.
On the other hand, configurations can also specify nodes which block flow.
In the example, this could be sanitizer methods which for example strip HTML tags.
This enables the user to filter out paths which are known to be safe.

The most important predicate on the \texttt{Configuration} class 
is the \texttt{hasFlow(Node source, Node sink)} predicate.
With that predicate users can query the dataflow relation.
It holds whenever there is dataflow starting at the given node \texttt{source} 
(which has to be a source) which flows to the given node \texttt{sink} (which has to be a sink).
Note that this interface does \textbf{not} provide information about specific
dataflow paths.
It also limits dataflow queries to dataflow between the (previously defined) sources and sinks.
Is design limitation is intentionally restrictive, because it allows performance optimizations
that make it feasible to compute whole-program dataflow.

\subsection{General Design}

% jumpstep+localstep

In theory, implementing a dataflow algorithm in a logic programming language is very simple.
Given the \texttt{Node} concept discussed above, one only needs to provide a relation
\texttt{step(Node node1, Node node2)}.
This step relation holds if \texttt{node1} has dataflow in one step to \texttt{node2}
(local and global steps!).
Then, the transitive closure of \texttt{step} has all possible dataflow paths in the program.
These dataflow paths then can be filtered so that only paths which begin in a source and 
end in a sink are considered.

In practice however, this example wouldn't perform well at all.
First, note that the number of \texttt{Node}s grows roughly linear in the size of
the program.
Second, the transitive closure of the \texttt{step} relation enumerates all dataflow
paths in the program from one node to another.
It is defined on the Cartesian product of \texttt{Node} with itself, which is roughly quadratic 
in the size of the program.
We would end up with a predicate that grows probably superlinear in the program size.
Such a growth rate is too big TODO.

On a very high level, the algorithm used in the dataflow library is very similar 
to the algorithm sketched above.
The last step of it is indeed taking the transitive closure of a step-relation.
However, a lot of care is taken to define the step relation only on suitable 
nodes which will probably be part of a dataflow path from a source to a sink.
For example, nodes that cannot be reached from any source via a dataflow path 
are ignored.

\subsection{Description of the algorithm}
We will now present the details of how dataflow is computed for Java programs.
First, the algorithm runs multiple passes of finding candidate nodes which are part 
of dataflow paths.
Then the algorithm computes a step relation and takes the transitive closure of that.
The implementation of the algorithm is divided into a generic dataflow algorithm which is 
used for Java, C/C++ and C\# and a specific language-specific implementation of a common interface.
In this description, we are glossing over the separation of those two modules and 
describe the algorithm formed by the generic module together with the language-specific
module for Java.

\subsubsection*{Terminology}
Before we can start with the steps of the algorithm, we need to introduce some terminology though.

A \emph{local} flow step is a dataflow step that happens inside one callable.
To compute local flow, an SSA form is first computed.
Some examples of local flow steps are:
\begin{itemize}
    \item Data flows from the right-hand side of a variable assignment to a 
    first read of that variable
    \item Data flows from a variable read to an adjacent variable read (a use-use chain in SSA lingo)
    \item Data flows from a parameter node to a first read of the parameter
    \item Data flows from the inside of an expression in parentheses to the dataflow node
    representing the outer expression, i.e.\ in the expression \texttt{(var)} there is dataflow from the node 
    representing the variable read \texttt{var} to the dataflow node representing the entire expression \texttt{(var)}.
\end{itemize}
 % TODO check again, we're talking about single steps here, not entire graphs!
 % This might be a bit early here
Note that, unlike for global dataflow, the entire local dataflow graph for each function 
is computed.
This is necessary because the local dataflow is an important building block in computing
the global dataflow.
A priori it is not clear whether a local dataflow path is part of a (global) dataflow path
from a source to a sink, thus it is not possible to filter paths based on that condition.
As methods are much smaller in relation to the whole program, computing the whole local 
dataflow graph is still feasible and no combinatorial explosion occurs.

A \emph{jump} flow step is a dataflow step that jumps between callables 
in a way that ignores call contexts.
In Java, there are two possibilities for a jump step to occur.
\begin{itemize}
    \item \textbf{Static fields:} Data flows from an assignment to a static field to a read of that static field,
    if the field read is not determined by SSA.
    The field read is only determined by SSA, if a local value is assigned to the static field.
    However, in most cases the static field will be assigned to in one callable, and read 
    in another.
    \item \textbf{Closures:} Data flows from a use or a definition of a variable in the outer callable 
    to a use of a captured variable in the inner function.
\end{itemize}

An \emph{additional} (local or jump) flow step is a dataflow step that the user 
provides via the \texttt{Configuration}.

An \emph{extended return} is a generalized return that representing dataflow out of a callable.
It is either a return statement, which we call a \emph{value return} or a parameter update.
Parameter updates are often called having an \emph{out parameter}.
The programmer supplies the reference to an object in a parameter, and the method then
% TODO check what exactly out parameters are in Java, have a look at https://lgtm.com/query/4328056797028651291/
% Should be in fact vacuous


\subsubsection*{The Node Filtering}
As a first step to compute global dataflow, the algorithm does several filtering passes to 
determine candidate nodes which could possibly be part of a global dataflow path from
a source to a sink.
Note that in the description of the passes, \emph{candidate node} always refers
to being a candidate node in that pass, unless otherwise noted.

The first pass is done in the predicate \texttt{nodeCandFwd1}.
It uses a flooding-style algorithm to compute all nodes reachable from the source nodes,
while ignoring all call contexts.
In detail, this means a \texttt{Node n} is a candidate node if either of the following is true:
\begin{itemize}
    \item The node \texttt{n} is a source node.
    \item There exists a candidate node \texttt{mid} such that \texttt{n} is reachable with a single flow step,
    either a local step, a jump step or an additional step.
    \item There exists a candidate dataflow node \texttt{arg}, such that \texttt{arg} is an argument in 
    a call, and \texttt{n} is a dataflow node representing the parameter in the target of the call.
    In this case, we say that we have dataflow \emph{into} a callable.
    \item The opposite case is that we have dataflow \emph{out} of a callable.
    That happens when the node \texttt{n} is the target of an extended return, 
    where the return node is already a candidate node.
    \item TODO field flow
\end{itemize}

The second pass is done in the predicate \texttt{nodeCand1}.
The second pass filters the results from the first pass by only keeping nodes
which fulfill \texttt{nodeCandFwd1}, i.e.\ are reachable from the source nodes,
that also are reachable when going backwards from the sinks.
Like \texttt{nodeCandFwd1}, it ignores all call contexts.
In detail, this means a \texttt{Node n} is a candidate node if it is contained in 
\texttt{nodeCandFwd1} and either of the following is true:
\begin{itemize}
    \item The node \texttt{n} is a sink node.
    % TODO
\end{itemize}




% TODO: why localFlowStep flows through casts?
% simpleLocalFlowStep (Util): Follows SSA adjacent uses and def-first-use.
%   Data also flows from assignments to asignees (Check word??)
%   interestingly enough, also through casts

% start of considered nodes: nodeCandFwd1(node, config) -  simple local flow 
% node is reachable via dataflow from the source,
% including local flow, field stores, reads, flows into callables via parameters
% and flow out of callables via modified parameters (takes VD into account)
% doesn't take call contexts into account

% then nodeCand1 filters nodeCandFwd1 by just keeping all nodes which are
% reachable from a sink (through the same steps as before)

% nodeCand2: very simple call contexts, TODO find out exact differences to Cand1
% nodeCand:=nodeCand2 excluding call contexts

% bigstep relation: localFlowBigStep

% flowCandFwd0 something something accesspathfront, starts again with flooding
% from the source with all nodeCand2's
% flowCand0 filters flowCandFwd from the sink backwards to the source

% flowCandFwd = flowCandFwd0 AND CastingNode compatible type check

% flowCand = flowCand0 AND flowCandFwd
% flowCand0 uses flowCand and flowCandFwd, so mutual recursion, goes backwards from sink

% TODO: Sections about casts


% what is a node?
% user interface
\newpage
\section{Call Sensitivity}
\subsection{Motivation}
TODO: as discussed above, the dataflow library reports 

% TODO intro
Programmers write functions in their code which change their behavior based on 
a Boolean parameter.
Then on the call sites, sometimes a Boolean literal is used to determine the behavior
of the subroutine.
For example in the Apache Hadoop project, almost 10000 instances of this
pattern occurred\footnote{\url{https://lgtm.com/query/139379883091955/}}.
In pseudo-code, the pattern looks like this:
% TODO better
\begin{minted}{java}
        fun A(bool param) {
            if(param) {
                // do X
            }
            else {
                // do Y
            }
        }

        fun B() {
            A(false);
            A(true);
        }
\end{minted}
Previously, the dataflow library didn't take these into account.


\subsection{Description of Implementation}
The implementation consists of N (TODO) different parts.
First, there is the predicate \texttt{isUnreachableInCall(Node n, DataFlowCall call)} which
computes the set of all nodes in the dataflow graph which are unreachable
TODO more
Second, 
% TODO mention virtual dispatch
\subsection{Correctness}
The correctness of the implementation is based on the observation that constants can be
inlined.
% TODO more

\subsection{Examples of code where detection is now improved}
\subsection{Future improvements}
The analysis described in this pattern can be further generalized, as
this pattern applies to more than just functions that take a Boolean parameter.
Boolean types are generalized by enumerated types, providing (usually) more than two options.
In fact, using an enum-typed variable over a Boolean variable can be the better from a software engineering perspective
even when just encoding a choice of two options.
Choices such as left and right are better represented 
by enum constants \texttt{LEFT} and \texttt{RIGHT}, rather than the 
non-descriptive Boolean constants \texttt{true} and \texttt{false}.

Looking again at the Apache Hadoop project, we see almost 1800
function calls\footnote{\url{https://lgtm.com/query/1789970986252448931/}} with enum constants as parameter.
In all these instances the enum parameter influences control flow in the function.

However, adding support for enum constants was not in the scope of the project, so we didn't implement it.
Furthermore, our architecture is designed to be easily extended.
Thus adding support for enum constant would be a mere software engineering effort
with no significant academic interest.


\newpage
\section{Path Sensitivity}
\subsection{Motivation}
\subsection{Description of Implementation}
\subsection{Sketch of Correctness Proof}




%%% Local Variables:
%%% TeX-master: "main"
%%% TeX-engine: xetex
%%% End:
