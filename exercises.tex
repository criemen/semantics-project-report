% !TEX root = main.tex
\begin{abstract}
    \lipsum[1-2][1-20]
\end{abstract}
\tableofcontents

\newpage
\section{Introduction}
\subsection{Static Analysis}
\subsection{Description of CodeQL}
% Quick history Overview

\subsection{Architecture Outline}
% TODO check again
% overview paragraph is not very good/precise
In this section, we will give a short overview of the CodeQL architecture, as well as where our
project fits in. We describe the extraction process where source code is turned into a relational database,
as well as the QL language and the libraries supporting the queries.

In order to analyze source code, it first needs to be processed into a database representation.
For each language an \emph{extractor} parses the source code and outputs a database file.
The extractor is language specific and usually hooks into a (possibly modified) compiler for that language.
The output of the extractor is a relational database in a language-specific \emph{database schema}.
This database schema evolves along with the language, as well as extractor capabilities.
For example, the Java database schema changed when preliminary support for the Java 13 yield statement was
added to the extractor.

The \emph{database engine} allows users to query the database file produced by the extractor.
Queries are written in the QL language.
Both the database engine and the QL language are agnostic to the language the analyzed source code 
is written in.
This allows developers at GitHub with relative easy to add support for new languages to the CodeQL ecosystem.
In theory, QL queries are also independent of the language the analyzed source code is written.
However, in practice queries are always tied intimately to the language of the analyzed source code,
as they depend on the language-specific database schema.
Furthermore, queries can't gloss over language-specific semantics either {---} one example of many would be
that signed integer overflow needs to be treated differently in Java than in C/C++.

For each language, a set of \emph{libraries} supports the development of queries.
These libraries are implemented in QL.
The libraries implement features such as taint tracking, dataflow analysis or control flow graphs.
For every supported language by the CodeQL analyzers, developers need to re-implement all of these libraries from
scratch, as components can't be shared across languages.
The only exception is the dataflow library, which is separated into a language-agnostic and 
language-specific part.
This language-agnostic part of the dataflow library is shared between the libraries for Java, C/C++ and C\#.

Using those libraries, developers at GitHub write \emph{queries} to detect bugs in the analyzed program.
For example, one query detects when variable dereference in Java might be null.
Another query detects if user input ends up in a database query, without the user input being 
sanitized properly for use in such a query.
Queries often rely on the supporting libraries to do the heavy lifting, e.g.\ the SQL injection detection 
is built on top of the dataflow library.


% TODO
The queries are accessible to researchers in academia and all open-source projects free of charge 
via the \href{https://LGTM.com}{LGTM.com} online platform and the CodeQL CLI utility. % TODO font for LGTM
Bugs can be reported via the issue tracker and the project takes pull requests.


All queries as well as the supporting libraries are open-source and published 
on GitHub\footnote{\url{https://github.com/Semmle/ql}}.
The extractors and the query engine are available only in compiled form under a 
proprietary license\footnote{\url{https://github.com/github/codeql-cli-binaries}}.
While the license is quite restrictive,
usage of the CodeQL CLI utility for academic research is explicitly allowed.

Our project outside the course scope contributes improvements to the dataflow library,
both the shared and the language-specific part. We only implemented the language-specific 
functionality, and an GitHub employee provided stubs for C/C++ and C\#.
The individual features are discussed in the next chapters.

% Every language has its own extractor, which parses source code into the language-specif%ic database scheme.
% TODO? This database scheme evolves together with the language and extractor, 
%The queries and the supporting libraries are all specific to the language being analyzed,
%only the evaluator

\subsection{Philosophy}
Using static analysis for bug finding is lacking in the theoretical foundation {---} 
analyses are usually neither sound nor complete.
This means that a query can report \emph{false positives}, problems flagged by the query that 
are not bugs at all.
Furthermore, queries can also not report instances of a bug which are present in the code,
that instance is then called a \emph{false negative}.
Obviously both false positives and negatives are not desirable, but static analyzers go a long way
to reduce false positives as a means to reduce noise.
If false positives are reduced, almost every problem flagged by an analysis is a real bug.
This makes the static analyzer a very useful tool.
% TODO reword
% TODO general thing

\cite{qlpaper}
\subsection{Evaluation Model}
Every predicate is computed bottoms-up, so when a predicate is encountered, its
tuple set is computed.
Recursive predicates are computed as fixed-point iterations, which means that the database engine 
first adds the tuple sets arising from the base cases, and then from the recursive cases, until
no more tuples can be added. (TODO improve).
Mutual recursions are TODO.

Another useful feature of QL is that it includes syntactic sugar to compute the reflexive-transitive
and the transitive closure of predicates.
For example, a predicate \texttt{getSucc} on control flow nodes gives the set of all reachable
nodes by invoking \texttt{getSucc*}.

\subsection{QL And Relational Database TODO title}
At the heart of QL lives a relational database, and QL is designed to query a relational database.
Every predicate, the basic notion of a BLA, can be regarded as a tuple.

\newpage
\section{Description of Dataflow Library}
Designing a dataflow library which works in theory is a task that is not too difficult.
However, designing a dataflow library which performs well on projects with millions of lines of code is 
considerably more difficult.
In this section, we will describe the dataflow library for the CodeQL platform % TODO check marketing material, is this correct?
for analyzing programs in Java.
We will describe the version without our contributions which are outlined in the next sections.

As the dataflow library is used for bug finding, it is neither sound nor complete.
% TODO finish sentence
In general, the dataflow library reports \emph{possible} dataflow, i.e.\ as soon
as there is a path in the control flow graph TODO flow is reported.
On the other hand, sometimes dataflow paths which do exist are not reported by the 
dataflow library because finding them would be prohibitive performance-wise.
These limitations will be explained in detail later. 

The dataflow library tracks only the flow of exact values.
For example, dataflow of a string is interrupted when \java{toUpperCase()} is called.
However, using functions like \java{toUpperCase()} does not prevent XSS or SQL injection attacks.
To find such paths that do not interrupt flow when the content of a variable still depends 
heavily on the input data, the \texttt{TaintTracking} library exists.
It is built onto the dataflow library and provides a generic interface to enable
finding of bug classes XSS, SQL injections or directory traversals.
While the taint tracking library is probably the biggest user of the dataflow library,
it is not related to the implementation of the dataflow library.
Thus, it is not discussed here further.
However, all improvements made to the dataflow library directly benefit taint tracking.

\subsection{Interface}
Before we delve into the internals of the dataflow library, we will describe the API 
of the dataflow library users will encounter. %, as well as general design principles.

\subsubsection*{Nodes}
The main concept used throughout the dataflow library is that of a \texttt{Node}.
Every element in the program holding data is a node.
The type \texttt{Node} has several subtypes, corresponding to different kinds of nodes.
The most important subclass of \texttt{Node} is \texttt{ExprNode}, 
as every expression in the program corresponds to a \texttt{Node}.
Another important subclass is \texttt{ParameterNode}.
Both the implicit \texttt{this} parameter for methods and all explicit parameters
given to methods are tracked via these nodes.
Objects that potentially change their state through an operation are modelled
 by \texttt{PostUpdateNode}s.
Objects that have a method called on them (potentially mutating it),
field accesses on an object and parameters to method cals (the callee could mutate the object)
have two nodes for that expression - one so-called pre-update node, which represents 
the object before the operation occured, and a post-update node which represents 
the object after the operation occured.
This becomes more clear when looking at the statement \java{a.field = "string"}.
In this example, there exists an expression node for 
\java{"string"}, an expression node for \java{a.field},
and an expression node for \java{a}.
However, none of these expression nodes capture that the object \java{a}
is mutated by the assignment.
For this, there exists also a \texttt{PostUpdateNode} for \java{a},
which represents the fact that the statement (potentially) mutates object \java{a}.
It has the expression node for \java{a} as corresponding pre-update node.

Other subtypes of the \texttt{Node} class exist as well, 
but are less important to the bigger picture of understanding the dataflow library.
These subtypes correspond to instance creation, implicit reads of the \texttt{this} parameter,
the desugaring of vararg method calls and other more technical details.

One peculiarity of the \texttt{Node} class is that the dataflow graph
is stored in separate predicates, not on the nodes making up the graph itself.
This means that the graph is stored in an adjacency matrix, where the
 adjacency matrix is encoded in the tuple set of a predicate.
Different kinds of dataflow graphs can be represented by different predicates,
using the same nodes.
% TODO elaborate

\subsubsection*{Configuration}
The \texttt{Configuration} is the main class a user interacts with.
A configuration defines which nodes are sources and sinks of dataflow.
Furthermore, a configuration can also specify additional steps which yield flow.
In the example, a configuration could add that a call to 
\texttt{toUpperCase()} on a string still provides dataflow.
On the other hand, configurations can also specify nodes which block flow.
In the example, this could be sanitizer methods which for example strip HTML tags.
This enables the user to filter out paths which are known to be safe.

The most important predicate on the \texttt{Configuration} class 
is the \texttt{hasFlow(Node source, Node sink)} predicate.
With that predicate users can query the dataflow relation.
It holds whenever there is dataflow starting at the given node \texttt{source} 
(which has to be a source) which flows to the given node \texttt{sink} (which has to be a sink).
Note that this interface does \textbf{not} provide information about specific
dataflow paths.
It also limits dataflow queries to dataflow between the (previously defined) sources and sinks.
Is design limitation is intentionally restrictive, because it allows performance optimizations
that make it feasible to compute whole-program dataflow.

\subsection{General Design}

In theory, implementing a dataflow algorithm in a logic programming language is very simple.
Given the \texttt{Node} concept discussed above, one only needs to provide a relation
\texttt{step(Node node1, Node node2)}.
This step relation holds if \texttt{node1} has dataflow in one step to \texttt{node2}
(local and global steps!).
Then, the transitive closure of \texttt{step} has all possible dataflow paths in the program.
These dataflow paths then can be filtered so that only paths which begin in a source and 
end in a sink are considered.

In practice however, this example wouldn't perform well at all.
First, note that the number of \texttt{Node}s grows roughly linear in the size of
the program.
Second, the transitive closure of the \texttt{step} relation enumerates all dataflow
paths in the program from one node to another.
It is defined on the Cartesian product of \texttt{Node} with itself, which is roughly quadratic 
in the size of the program.
We would end up with a predicate that grows probably superlinear in the program size.
The CodeQL tools are capable of handling very large programs,
for which algorithms with superlinear behaviour typically don't scale,
so a more involved approach to compute dataflow is needed.

On a very high level, the algorithm used in the dataflow library is very similar 
to the algorithm sketched above.
The last step of it is indeed taking the transitive closure of a step-relation.
However, a lot of care is taken to define the step relation only on suitable 
nodes which will probably be part of a dataflow path from a source to a sink.
For example, nodes that cannot be reached from any source via a dataflow path 
are ignored.

\subsection{Description of the algorithm}
We will now present the details of how dataflow is computed for Java programs.
First, the algorithm runs multiple passes of finding candidate nodes which are part 
of dataflow paths.
Then the algorithm computes a step relation and takes the transitive closure of that.
The implementation of the algorithm is divided into a generic dataflow algorithm which is 
used for Java, C/C++ and C\# and a specific language-specific implementation of a common interface.
In this description, we are glossing over the separation of those two modules and 
describe the algorithm formed by the generic module together with the language-specific
module for Java.
The implementation of the dataflow library for Java encompasses roughly 4000 lines of
QL code. Describing every single detail of the implementation would be neither feasible nor 
useful. Thus, some details are omitted or the implementation of a particular step 
of the algorithm is only described on a very abstract level.
Other parts of the algorithm are described more in-depth.

\subsubsection*{Terminology}
Before we can start with the steps of the algorithm, we need to introduce some terminology.

A \emph{local} flow step is a dataflow step where data flows from one node to another
and both nodes are inside the same callable.
Before local flow can be computed, an SSA form is computed first.
Some examples of local flow steps are:
\begin{itemize}
    \item Data flows from the right-hand side of a variable assignment to a 
    first read of that variable
    \item Data flows from a variable read to an adjacent variable read (a use-use chain in SSA lingo).
    That means that both nodes read the same variable, and there is no other operation between the 
    reads, except possibly the presence of $\phi$-nodes.
    \item Data flows from a parameter node to a first read of the parameter
    \item Data flows from the inside of an expression in parentheses to the dataflow node
    representing the outer expression, i.e.\ in the expression \java{(var)} there is dataflow from the node 
    representing the variable read \java{var} to the dataflow node representing the entire expression \texttt{(var)}.
    \item Data flows from a post-update node that represents an object to an adjacent read 
    of that object
\end{itemize}
There are some more 
 % TODO check again, we're talking about single steps here, not entire graphs!
 % This might be a bit early here
Note that, unlike for global dataflow, the entire local dataflow graph for each function 
is computed.
This is necessary because the local dataflow is an important building block in computing
the global dataflow.
A priori it is not clear whether a local dataflow path is part of a (global) dataflow path
from a source to a sink, thus it is not possible to filter paths based on that condition.
As methods are much smaller in relation to the whole program, computing the whole local 
dataflow graph is still feasible and no combinatorial explosion occurs.

A \emph{jump} flow step is a dataflow step that jumps between callables 
in a way that ignores call contexts.
In Java, there are two possibilities for a jump step to occur.
\begin{itemize}
    \item \textbf{Static fields:} Data flows from an assignment to a static field to a read of that static field,
    if the field read is not determined by SSA.
    The field read is only determined by SSA, if a local value is assigned to the static field.
    However, in most cases the static field will be assigned to in one callable, and read 
    in another.
    \item \textbf{Closures:} Data flows from a use or a definition of a variable in the outer callable 
    to a use of a captured variable in the inner function.
\end{itemize}

An \emph{additional} (local or jump) flow step is a dataflow step that the user 
provides via the \texttt{Configuration}.

% TODO remove this, check for other references to it
An \emph{extended return} is a generalized return that representing dataflow out of a callable.
It is either a return statement, which we call a \emph{value return} or a parameter update.
Parameter updates are often called having an \emph{out parameter}.
The programmer supplies the reference to an object in a parameter, and the method then
% TODO check what exactly out parameters are in Java, have a look at https://lgtm.com/query/4328056797028651291/
% Should be in fact vacuous


\subsubsection*{Flow Through Methods}
As a preparatory step, the dataflow algorithm summarizes methods that at some point 
return one of their parameters to a single dataflow step.
The dataflow from the argument node to the node for the value returned by
the method is thus represented by only a single edge.
\begin{listing}[H]
    \begin{javacode}
        class A {
            String name;
        
            public void setName(String name) {
                this.name = name;
                return this;
            }
        }
    
        class B {
            public void caller() {
                A a = new A();
                a.setName("a name");
            }
        }
    \end{javacode}
    \caption{Example code for flow through method summarization}    
    \label{lst:flow-through-example}
\end{listing}
In the code example in~\autoref{lst:flow-through-example}, there is dataflow of the instance
of class \java{A} to the method \java{setName}, because the instance a 
is (implicitly) passed as \java{this} parameter.
Because of the return statement, there is dataflow back from \texttt{setName} to
\texttt{caller}.
After the summarization step, the dataflow in line 13 is described in a single step,
from the node representing \texttt{a} as implicit argument to the method call, to the node 
representing the expression \java{a.setName("a name")}.

TODO overview of the algorithm used.
% TODO first pass is used to detect getters


\subsubsection*{Node Filtering --- Phase 1}
As a first step to compute global dataflow, the algorithm does several filtering passes to 
determine candidate nodes which could possibly be part of a global dataflow path from
a source to a sink.
Note that in the description of the passes, \emph{candidate node} always refers
to being a candidate node in that pass, unless otherwise noted.

The first pass is done in the predicate \texttt{nodeCandFwd1}.
It uses a flooding-style algorithm to compute all nodes reachable from the source nodes,
while ignoring all call contexts.
In detail, this means a \texttt{Node n} is a candidate node if either of the following is true:
\begin{itemize}
    \item The node \texttt{n} is a source node.
    \item There exists a candidate node \texttt{mid} such that \texttt{n} is reachable with a single flow step,
    either a local step, a jump step or an additional step.
    \item There exists a candidate dataflow node \texttt{arg}, such that \texttt{arg} is an argument in 
    a call, and \texttt{n} is a dataflow node representing the parameter in the target of the call.
    In this case, we say that we have dataflow \emph{into} a callable.
    \item The opposite case is that we have dataflow \emph{out} of a callable.
    That happens when the node \texttt{n} is the target of an extended return, 
    where the return node is already a candidate node.
    \item TODO field flow
\end{itemize}
TODO: Field flow is only done when branching limit >= 1
% TODO reformulate
If field flow is enabled and data from a candidate node is stored in a field
(directly, through a setter method, or through a method call that returns an
 object in which an argument was stored), the node associated with that store
 is also a candidate node.
 If field flow is enabled and a candidate node is an object which appears in a 
 read expression, the expression node for the field read is also a candidate node.
Getter methods are also detected and treated exactly the same as a regular field read.


The second pass is done in the predicate \texttt{nodeCand1}.
The second pass filters the results from the first pass by only keeping nodes
which fulfill \texttt{nodeCandFwd1}, i.e.\ are reachable from the source nodes,
that also are reachable when going backwards from the sinks.
Like \texttt{nodeCandFwd1}, it ignores all call contexts.
In detail, this means a \texttt{Node n} is a candidate node if it is contained in 
\texttt{nodeCandFwd1} and either of the following is true:
\begin{itemize}
    \item The node \texttt{n} is a sink node.
    \item There exists a candidate node \texttt{mid} such that \texttt{mid} is reachable with a single flow step,
    either a local step, a jump step or an additional step from the node \texttt{n}.
    \item Flow into a callable:
    There exists a candidate node \texttt{param} that is a parameter of a callable,
    such that \texttt{node} is a possible argument that gets passed to the callable.
    \item Flow out of a callable:
    There exists a candidate node that is a return statement, and \texttt{n} is 
    a node in a caller, receiving the returned value
    \item TODO field flow
\end{itemize}

Both passes take virtual dispatch into account by dispatching to all possible callees
for each call site. Only later phases use a more advanced analysis to use the call context 
to restrict the virtual dispatch targets.

\subsubsection*{Node Filtering --- Phase 2}
The second phase of node filtering works very similar to phase 1 --- it 
operates on all nodes which are present in \texttt{nodeCand1} and filters them further.
It is again implemented in a flooding-style algorithm with a forward and backward 
pass, like in phase 1.
It takes simple call contexts into account to make the analysis more precise.

The forward pass is implemented in the predicate \texttt{nodeCandFwd2}.
A node is a candidate node if it is a candidate node as per phase 1 
(i.e. it is in \texttt{nodeCand1})
and fulfills one of the following conditions:
\begin{itemize}
    \item bla
\end{itemize}
Compared to \texttt{nodeCandFwd1}, it skips over methods where the argument value 
flows through (see the subsection directly above), and uses the summarized dataflow for that.
This reduces the number of nodes in the predicate.
Furthermore, TODO.% limits virtual dispatch

% Note: the fromArg parameter in nodeCandFwd2 is there to exclude the nodes
% which have been summarized away in the flow-through-analysis



% TODO: why localFlowStep flows through casts?
% simpleLocalFlowStep (Util): Follows SSA adjacent uses and def-first-use.
%   Data also flows from assignments to asignees (Check word??)
%   interestingly enough, also through casts
% TODO for the purpose of dataflow analysis, all numeric types are considered the same,
% so all numeric conversions are assumed to prevent dataflow
% TODO find out when we use call contexts to help Virutal Dispatch
% Question: compatibleTypes in DataFlowPrivate is always already called with erased representations,
% and then calls getErasedRepr again?
% TODO: local data flow: We have flow from PostUpdateNodes to the next adjacent read,
% but we also have flow from the pre-update node to the next adjacent read?
% Isn't that (more or less) two paths for the same?

% start of considered nodes: nodeCandFwd1(node, config) -  simple local flow 
% node is reachable via dataflow from the source,
% including local flow, field stores, reads, flows into callables via parameters
% and flow out of callables via modified parameters (takes VD into account)
% doesn't take call contexts into account

% then nodeCand1 filters nodeCandFwd1 by just keeping all nodes which are
% reachable from a sink (through the same steps as before)

% nodeCand2: very simple call contexts, TODO find out exact differences to Cand1
% nodeCand:=nodeCand2 excluding call contexts

% bigstep relation: localFlowBigStep

% flowCandFwd0 something something accesspathfront, starts again with flooding
% from the source with all nodeCand2's
% flowCand0 filters flowCandFwd from the sink backwards to the source

% flowCandFwd = flowCandFwd0 AND CastingNode compatible type check

% flowCand = flowCand0 AND flowCandFwd
% flowCand0 uses flowCand and flowCandFwd, so mutual recursion, goes backwards from sink

% TODO: Sections about casts


% what is a node?
% user interface
\newpage
\section{Call Sensitivity}
\subsection{Motivation}
TODO: as discussed above, the dataflow library reports 

% TODO intro
Programmers write functions in their code which change their behavior based on 
a Boolean parameter.
Then on the call sites, sometimes a Boolean literal is used to determine the behavior
of the subroutine.
For example in the Apache Hadoop project, almost 10000 instances of this
pattern occurred\footnote{\url{https://lgtm.com/query/139379883091955/}}.
In pseudo-code, the pattern looks like this:
% TODO better
\begin{minted}{java}
        fun A(bool param) {
            if(param) {
                // do X
            }
            else {
                // do Y
            }
        }

        fun B() {
            A(false);
            A(true);
        }
\end{minted}
Previously, the dataflow library didn't take these into account.


\subsection{Description of Implementation}
The implementation consists of N (TODO) different parts.
First, there is the predicate \texttt{isUnreachableInCall(Node n, DataFlowCall call)} which
computes the set of all nodes in the dataflow graph which are unreachable
TODO more
Second, 
% TODO mention virtual dispatch
\subsection{Correctness}
The correctness of the implementation is based on the observation that constants can be
inlined.
% TODO more

\subsection{Examples of code where detection is now improved}
\subsection{Future improvements}
The analysis described in this pattern can be further generalized, as
this pattern applies to more than just functions that take a Boolean parameter.
Boolean types are generalized by enumerated types, providing (usually) more than two options.
In fact, using an enum-typed variable over a Boolean variable can be the better from a software engineering perspective
even when just encoding a choice of two options.
Choices such as left and right are better represented 
by enum constants \texttt{LEFT} and \texttt{RIGHT}, rather than the 
non-descriptive Boolean constants \texttt{true} and \texttt{false}.

Looking again at the Apache Hadoop project, we see almost 1800
function calls\footnote{\url{https://lgtm.com/query/1789970986252448931/}} with enum constants as parameter.
In all these instances the enum parameter influences control flow in the function.

However, adding support for enum constants was not in the scope of the project, so we didn't implement it.
Furthermore, our architecture is designed to be easily extended.
Thus adding support for enum constant would be a mere software engineering effort
with no significant academic interest.


\newpage
\section{Path Sensitivity}
\subsection{Motivation}
\subsection{Description of Implementation}
\subsection{Soundness Problems}
\subsection{Improved Algorithm}




%%% Local Variables:
%%% TeX-master: "main"
%%% TeX-engine: xetex
%%% End:
