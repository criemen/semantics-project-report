% !TEX root = main.tex
\begin{abstract}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus efficitur suscipit volutpat. In quis diam semper, finibus urna vitae, vehicula tellus. Phasellus tincidunt ullamcorper tortor vitae mattis. Curabitur turpis arcu, faucibus vel sem vitae, facilisis finibus ipsum. Aenean imperdiet tincidunt dui, sed pretium magna imperdiet in. Nam consequat porta nibh, vel iaculis elit. Phasellus ut ipsum et magna malesuada viverra ut ac nulla. Aliquam erat volutpat. Praesent ac rutrum arcu, nec feugiat dui. Proin mollis libero placerat volutpat blandit. Nulla placerat nibh sed tellus fermentum, ac tempor risus dictum. Nullam quam libero, tincidunt sed orci at, rhoncus posuere felis. Aenean ac ipsum sed odio aliquam aliquet ac et sem.

Sed aliquam id nibh sit amet volutpat. Maecenas lobortis velit sed ante congue, sed lobortis diam vehicula. Vivamus in commodo purus. In id condimentum elit. Cras volutpat sagittis orci, vel ornare massa mollis vitae. Donec efficitur gravida quam, eget vehicula purus semper in. In facilisis, diam ut rutrum elementum, nisl magna dictum tortor, id gravida magna ante quis erat.
\end{abstract}
\tableofcontents

\newpage
\section{Introduction}
\subsection{Static Analysis}
\subsection{Description of CodeQL}
Quick history overview

\subsection{Architecture Outline}
% TODO check again
In this section, we will give a short overview of the CodeQL architecture, as well as where our
project fits in. We describe the extraction process where source code is turned into a relational database,
as well as the QL language and the libraries supporting the queries.

In order to analyze source code, it first needs to be processed into a database representation.
For each language an \emph{extractor} parses the source code and outputs a database file.
The extractor is language specific and usually hooks into a (possibly modified) compiler for that language.
The output of the extractor is a relational database in a language-specific \emph{database schema}.
This database schema evolves along with the language, as well as extractor capabilities.
For example, the Java database schema changed when preliminary support for the Java 13 yield statement was
added to the extractor.

The \emph{database engine} allows users to query the database file produced by the extractor.
Queries are written in the QL language.
Both the database engine and the QL language are agnostic to the language the analyzed source code 
is written in.
This allows developers at GitHub with relative easy to add support for new languages to the CodeQL ecosystem.
In theory, QL queries are also independent of the language the analyzed source code is written.
However in practice queries are always tied intimately to the language of the analyzed source code,
as they depend on the language-specific database schema.
Furthermore, queries can't gloss over language-specific semantics either - one examples of many would be
that signed integer overflow needs to be treated differently in Java than in C/C++.

Queries for each language are supported by a set of \emph{libraries}, implemented in QL.
These libraries implement features such as traint tracking, dataflow analysis or control flow graphs.
For every supported language by the CodeQL analyzers, developers need to re-implement all of these libraries from
scratch, as components can't be shared across languages.
The only exception is the dataflow library, which is separated into a language-agnostic and 
language-specific part.
This language-agnostic part of the dataflow library is shared between the libraries for Java, C/C++ and C\#.

Using those libraries, developers at GitHub  write \emph{queries} to detect bugs in the analyzed program.
For example, one query detects when a variable access in Java might be null.
Another query detects if untrusted user input ends up in a SQL query, without the user input being 
sanitized properly for use in a database query.
Queries often rely on the supporting libraries to do the heavy lifting, e.g.\ the SQL injection detection 
is built on top of the dataflow library.


% TODO
The queries are accessible to researchers in academia and all open-source projects free of charge 
via the \href{https://LGTM.com}{LGTM.com} online platform and the CodeQL CLI utility . % TODO font for LGTM
Bugs can be reported via the issue tracker and the project takes pull requests.


All queries as well as their supporting libraries are open-source and published 
on GitHub\footnote{\url{https://github.com/Semmle/ql}}.
The extractors and the query engine are available only in compiled form under a 
very restrictive license\footnote{\url{https://github.com/github/codeql-cli-binaries}}.

Our project outside the course scope contributes improvements to the dataflow library,
both the shared and the language-specific part. We only implemented the language-specific 
functionality, and an GitHub employee provided stubs for C/C++ and C\#.
The individual features are discussed in the next chapters.

% Every language has its own extractor, which parses source code into the language-specif%ic database scheme.
% TODO? This database scheme evolves together with the language and extractor, 
%The queries and the supporting libraries are all specific to the language being analyzed,
%only the evaluator

\subsection{Philosophy}
Using static analysis for bug finding is lacking in the theoretical foundation - 
analyses are usually neither sound nor complete.
This means that a query can report \emph{false positives}, problems flagged by the query that 
are not bugs at all.
Furthermore, queries can also not report instances of a bug which are present in the code,
that instance is then called a \emph{false negative}.
Obviously both false positives and negatives are not desirable, but static analyzers go a long way
to reduce false positives as a means to reduce noise.
If false positives are reduced, almost every problem flagged by an analysis is a real bug.
This makes the static analyzer a very useful tool.
% TODO reword
% TODO general thing


\cite{qlpaper}
\subsection{Evaluation Model}
Every predicate is computed bottoms-up, so when a predicate is encountered, its
tuple set is computed.
Recursive predicates are computed as fixed-point iterations, meaning that the database engine 
first adds the tuple sets arising from the base cases, and then from the recursive cases, until
no more tuples can be added. (TODO improve).
Mutual recursions are 

Another useful feature of QL is that it includes syntactic sugar to compute the reflexive-transitive
and the transitive closure of predicates.
For example, a predicate \texttt{getSucc} on control flow nodes gives the set of all reachable
nodes by invoking \texttt{getSucc*}.

\subsection{QL And Relational Database TODO title}
At the heart of QL lives a relational database, and QL is designed to query a relational database.
Every predicate, the basic notion of a BLA, can be regarded as a tuple

\newpage
\section{Description of Dataflow Library}
TODO: I have no idea about this part yet

% start of considered nodes: nodeCandFwd1(node, config) -  simple local flow 


\newpage
\section{Call Sensitivity}
\subsection{Motivation}
TODO: as discussed above, the dataflow library reports 

% TODO intro
Programmers write functions in their code which change their behaviour based on 
a boolean parameter.
Then on the call sites, sometimes a boolean literal is used to determine the behaviour
of the subroutine.
For example in the apache hadoop project, almost 10000 instances of this
pattern occured\footnote{\url{https://lgtm.com/query/139379883091955/}}.
In pseudo-code, the pattern looks like this:
% TODO better
\begin{minted}{java}
        fun A(bool param) {
            if(param) {
                // do X
            }
            else {
                // do Y
            }
        }

        fun B() {
            A(false);
            A(true);
        }
\end{minted}


Previously, the dataflow library didn't take these into account.


\subsection{Description of Implementation}
The implementation consists of N (TODO) different parts.
First, there is the predicate \texttt{isUnreachableInCall(Node n, DataFlowCall call)} which
computes the set of all nodes in the dataflow graph which are unreachable 
% TODO mention virtual dispatch
\subsection{Sketch of Correctness proof?}
\subsection{Examples of code where detection is now improved}
\subsection{Future improvements}
The analysis described in this pattern can be further generalized, as
this pattern applies to more than just functions that take a boolean parameter.
Boolean types are generalized by enumerated types, providing (usually) more than two options.
In fact, using an enum over a boolean is can be the better from a software engineering perspective
even when just encoding a choice of two options.
Choices such as left and right are better represented 
by enum constants \texttt{LEFT} and \texttt{RIGHT}, rather than the 
nondescriptive \texttt{true} and \texttt{false}.

Looking again at the apache hadoop project, we see almost 1800
function calls\footnote{\url{https://lgtm.com/query/1789970986252448931/}} with enum constants as parameter.
In all these instances the enum parameter influences control flow in the function.

However, adding support for enum constants was not in the scope of the project, so we didn't implement it.
Furthermore, our architecture is designed to be easily extended.
Thus adding support for enum constant would be a mere software engineering effort
with no significant academic interest.


\newpage
\section{Path Sensitivity}
\subsection{Motivation}
\subsection{Description of Implementation}
\subsection{Sketch of correctness proof}




%%% Local Variables:
%%% TeX-master: "main"
%%% TeX-engine: xetex
%%% End:
