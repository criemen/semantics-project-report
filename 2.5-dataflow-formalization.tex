% !TEX root = main.tex
\section{Formalization of Dataflow with dIMP}

In this section, we are going to formalize what we mean by dataflow,
give a simple algorithm for computing dataflow and proving it is sound.
We will furthermore use this formalization to prove that our
path-sensitivity extension of the dataflow algorithm preserves soundness.

We use the toy language IMP as presented in~\cite{sat}.
The lecture notes themselves heavily borrow from~\cite{fsopl} for the
presentation and semantics of IMP.
We assume that the reader is familiar with the syntax and semantics of IMP
as presented in section 2.1 of~\cite{sat}.

% expr, store, result
\newcommand{\bigstep}[3]{\langle #1, #2 \rangle \downarrow #3}
\newcommand{\bigstepderiv}[4]{\overset{#1}{\langle #2, #3 \rangle \downarrow #4}}
\newcommand{\bigstepw}[4]{\langle #1, #2, #3 \rangle \downarrow^w #4}
\newcommand{\bigsteppl}[4]{\langle #1, #2, #3 \rangle \downarrow^{\Phi1} #4}
\newcommand{\bigsteppr}[4]{\langle #1, #2, #3 \rangle \downarrow^{\Phi2} #4}
% name, requirement, result
\newcommand{\bsrule}[3]{\textsc{#1}\ddfrac{#2}{#3}}
\newcommand{\literalint}[1][n]{\bar{#1}}
\newcommand{\store}[1][\sigma]{#1}
\newcommand{\clean}[1][n]{#1^\textbf{c}}
\newcommand{\tracked}[1][n]{#1^\textbf{t}}
\newcommand{\flagged}[1][n]{#1^f}
\newcommand{\source}[1]{\textbf{source }{#1}}
\newcommand{\defaultsource}{\source{a}}
\newcommand{\sink}[1]{\textbf{sink }{#1}}
\newcommand{\defaultsink}{\sink{a}}
\newcommand{\excstore}[1][\sigma]{\hat{#1}}
\newcommand{\initialstore}[1][\sigma]{\tilde{#1}}
\newcommand{\Loc}{\textbf{Loc}}
\newcommand{\partialf}{\dashrightarrow}
\newcommand{\StoreDomain}{\tilde{Z}}
\newcommand{\exception}{\textbf{exc}}
\newcommand{\skipcmd}{\textbf{skip}}
\newcommand{\seq}[2]{{#1}; {#2}}
\newcommand{\defaultseq}{\seq{c_0}{c_1}}
\newcommand{\ifcmd}[3]{\textbf{if }{#1} \textbf{ then }{#2}\textbf{ else }{#3}}
\newcommand{\defaultif}{\ifcmd{b}{c_0}{c_1}}
\newcommand{\btrue}{\textbf{true}}
\newcommand{\bfalse}{\textbf{false}}
\newcommand{\whilecmd}[2]{\textbf{while }{#1} \textbf{ do } {#2}}
\newcommand{\defaultwhile}{\whilecmd{b}{c_0}}
\newcommand{\storecmd}[2]{{#1} \coloneqq {#2}}
\newcommand{\defaultstore}{\storecmd{X}{a}}
\newcommand{\phistore}[3]{{#1} \coloneqq \Phi({#2}, {#3})}
\newcommand{\philist}{\Phi_{list}}
\newcommand{\defaultphi}{\phistore{X_1}{Y_1}{Z_1}, \ldots}
\newcommand{\defaultphilist}{\philist}
\newcommand{\ssaif}[4]{(\textbf{if }{#1} \textbf{ then }{#2}\textbf{ else }{#3});{#4}}
\newcommand{\defaultssaif}{\ssaif{b}{c_0}{c_1}{\defaultphilist}}
\newcommand{\ssawhile}[3]{\textbf{while }#3;{#1} \textbf{ do } {#2}}
\newcommand{\defaultssawhile}{\ssawhile{b}{c_0}{\defaultphilist}}
\newcommand{\dunion}{\sqcup}
\newcommand{\storeassign}[2]{{#1} \dunion [{#2}]}
\newcommand{\leftphi}[1]{X_1 \mapsto {#1}(Y_1), \ldots}
\newcommand{\rightphi}[1]{X_1 \mapsto {#1}(Z_1), \ldots}

\subsection{Syntax and Semantic of dIMP}
We extend the syntax of IMP a little bit
and call the resulting language dIMP (for dataflow IMP):
\begin{align*}
    &a \Coloneqq \dots | \defaultsource\\
    &c \Coloneqq \dots | \defaultsink\\
\end{align*}
We define the set $\tilde{\textbf{Z}} =\{\flagged{} | n \in \textbf{Z}, f \in \{\textbf{c}, \textbf{t}\}\}$.
Stores are denoted with $\store \in \Sigma = \Loc \to \tilde{\textbf{Z}}$.
Stores are function from locations to the set of tagged integer
values.
A further restriction on allowed programs is that only up to one expression
of the form $\textbf{source } a$ is allowed in a program.
This technical restriction makes the later definition of a minimal dataflow graph
easier.
TODO check again if still needed

The intuition here is that for all $n \in \textbf{Z}$, 
each value is tagged by either \textbf{c} for clean or \textbf{t} for being tracked
as having dataflow.
A tracked value originates from the source, and is preserved by value-preserving
operations (see the semantics below).
The command $\defaultsink$ is a special marker for the dataflow sink.
It aborts the program via an exception if a tracked value reaches the sink.
This ensures termination for all programs that have dataflow.
In real programs, the dataflow source would likely be a procedure reading input,
i.e.\ from a terminal, and the dataflow sink would likely be a procedure 
printing the data to a terminal, or otherwise communicating with the outside world.


There is a new judgment of the form 
$\bigstep{a}{\sigma}{\flagged{}}$:
\begin{align*}
    &\bsrule{EA-Num}{}{\bigstep{\literalint}{\store}{\clean}} \qquad
    \bsrule{EA-Loc}{}{\bigstep{X}{\store}{\store(X)}}\qquad
    \bsrule{EA-Plus}
    {\bigstep{a_0}{\store}{\flagged[n_0]}
    \qquad \bigstep{a_1}{\store}{\flagged[n_1]}}
    {\bigstep{a_0+a_1}{\store}{\clean[(n_0+n_1)]}}\\
    &\bsrule{EA-Minus}
    {\bigstep{a_0}{\store}{\flagged[n_0]}
    \qquad \bigstep{a_1}{\store}{\flagged[n_1]}}
    {\bigstep{a_0-a_1}{\store}{\clean[(n_0-n_1)]}} \qquad
    \bsrule{EA-Times}
    {\bigstep{a_0}{\store}{\flagged[n_0]}
    \qquad \bigstep{a_1}{\store}{\flagged[n_1]}}
    {\bigstep{a_0 \times a_1}{\store}{\clean[(n_0 \times n_1)]}}\\
    &\bsrule{EA-Source}{\bigstep{a}{\store}{\flagged}}
    {\bigstep{\source}{\store}{\tracked{}}}
\end{align*}

The Judgment $\langle c, \sigma \rangle \downarrow \sigma'$ is replaced by 
$\bigstep{c}{\sigma}{\excstore'}$
where $\excstore{}: \Loc \to \StoreDomain \dunion \exception$.
The state $\exception$ indicates that an exception was thrown.
Exceptions are thrown when a dataflow-tracked value reaches a sink, the program
is then aborted.
Exceptions in dIMP cannot be caught.

\begin{align*}
    &\bsrule{EC-SinkExc}{\bigstep{a}{\store}{\tracked{}}}
    {\bigstep{\defaultsink}{\store}{\exception}} \qquad
    \bsrule{EC-Sink}{\bigstep{a}{\store}{\clean{}}}
    {\bigstep{\defaultsink}{\store}{\store}} \qquad
    \bsrule{EC-Skip}{}{\bigstep{\skipcmd}{\store}{\store}}\\
    &\bsrule{EC-SeqExc}{\bigstep{c_0}{\sigma}{\exception}}
    {\bigstep{\defaultseq}{\store}{\exception}} \qquad
    \bsrule{EC-Seq}{\bigstep{c_0}{\sigma}{\store''} \qquad \bigstep{c_1}{\store''}{\excstore'}}
    {\bigstep{\defaultseq}{\store}{\excstore'}}\\
    &\bsrule{EC-IfT}{\bigstep{b}{\store}{\btrue}\qquad \bigstep{c_0}{\store}{\excstore'}}
    {\bigstep{\defaultif}{\store}{\excstore'}} \qquad
    \bsrule{EC-IfF}{\bigstep{b}{\store}{\bfalse}\qquad \bigstep{c_1}{\store}{\excstore'}}
    {\bigstep{\defaultif}{\store}{\excstore'}}\\
    &\bsrule{EC-WhileF}{\bigstep{b}{\sigma}{\bfalse}}
    {\bigstep{\defaultwhile}{\store}{\store}} \qquad
    \bsrule{EC-WhileTExc}{\bigstep{b}{\sigma}{\btrue} \qquad \bigstep{c_0}{\store}{\exception}}
    {\bigstep{\defaultwhile}{\store}{\exception}}\\
    &\bsrule{EC-WhileT}{\bigstep{b}{\sigma}{\btrue} \qquad \bigstep{c_0}{\store}{\store''}\qquad
    \bigstep{\defaultwhile}{\store''}{\excstore'}}
    {\bigstep{\defaultwhile}{\store}{\excstore'}}\\
    &\bsrule{EC-Assign}{\bigstep{a}{\store}{\flagged{}}}
    {\bigstep{\defaultstore}{\store}{\store{}[X \mapsto \flagged{}]}}
\end{align*}

The judgement operations for booleans are kept as in IMP.
To support them, we introduce a judgement of the form
$\langle a, \sigma \rangle \downarrow n$:
\begin{align*}
    \textsc{EA-ToPlainNum}\ddfrac{\langle a, \sigma \rangle \downarrow \flagged{}}
    {\langle a, \sigma \rangle \downarrow n}
\end{align*}
{\color{red} @Andrzej: This is a questionable design choice,
 solely to avoid further texing}

\subsection{Syntax and Semantic of SSA-dIMP}
To facilitate the dataflow analysis, we describe a SSA form on dIMP with a concrete
syntax and semantics.
We do not describe how to transform a dIMP program to a SSA-dIMP program, but 
refer the reader to the literature, where different SSA construction algorithms are
described. (TODO cite). Furthermore, there are correctness proofs of SSA construction
available (TODO).

The syntax for boolean and arithmetic expressions in SSA-dIMP is the same as in dIMP.
However, the command syntax changes.
\begin{align*}
    c \Coloneqq &\:\skipcmd \mid \defaultsink \mid \defaultstore \mid \defaultseq 
    \mid \defaultssaif\\
    &\mid \defaultssawhile \\
    \philist \Coloneqq &\:[] \mid \Phi_{node} : \philist\\
    \Phi_{node} \Coloneqq &\:X \coloneqq \Phi(X, Y)
\end{align*}
At every point where $\Phi$ nodes can placed in the control flow graph
of a dIMP program, the syntax of SSA-dIMP has an explicit list of $\Phi$ nodes.
The abstract syntax is given in the normal functional nil/cons list style.
We also write $[X_1 \coloneqq \Phi(Y_1, Z_1), \ldots]$
for a list of $\Phi$ nodes.

A program in SSA-dIMP is only valid if it conforms to the SSA properties.
Thus, every variable definition dominates all of the uses of the variable.
%, and the number of $\Phi$ nodes in the program is minimal.
The easiest way to achieve this is to run a SSA construction algorithm on a dIMP program.
It is a gap in the provided proofs that we do not show that every dIMP program
can be transformed in a valid SSA-dIMP program (especially, we omit showing that
$\Phi$ nodes will be placed exactly at the places they are allowed in the syntax
of SSA-dIMP).
We also require that the first argument for $\Phi$ nodes for \textbf{if} commands
corresponds to the variable after executing the \btrue branch, and the second argument to that
of the variable after executing the \bfalse branch.
For \textbf{while} commands, the first argument to the $\Phi$ node is the value
before executing $c_0$ the first time, and the second argument is the value 
after executing the loop body.
TODO something about use of initial values, as they cannot have a definition

Stores are now partial functions $\store : \Sigma \partialf \Loc$ to indicate that
variables have to be defined before they can be read.
Furthermore, when updating a store, we now use the syntax 
$\store \dunion [X \mapsto \flagged{}]$ to indicate that the store $\store$ did not
contain the variable $X$ before.

The judgment for booleans and arithmetic expressions does not change.
As before, we use $\excstore{}$ to denote partial functions
$\excstore{}: \Loc \partialf \StoreDomain \dunion \exception$.
For $\Phi$ nodes, we introduce to judgements - one that evaluates them to the first argument,
and on that evaluates them to the second argument.
They are of the form $\bigsteppl{\philist}{\sigma}{\sigma'}{\sigma''}$ 
and $\bigsteppr{\philist}{\sigma}{\sigma'}{\sigma''}$,
where the evaluation of $\philist$ with the variables from $\sigma'$
is combined with the variables in the store $\sigma$.
\begin{align*}
    &\bsrule{E$\Phi_1$-Empty}{}{\bigsteppl{[]}{\store}{\store''}{\store}}\\
    &\bsrule{E$\Phi_1$-Assign}
    {\bigsteppl{\defaultphilist}{\store}{\store''}{\store'}}
    {\bigsteppl{\phistore{X_1}{Y_1}{Z_1} : \defaultphilist}{\store}{\store''}{\storeassign{\store'}{X_1 \mapsto \store''(Y_1)}}}\\
    &\bsrule{E$\Phi_2$-Empty}{}{\bigsteppr{[]}{\store}{\store''}{\store}}\\
    &\bsrule{E$\Phi_2$-Assign}
    {\bigsteppr{\defaultphilist}{\store}{\store''}{\store'}}
    {\bigsteppr{\phistore{X_1}{Y_1}{Z_1} : \defaultphilist}{\store}{\store''}{\storeassign{\store'}{X_1 \mapsto \store''(Z_1)}}}\\
\end{align*}

%\begin{landscape}
For while loops, we introduce a special judgment of the form $\bigstepw{c}{\sigma, \sigma''}{\excstore'{}}$:
\begin{align*}
    &\bsrule{EW-WhileF}{\bigsteppr{\philist}{\store}{\store''}{\store'} \qquad
        \bigstep{b}{\store'}{\bfalse}}
    {\bigstepw{\defaultssawhile}{\store}{\store''}{\store'}}\\
    &\bsrule{EW-WhileTExc}{\bigsteppr{\philist}{\store}{\store''}{\store'} \qquad
        \bigstep{b}{\store'}{\btrue} \qquad 
    \bigstep{c_0}{\store'}{\exception}}
    {\bigstepw{\defaultssawhile}{\store}{\store''}{\exception}}\\
    &\bsrule{EW-WhileT}{\bigsteppr{\philist}{\store}{\store''}{\store'''} \ 
        \bigstep{b}{\store'''}{\btrue} \  
    \bigstep{c_0}{\store'''}{\store''''} \  
    \bigstepw{\defaultssawhile}{\store}{\store''''}{\excstore'}}
    {\bigstepw{\defaultssawhile}{\store}{\store''}{\excstore'}}\\
 \end{align*}
\\
We have the following rules for the judgment $\bigstep{c}{\store}{\excstore{}}$
and $\excstore{}: \Loc \partialf \StoreDomain \dunion \exception$:
\begin{align*}
    &\bsrule{EC-WhileF}{\bigsteppl{\philist}{\store}{\store}{\store'} \qquad
        \bigstep{b}{\sigma'}{\bfalse}}
    {\bigstep{\defaultssawhile}{\store}{\store'}}\\
    &\bsrule{EC-WhileTExc}{\bigsteppl{\philist}{\store}{\store}{\store'} \qquad
    \bigstep{b}{\store'}{\btrue} \qquad 
    \bigstep{c_0}{\store'}{\exception}}
    {\bigstep{\defaultssawhile}{\sigma}{\exception}}\\
    &\bsrule{EC-WhileT}{\bigsteppl{\philist}{\store}{\store}{\store''} \ 
        \bigstep{b}{\store''}{\btrue} \ 
    \bigstep{c_0}{\store''}{\store'''} \ 
    \bigstepw{\defaultssawhile}{\store}{\store'''}{\excstore'}}
    {\bigstep{\defaultssawhile}{\store}{\excstore'}}\\
%\end{align*}
%\end{landscape}
%\begin{align*}
    &\bsrule{EC-SinkExc}{\bigstep{a}{\store}{\tracked{}}}
    {\bigstep{\defaultsink}{\store}{\exception}} \qquad
    \bsrule{EC-Sink}{\bigstep{a}{\store}{\clean{}}}
    {\bigstep{\defaultsink}{\store}{\store}} \qquad
    \bsrule{EC-Skip}{}{\bigstep{\skipcmd}{\store}{\store}}\\
    &\bsrule{EC-SeqExc}{\bigstep{c_0}{\sigma}{\exception}}
    {\bigstep{\defaultseq}{\store}{\exception}} \qquad
    \bsrule{EC-Seq}{\bigstep{c_0}{\sigma}{\store''} \qquad \bigstep{c_1}{\store''}{\excstore'}}
    {\bigstep{\defaultseq}{\store}{\excstore'}}\\
    &\bsrule{EC-IfTExc}{\bigstep{b}{\store}{\btrue}\qquad \bigstep{c_0}{\store}{\exception}}
    {\bigstep{\defaultssaif}{\store}{\exception}}\\
    &\bsrule{EC-IfFExc}{\bigstep{b}{\store}{\bfalse}\qquad \bigstep{c_1}{\store}{\exception}}
    {\bigstep{\defaultssaif}{\store}{\exception}}\\
    &\bsrule{EC-IfT}{\bigstep{b}{\store}{\btrue}\qquad \bigstep{c_0}{\store}{\store''}
    \qquad \bigsteppl{\philist}{\sigma}{\sigma''}{\sigma'}}
    {\bigstep{\defaultssaif}{\store}{\store'}}\\
    &\bsrule{EC-IfF}{\bigstep{b}{\store}{\bfalse}\qquad \bigstep{c_1}{\store}{\store''}
    \qquad \bigsteppr{\philist}{\sigma}{\sigma''}{\sigma'}}
    {\bigstep{\defaultssaif}{\store}{\store'}}\\
    &\bsrule{EC-Assign}{\bigstep{a}{\store}{\flagged{}}}
    {\bigstep{\defaultstore}{\store}{\storeassign{\store}{X \mapsto \flagged{}]}}}\\
\end{align*}

In order to correctly model the execution of $\Phi$ nodes for \textbf{while} commands,
a second judgment is set up.
The regular command judmenent only evaluates the first execution of the loop body
(if any), where the variables are populated by the values as defined before the loop.
Then it delegates to a special while-command-judgement that evaluates all other 
loop executions, evaluating the $\Phi$ nodes with the store of the previous loop 
iteration.
Furthermore, the special while judgment takes two stores, as it both needs to evaluate
the condition $b$ in a store with evaluated $\Phi$ nodes, and it also needs to have
the store \emph{before} the while loop executed, so all the variable definitions are,
in fact, not defined in the store yet.

\subsection{Definition of Dataflow}
\begin{definition}[Program]
    A \emph{program} in dIMP is a command $c$.
    Complex programs are expressed by using the recursive nature of
    the definition of commands.
    Any program in dIMP can be transformed to be a program in SSA-dIMP.
\end{definition}

\begin{definition}[Initial Store]
    An \emph{initial store} is a store $\initialstore$ s.t.\ 
    $\forall X: \exists n^c: \initialstore(X) = \clean{}$.
    This means that in an initial store all values are flagged as being clean.
    We will implicitly denote inital stores by $\initialstore$.
\end{definition}

\begin{definition}[Dataflow]
    A tuple $(c, \initialstore)$ of a program $c$ and initial store $\initialstore{}$ 
    has \emph{dataflow from the source to a sink} if
    $\bigstep{c}{\initialstore{}}{\exception}$ holds.
    This means that the program terminates with a dataflow exception.
\end{definition}

\begin{definition}[Dataflow Algorithm]
    A \emph{dataflow algorithm} $\A(c)$ computes, given a program $c$,
    if there exists an initial store $\initialstore{}$ 
    such that $(c, \initialstore)$ has dataflow from the source to a sink.
\end{definition}

\begin{definition}[Soundness]
    A dataflow algorithm is \emph{sound} if for any tuple $(c, \initialstore)$ that
    has dataflow it holds that $\A(c) = \text{HAS\_FLOW}$.
\end{definition}

\begin{definition}[Completeness]
    A dataflow algorithm is \emph{complete} if $\A(c) = \text{HAS\_FLOW}$
    implies that there exists an initial store $\initialstore$
     such that $(c, \initialstore)$ has dataflow.
\end{definition}
\begin{definition}[False Positive]
    A program $c$ for which there exists no initial store $\initialstore$ such that 
    $(c, \initialstore)$ has dataflow, but for which $\A(c) = \text{HAS\_FLOW}$ holds
    is called a \emph{false positive} of the algorithm.
\end{definition}
\begin{remark}
    In general, it is impossible to construct a dataflow algorithm that is both 
    sound and complete.
    In practice, a dataflow algorithm will be neither sound nor complete.
    However, in the theoretical setting of this chapter, we are interested in 
    sound dataflow algorithms.
    The \emph{trivially sound dataflow algorithm} $\A_0(c) = \text{HAS\_FLOW}$ 
    is sound by definition, but not very interesting.
    We will not consider it further, but it is interesting to keep in mind,
    because it shows that just proving that a dataflow algorithm is sound does not
    mean it is useful.    
\end{remark}

\subsection{The Dataflow Graph}
To aid the construction of dataflow algorithms, we define the concept
of a dataflow graph.
The dataflow graph is computed upon an SSA form of the program.

\begin{definition}[Dataflow Graph]
    The \emph{node set} $V$ of a \emph{dataflow graph} $G(c)$
    is defined on the syntactic structure
    of the SSA-dIMP program $c$. It is a subset of the node set of the abstract syntax tree
    (every node knows about its location in the program text).
    Every (sub-)command of $c$, as well as any arithmetic expression $a$ in the 
    program is a node in the dataflow graph.
    Furthermore, every $\Phi$ node in the program is also a node in the dataflow
    graph.
    For example for
    $c = \textbf{skip}; (\textbf{skip}; x \coloneqq \bar{4} + y)$
    the node set consists of $c$ itself, the first \textbf{skip}, the command 
    $\textbf{skip}; x \coloneqq \bar{4} + y$, the second \textbf{skip} (that is different 
    from the first because it appears in another location), the command
    $x \coloneqq \bar{4} + y$, and the arithmetic expressions $\bar{4}+y$,
    $\bar{4}$ and $y$.

    We call a graph $G(c)$ a \emph{dataflow graph} for $c$ if, should an initial store
    $\initialstore{}$ exist such that $(c, \initialstore{})$ has dataflow,
    there exists a path from a node of the form $\defaultsource$ to a node of 
    the form $\defaultsink$ in $G$.
\end{definition}

\begin{definition}[Dataflow Algorithm]
    We define the algorithm $\A_{G}(c)$ that takes a dataflow graph $G(c)$
    and outputs HAS\_FLOW if there exists a path from a node of the form \defaultsource
    to a node of the form \defaultsink, and NO\_FLOW otherwise.
\end{definition}
\begin{remark}
    By the definition of a dataflow graph, $\A_G(c)$ is a sound dataflow algorithm.
\end{remark}

\subsection{A Dataflow Algorithm}
In this section we describe an algorithm $\B(c) = G$ that computes a 
graph $G$ for $c$. We then prove that this algorithm indeed
computes a dataflow graph and thus $\A_G(c)$ is a sound dataflow algorithm.

\subsubsection*{The Algorithm}
Given a program $c$ in SSA-dIMP, the algorithm computes
the edge set $E$ as a least fixed point iteration.
The following recursive rules are used:\\
\textbf{Base Case:} Start at an expression $\textbf{source }a_0$.
If it is used in either the context $X \coloneqq \textbf{source }a_0$
or $\textbf{sink }(\textbf{source }a_0)$ then add an edge from the node for the
source to the node of the store/sink.\\
\textbf{Recursive Case 1:} For any store node $X \coloneqq a_0$ or
$\Phi$ node $\phistore{X}{Y}{Z}$ with an incoming edge
in the dataflow graph, enumerate all uses (traverse
the def-use-chain of the SSA form). Add edges from the store (or $\Phi$ node)
to all reads of $X$.
Reads are either arithmetic expressions reading from that variable,
or $\Phi$-nodes where one of the two possible variables is $X$.\\
\textbf{Recursive Case 2:} For any sink node of the form $\sink{X}$,
where the node for the variable read $X$ has an incoming edge, connect 
the read of $X$ with an edge to the node of the sink.

The fixed point iteration stops eventually as the set of possible edges is
finite. Furthermore, if the program doesn't contain a \textbf{source} expression,
the edge set will be empty.

TODO: Add a second pass, like in QL, that starts at the sink nodes and thus
filters out edges that don't lead to any sink.

\subsubsection*{Soundness Proof}
\begin{theorem}
    \label{thm:df-soundness}
    The algorithm $\B(c) = G$ computes a dataflow graph, and thus 
    $\A_G(c)$ is a sound dataflow algorithm.
\end{theorem}

\begin{definition}[Store-Read Chain]
    Let $(c, \initialstore{})$ be a tuple that has dataflow.
    Let $\E$ be the bigstep derivation of
    $\bigstep{c}{\initialstore{}}{\exception}$.
    A \emph{store-read chain} in $\E$ is a list of $m$ tuples of the form
    \begin{equation*}
        (\E^1_i, \E^2_i)
    \end{equation*}
    with $i \in \{1, \ldots, m\}$.
    $\E^0_i$, $\E^1_i$, $\E^2_i$ and $\E^3_i$ are subderivations of $\E$.
    Furthermore, we require that the $\E^1_i$ are of one of the following forms:\\
    \textbf{Case 1.1:}\\
    \begin{align*}
        \E^1_i=\bsrule{EC-Assign}
        {\bigstepderiv{\E^0_i}{a_i}{\sigma_i}{\flagged{}}}
        {\bigstep{\storecmd{X_i}{a_i}}{\store_i}
        {\store_i \dunion [X_i \mapsto \flagged{}]}}
    \end{align*}
    \textbf{Case 1.2:}\\
    \begin{align*}
        \E^1_i=\bsrule{E$\Phi_1$-Assign}
        {\overset{\E^0_i}{\bigsteppl{\defaultphilist}{\store}{\store''}{\store'}}}
        {\bigsteppl{\phistore{X_i}{Y_i}{Z_i} : \defaultphilist}{\store}{\store''}{\storeassign{\store'}{X_i \mapsto \store''(Y_i)}}}\\
    \end{align*}\\
     \textbf{Case 1.3:}\\
     \begin{align*}
        \E^1_i=\bsrule{E$\Phi_2$-Assign}
        {\overset{\E^0_i}{\bigsteppr{\defaultphilist}{\store}{\store''}{\store'}}}
        {\bigsteppr{\phistore{X_i}{Y_i}{Z_i} : \defaultphilist}{\store}{\store''}{\storeassign{\store'}{X_i \mapsto \store''(Z_i)}}}\\
    \end{align*}\\
    We require that the derivations $\E^2_i$ are of one of the following forms:\\
    \textbf{Case 2.1:}\\
    \begin{align*}
        \E^2_i=\bsrule{EA-Loc}{}
        {\bigstep{X_i}{\store_i'}{\store_i'(X_i) = \flagged{}}}
    \end{align*}\\
    \textbf{Case 2.2:}\\
    \begin{align*}
        \E^2_i=\bsrule{E$\Phi_1$-Assign}
        {\overset{\E^3_i}{\bigsteppl{\defaultphilist}{\store}{\store''}{\store'}}}
        {\bigsteppl{\phistore{X_{i+1}}{X_i}{Z_{i+1}} : \defaultphilist}{\store}{\store''}{\storeassign{\store'}{X_{i+1} \mapsto \store''(X_i)}}}\\
    \end{align*}\\
    \textbf{Case 2.3:}\\
    \begin{align*}
        \E^2_i=\bsrule{E$\Phi_2$-Assign}
        {\overset{\E^3_i}{\bigsteppr{\defaultphilist}{\store}{\store''}{\store'}}}
        {\bigsteppr{\phistore{X_{i+1}}{Y_{i+1}}{X_i} : \defaultphilist}{\store}{\store''}{\storeassign{\store'}{X_{i+1} \mapsto \store''(X_i)}}}\\
    \end{align*}

    Thus, the first element in the tuple is the derivation of a store into the variable
    $X_i$, and the second element in the tuple is the derivation of some read of the 
    variable $X_i$.
    Furthermore, we require that the tuples are connected in the following way:
    
    For $i \in \{1, \ldots, m-1\}$ the store described by the derivation
    $\E^1_{i+1}$ reads the variable $X_i$.
    Thus, if $\E^2_i$ uses \textsc{EA-Loc} (case 2.1), we require that $\E^1_{i+1}$ uses case 1,
    and that $\E^0_{i+1} = \E^2_i$.
    If $\E^2_i$ uses case 2.2 or 2.3 (i.e. the read of $X_i$ is in a $\Phi$-node), we require
    $\E^2_i = \E^1_{i+1}$.
    This is because the \textsc{E$\Phi$-Assign} rules represents both a read and a store
    in the same rule.
\end{definition}

\begin{lemma}
    Let $(c, \initialstore{})$ be a tuple that has dataflow.
    Let $G$ be the graph computed by $\B$.
    Then for every store-read-chain in $(c, \initialstore{})$ 
    there is a path from in $G$ from the first store to the last read.
\end{lemma}
\begin{proof}
    TODO
\end{proof}


\begin{lemma}
    Let $(c, \initialstore{})$ be a tuple that has dataflow.
    Let $G$ be the graph computed by $\B$.
    Then there is a path
    from a source to every node of a variable read of $X$ that has a
    derivation $\bsrule{EA-Loc}{}{\bigstep{X}{\sigma}{\flagged{}}}$.
    
\end{lemma}

\begin{proof}[Proof of~\autoref{thm:df-soundness}]
    Because $(c, \initialstore{})$ has dataflow, the program contains a $\sink{}$
    command that evaluates to $\exception$.
    We proof the theorem by case distinction over the structure of that $\sink{}$
    command.\\
    \textbf{Case 1:}
    The $\sink{}$ has form $\sink{(\source{ a_0})}$.
    Then, by the base case of the algorithm there is a path from a source node
    to a sink node in the graph.\\
    \textbf{Case 2:}
    The $\sink{}$ has form $\sink{X}$.
    As we know that $\bigstep{\sink{X}}{\sigma}{\exception}$ holds, we know that
    we have a bigstep derivation
    \begin{align*}
        \bsrule{EC-SinkExc}
        {\bsrule{EA-Loc}{}{\bigstep{X}{\sigma}{\flagged{}}}}
        {\bigstep{\sink{X}}{\sigma}{\exception}}
    \end{align*}
    Then, by the lemma (TODO) we get a path from the source to the variable read.
    By the recursive case 2, we get an edge from the variable read to the sink.
    Thus, we have a path from a source to the sink
    This concludes the proof, as there are no other forms a sink command evaluating 
    to $\exception$ can take.
\end{proof}
\iffalse
\subsection{IGNORE: The Dataflow Graph}
To aid the construction of dataflow algorithms, we define the concept
of a dataflow graph.
Then we show that an algorithm that outputs HAS\_FLOW if 
there exists a path from the source 
node to a sink node in a dataflow graph %and NO\_FLOW otherwise
is a sound dataflow algorithm.
Furthermore, we introduce an algorithm to compute a dataflow graph.

The dataflow graph is computed upon an SSA form of the program.
Thus, it may contain assignments of the form $X \coloneqq \phi(a_1, a_2)$.
We will assume that for any given store $\sigma$ the derivation only contains 
the assignment with an evaluated $\phi$ function, as to not overload the notation.
Furthermore, the dataflow graph defined on an SSA form of the program can easily 
be transformed to a dataflow graph defined on the regular program.
The minimal dataflow graph is directed and may, depending on the 
program structure, contain cycles.

The \emph{node set} $V$ of the \emph{minimal dataflow graph} $G_\text{min}(c)$
is defined on the syntactic structure
of the SSA form of $c$. It is a subset of the node set of the abstract syntax tree
(every node knows about its location in the program text).
Every (sub-)command of $c$, as well as any arithmetic expression $a$ in the 
program is a node in the dataflow graph.
For example for
$c = \textbf{skip}; (\textbf{skip}; x \coloneqq \bar{4} + y)$
the node set consists of $c$ itself, the first \textbf{skip}, the command 
$\textbf{skip}; x \coloneqq \bar{4} + y$, the second \textbf{skip} (that is different 
from the first because it appears in another location), the command
$x \coloneqq \bar{4} + y$, and the arithmetic expressions $\bar{4}+y$,
$\bar{4}$ and $y$.


For every initial store $\sigma_0$, 
let $\E$ be a bigstep derivation of $\langle c, \sigma_0 \rangle \downarrow \sigma'$.
The edge set $E^0_{\sigma_0}$ contains the following edges:
\begin{enumerate}
    \item $\E$ contains a subderivation
    \begin{equation*}
        \E_1 = \textsc{EC-Assign1}\ddfrac{\overset{\E'}{\langle a, \sigma \rangle \downarrow \top_n}}
        {\langle X \coloneqq a, \sigma \rangle \downarrow
         \sigma[X \mapsto \top_n]}
    \end{equation*}
    Then there is an edge from $a$ to the node of $X \coloneqq a$.
    This case includes both outgoing edges from sources and variable reads
    of locations to which a tracked value was written to.
    \item $\E$ contains a subderivation
    \begin{equation*}
        \E_1 = \textsc{EC-Sink1}\ddfrac{\overset{\E'}{\langle a, \sigma \rangle \downarrow \top_n}}
        {\langle \textbf{sink }a, \sigma \rangle \downarrow
         \sigma}
    \end{equation*}
    Then there is an edge from $a$ to the node of $\textbf{sink }a$.
    \item
    $\E$ contains a subderivation
    \begin{equation*}
        \E_1 = \textsc{EA-Loc}\ddfrac{}
        {\langle X, \sigma \rangle \downarrow \top_n}
    \end{equation*}
    and a subderivation
    \begin{equation*}
        \E_2 = \textsc{EC-Assign1}\ddfrac{\overset{\E'}{\langle a, \sigma' \rangle \downarrow \top_n}}
        {\langle X \coloneqq a, \sigma' \rangle \downarrow
         \sigma'[X \mapsto \top_n]}
    \end{equation*}
    with $\sigma'(X) \neq \top_n$ (this prevents cyclic edges).
    Then there is an edge from the store $X \coloneqq a$ to the read node $X$ as 
    refered to by $\E_1$.\\
    Technical note 1: Here we exploit the fact that there is only one source ---
    else this case could introduce edges between unrelated paths if two sources for the same 
    $\sigma_0$ would evaluate to $\top_n$. Then the edge set would not be minimal
    anymore.\\
    Technical note 2: Because the program is in SSA, edges from a store to a read 
    are in execution-order. A read has an incoming edge from a store only if the 
    store happens before the read in the program.
\end{enumerate}

Based on that we define the (minimized) edge set $E^1_{\sigma_0}$
that contains an edge from $E^0_{\sigma_0}$ iff it is part of any path 
from a node of the form $\textbf{source }a_0$
to a node of the form $\textbf{sink }a_1$.

Then we define the \emph{edge set} $E$ of the minimal dataflow graph as
\begin{equation*}
    E = \bigcup_{\sigma_0} E^1_{\sigma_0}
\end{equation*}
Even though the union is over an infinite set, the result is finite
as $E \subseteq V \times V$ and $|V| < \infty$.

We call a graph a \emph{dataflow graph} if it has the node set as the minimal
dataflow graph and its edge set contains the edges of the minimal dataflow 
graph.

Let $\A_{G_\text{min}}(c)$ be the algorithm that takes the
minimal dataflow graph $G_\text{min}(c)$ and outputs HAS\_FLOW if there 
exists a node of the form 
$\textbf{source }a$ and a node $\textbf{sink }a$ such that there is a path in 
$G_\text{min}(c)$ from the source node to the sink node.
If no such path exists, it outputs NO\_FLOW.


\begin{theorem}
    \label{thm:min-dg}
    The algorithm $\A_{G_\text{min}}(c)$ is a sound dataflow algorithm.
\end{theorem}
In order to prove the theorem, we introduce the concept of store-read chains.

\begin{definition}[Store-Read Chain]
    Let $(c, \sigma_0)$ be a tuple that has dataflow.
    Let $\E$ be the bigstep derivation of
    $\langle c, \sigma_0 \rangle \downarrow \sigma'$.
    A \emph{store-read chain} in $\E$ is a list of $m$ tuples of the form
    $(\E^1_i=\ddfrac{\overset{\E^0_i}{\langle a_i, \sigma_i \rangle \downarrow \top_n}}
    {\langle X_i \coloneqq a_i, \sigma_i \rangle \downarrow \sigma_i[X_i \mapsto \top_n]},
    \E^2_i=\ddfrac{}{\langle X_i, \sigma_i' \rangle \downarrow \sigma_i'(X_i) = \top_n})$
    with $i \in \{1, \ldots, m\}$.
    $\E^0_i$, $\E^1_i$ and $\E^2_i$ are subderivations of $\E$.
    We require that the following holds:
    \begin{enumerate}
        \item $\sigma_i(X) \neq \top_n$ (so we have a first store). 
        \item For $i \in \{1, \ldots, m-1\}$ it holds that 
        $\E^0_{i+1}=\E^2_i$. This means that every store except the first one
        in the chain is connected via the variable it reads from to the previous 
        store in the chain.
        \item Repeating tuples are not allowed in the list, all tuples have to 
        be pairwise disjoint --- equality on subderivations of $\E$ includes
        placement in $\E$, not just equality of the subderivation trees.
    \end{enumerate}
\end{definition}

\begin{remark}
    By definition a read-store-chain does not contain any cycles.
    As the derivation $\E$ is finite, this implies that the set of 
    read-store chains is finite.
    By defining the obvious partial order induced by inclusion
    on the set of read-store-chains, we see that 
    every non-empty read-store-chain can be extended to a maximal 
    read-store-chain.
\end{remark}

\begin{lemma}
    \label{lem:chain-to-graph}
    Any store-read-chain induces a path in $E^0_{\sigma_0}$ from the node 
    of the first store in the chain to the node of the last read in the chain.
    Furthermore, there is an edge from the expression that the first store reads
    to the first store.
\end{lemma}
\begin{proof}
    We proof the lemma by induction over the length of the store-read chain.\\
    \textbf{Base Case (m=1):}
    There is by definition of $\E^0_{\sigma_0}$
    one edge of type (3) connecting the store and the read in the chain.
    Furthermore, there is an edge of type (1) connecting the read-from expression
    to the store.\\
    \textbf{Inductive Case:}
    Let $m \geq 1$ be fixed. By the IH, there is a path in $E^0_{\sigma_0}$
    corresponding to the chain of length $m-1$ from the first store in the chain 
    to the read of $X_{m-1}$ in the chain.
    By definition of the chain, we have an edge of type (1) connecting 
    the read of $X_{m-1}$ to the store of $X_m$.
    Furthermore, again by definition of $\E^0_{\sigma_0}$
    there is one edge of type (3) connecting the store of $X_m$ to the
    read of $X_m$.
\end{proof}

\begin{lemma}
    \label{lem:max-store-read}
    Any maximal store-read-chain contains a store from the source as first store.
\end{lemma}
\begin{proof}
    We look at the derivation $\E^0_1$.
    If $a = Y$ for some $Y \in \textbf{Loc}$, then the store-read-chain is not 
    maximal, because it can be extended at the front by a tuple with a store to and a
    read from $Y$.
    The only other possibility for an arithmetic expression $a$ to evaluate to 
    $\top_n$ is that it is of form $\textbf{source } a_0$.
    This concludes the proof.
\end{proof}


\begin{proof}[Proof Of~\autoref{thm:min-dg}]
    Let $\sigma_0$ be an initial store such that $(c, \sigma_0)$ has dataflow.
    If no such $\sigma_0$ exists, by the definition of soundness
    the proof is concluded.

    Let $\E$ be the bigstep derivation of
    $\langle c, \sigma_0 \rangle \downarrow \sigma'$.
    Because $(c, \sigma_0)$ has dataflow, there exists a derivation of the form 
    \begin{equation*}
        \E_1 = \textsc{EC-Sink1}\ddfrac
        {\overset{\E_2}{\langle a, \sigma \rangle \downarrow \top_n}}
        {\langle \textbf{sink } a, \sigma} \downarrow \sigma
    \end{equation*}
    There are two possibilities for the structure of a.\\
    \textbf{Case 1:} $a = \textbf{source }a_0$\\
    Then $E^0_{\sigma_0}$ contains an edge of type (2) that connects the source 
    to a sink. This constitues a path from the source to a sink.\\
    \textbf{Case 2:} $a = X$\\
    Then $X$ is a variable read that evaluates to $\top_n$.
    Because $\sigma_0(X) \neq \top_n$, there has to be a first store
    of $X$ in $\E$.
    The derivation of that store, together with the derivation of the read make 
    up a store-read chain of length one.
    This chain can be extended to a maximal chain, and 
    by~\autoref{lem:max-store-read}, we know it starts with a store reading the
    source.
    Furthermore, by~\autoref{lem:chain-to-graph} we get a path in $E^0_{\sigma_0}$
    from the source node to the read. The read is then connected by an edge
    of type (2) to the sink, thus we have a path from the source to a sink in 
    $E^0_{\sigma_0}$.

    In both cases we have shown that $E^0_{\sigma_0}$ contains a path from the 
    source to a sink. Thus, $E^1_{\sigma_0}$ and in turn $E$ contain a path
    from the source to a sink, and $\A_{G_\text{min}}(c) = \text{HAS\_FLOW}$.
    This concludes the proof.
\end{proof}

\begin{corollary}
    \label{cor:dg-sound}
    For any dataflow graph $G(c)$ the algorithm $\A_{G}(c)$ is a sound
     dataflow algorithm.
\end{corollary}
\begin{proof}
    Remember that any dataflow graph contains the minimal dataflow graph as 
    subgraph. Thus for every pair $(c, \sigma_0)$ that has dataflow
    there is a path from the source node to a sink node, 
    thus $\A$ reports flow by~\autoref{thm:min-dg}.
    It doesn't matter if $G$ has more edges for soundness.
\end{proof}

\begin{remark}
    The \emph{minimal} dataflow graph gets name by being the minimal dataflow graph
    among all dataflow graphs that are considered in this paper.
    It is not the smallest graph $G$ on the node set 
    such that $\A_G(c)$ is a sound dataflow algorithm,
    because $G_\text{min}$ contains back-edges for loops that would, strictly
    speaking, not be necessary to make $\A_G(c)$ a sound dataflow algorithm.
\end{remark}

\begin{remark}
    Note that not all (sound) dataflow algorithms have the form $\A_G(c)$.
    For example the trivially sound dataflow algorithm disagrees on the 
    program $\textbf{skip}$ with any dataflow algorithm of the form 
    $\A_G(c)$ - the trivially sound
    dataflow algorithm outputs HAS\_FLOW, whereas
    $A_G(\textbf{skip}) = \text{NO\_FLOW}$ because the node set contains
    neither source nor sink.
\end{remark}

\subsection{IGNORE: An Algorithm to Compute A Dataflow Graph}
In this section we describe an algorithm $\B(c) = G$ that computes a dataflow 
graph $G$. We then prove that this algorithm indeed
computes a dataflow graph, and, using~\autoref{cor:dg-sound},
$\A_G(c)$ is a sound dataflow algorithm.

\subsubsection*{The Algorithm}
Given a program $c$ in SSA form, the algorithm computes
the edge set $E$ as a least fixed point iteration.
The following recursive rules are used:\\
\textbf{Base Case:} Start at the expression $\textbf{source }a_0$.
If it is used in either the context $X \coloneqq \textbf{source }a_0$
or $\textbf{sink }(\textbf{source }a_0)$ then add an edge from the node for the
source to the node of the store.\\
\textbf{Recursive Case:} For any store node $X \coloneqq a_0$, enumerate all uses (traverse
the def-use-chain of the SSA form). Add edges to all uses that are themselves
either stores or \textbf{sink} commands of the form $\textbf{sink } X$.
Note that stores of form $Z \coloneqq \phi(X, Y)$ get an incoming edge from $X$,
if the definition of $X$ itself has an incoming edge.

The fixed point iteration stops eventually as the set of possible edges is
finite. Furthermore, if the program doesn't contain a \textbf{source} expression,
the edge set will be empty.

TODO: Add a second pass, like in QL, that starts at the sink nodes and thus
filters out edges that don't lead to any sink.

\subsubsection*{Soundness Proof}
\begin{theorem}
    The algorithm $\B(c) = G$ computes a dataflow graph, and thus 
    $\A_G(c)$ is a sound dataflow algorithm.
\end{theorem}
\begin{proof}
    Because of~\autoref{cor:dg-sound}, we only need to show that $\B(c)$ is 
    a dataflow graph.
    To do that, we show that for $G=(V, E)$, for all initial stores $\sigma_0$,
    all edges in $E^0_{\sigma_0}$ (as in the definition of the minimal dataflow
    graph) are contained in the set $E$.

    Let $\sigma_0$ be an initial store.
    Let $e$ be an edge in $E^0_{\sigma_0}$.\\
    \textbf{Case 1:} $e$ is an edge of type (1).\\
    Then the end node of $e$
    has form $X \coloneqq a$.
    Then there are two cases for the structure of $a$.\\
    \textbf{Subcase 1:} $a = \textbf{source }a_0$.
    Then there is a corresponding edge in $E$ per the base case of the recursive
    definition of $E$.\\
    \textbf{Subcase 2:} $a = Y$ for some variable $Y \neq X$ (remember, we have 
    a SSA form)
    TODO\\
    \textbf{Case 2:} $e$ is an edge of type (2).\\
    TODO\\
    \textbf{Case 3:} $e$ is an edge of type (3).\\
    TODO
\end{proof}


\subsection{IGNORE: A Path-Sensitive Dataflow Algorithm}
TODO: description of path-pruning, proof that the only pruned edges are not
in the definition of the minimal dataflow graph, thus it is still sound

%However, as a dataflow algorithm cannot be sound and complete at the same time,
%dataflow algorithms constitute an heuristic.
%The evaluation of heuristics in a theoretic setting is difficult.
%We do aim to show that 
% TODO: A_2 is better than A_1, as both sound+one has less FP
\fi