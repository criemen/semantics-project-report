% !TEX root = main.tex
\section{Formalization of Dataflow with dIMP}

In this section, we are going to formalize what we mean by dataflow,
give a simple algorithm for computing dataflow and proving it is sound.
We will furthermore use this formalization to prove that our
path-sensitivity extension of the dataflow algorithm preserves soundness.

We use the toy language IMP as presented in~\cite{sat}.
The lecture notes themselves heavily borrow from~\cite{fsopl} for the
presentation and semantics of IMP.
We assume that the reader is familiar with the syntax and semantics of IMP
as presented in section 2.1 of~\cite{sat}.

% expr, store, result
\newcommand{\bigstep}[3]{\langle #1, #2 \rangle \downarrow #3}
\newcommand{\bigstepderiv}[4]{\overset{#1}{\langle #2, #3 \rangle \downarrow #4}}
\newcommand{\bigstepw}[4]{\langle #1, #2, #3 \rangle \downarrow^w #4}
\newcommand{\bigsteppl}[4]{\langle #1, #2, #3 \rangle \downarrow^{\Phi1} #4}
\newcommand{\bigsteppr}[4]{\langle #1, #2, #3 \rangle \downarrow^{\Phi2} #4}
\newcommand{\typestep}[3]{#1 \vdash {#2}: #3}
\newcommand{\typestmt}[2]{#1 \vdash {#2}}
% name, requirement, result
\newcommand{\bsrule}[3]{\textsc{#1}\ddfrac{#2}{#3}}
\let\rule\undefined
\newcommand{\rule}[3]{\textsc{#1}\ddfrac{#2}{#3}}
\newcommand{\literalint}[1][n]{\bar{#1}}
\newcommand{\store}[1][\sigma]{#1}
%\newcommand{\clean}[1][n]{#1^\textbf{c}}
%\newcommand{\tracked}[1][n]{#1^\textbf{t}}
%\newcommand{\flagged}[1][n]{#1^f}
\NewDocumentCommand{\clean}{O{n}+o}{#1[\textbf{c}\IfValueT{#2}{_#2}]}
\NewDocumentCommand{\tracked}{O{n}+o}{#1[\textbf{t}\IfValueT{#2}{_#2}]}
\NewDocumentCommand{\flagged}{O{n}+o}{#1[{f}\IfValueT{#2}{_#2}]}
\newcommand{\source}[1]{\textbf{source }{#1}}
\newcommand{\defaultsource}{\source{a}}
\newcommand{\sink}[1]{\textbf{sink }{#1}}
\newcommand{\defaultsink}{\sink{a}}
\newcommand{\excstore}[1][\sigma]{\hat{#1}}
\newcommand{\initialstore}[1][\sigma]{\tilde{#1}}
\newcommand{\Loc}{\textbf{Loc}}
\newcommand{\partialf}{\rightharpoonup} %{\dashrightarrow}
\newcommand{\StoreDomain}{\tilde{Z}}
\newcommand{\exception}{\textbf{abrt}}
\newcommand{\skipcmd}{\textbf{skip}}
\newcommand{\seq}[2]{{#1}; {#2}}
\newcommand{\defaultseq}{\seq{c_0}{c_1}}
\newcommand{\defaultssaseq}{\seq{s_0}{s_1}}
\newcommand{\ifcmd}[3]{\textbf{if }{#1} \textbf{ then }{#2}\textbf{ else }{#3}}
\newcommand{\defaultif}{\ifcmd{b}{c_0}{c_1}}
\newcommand{\btrue}{\textbf{true}}
\newcommand{\bfalse}{\textbf{false}}
\newcommand{\whilecmd}[2]{\textbf{while }{#1} \textbf{ do } {#2}}
\newcommand{\defaultwhile}{\whilecmd{b}{c_0}}
\newcommand{\storecmd}[2]{{#1} \coloneqq {#2}}
\newcommand{\defaultstore}{\storecmd{X}{a}}
\newcommand{\phistore}[3]{{#1} \coloneqq \varphi({#2}, {#3})}
% Do not use anymore!
%\newcommand{\philist}{\Phi_{list}}
\newcommand{\defaultphi}{\phistore{X_1}{Y_1}{Z_1}, \ldots}
\newcommand{\defaultphilist}{[\defaultphi]}
\newcommand{\defaultphilistn}{[\defaultphi, \phistore{X_n}{Y_n}{Z_n}]}
\newcommand{\ssaif}[4]{\textbf{if }{#1} \textbf{ then }{#2}\textbf{ else }{#3};{#4}}
\newcommand{\defaultssaif}{\ssaif{b}{s_0}{s_1}{\defaultphilist}}
\newcommand{\ssawhile}[3]{\textbf{while }#3;{#1} \textbf{ do } {#2}}
\newcommand{\defaultssawhile}{\ssawhile{b}{s_0}{\defaultphilist}}
\newcommand{\dunion}{\sqcup}
\newcommand{\union}{\cup}
\newcommand{\storeassign}[2]{{#1} \dunion [{#2}]}
\newcommand{\leftphi}[1]{X_1 \mapsto {#1}(Y_1), \ldots}
\newcommand{\rightphi}[1]{X_1 \mapsto {#1}(Z_1), \ldots}
\newcommand{\dom}[1]{\text{dom }{#1}}

\subsection{Syntax and Semantic of dIMP}
We extend the syntax of IMP a little bit
and call the resulting language dIMP (for dataflow IMP):
\begin{align*}
    &a \Coloneqq \dots | \defaultsource\\
    &c \Coloneqq \dots | \defaultsink\\
\end{align*}
We define the set $\tilde{\textbf{Z}} =\{\flagged{} | n \in \textbf{Z}, f \in \{\textbf{c}, \textbf{t}\}\}$.
Stores are denoted with $\store \in \Sigma = \Loc \to \tilde{\textbf{Z}}$.
Stores are function from locations to the set of tagged integer
values.
A further restriction on valid programs is that only up to one expression
of the form $\textbf{source } a$ is allowed in a program.
This is a technical restriction, making the proof later easier.

The intuition here is that for all $n \in \textbf{Z}$, 
each value is tagged by either \textbf{c} for clean or \textbf{t} for being tracked
as having dataflow.
A tracked value originates from the source, and is preserved by value-preserving
operations (see the semantics below).
The command $\defaultsink$ is a special marker for the dataflow sink.
It aborts the program if a tracked value reaches the sink.
%This ensures termination for all programs that have dataflow.
In real programs, the dataflow source would likely be a procedure reading input,
i.e.\ from a terminal, and the dataflow sink would likely be a procedure 
printing the data to a terminal, or otherwise communicating with the outside world.


There is a new judgment of the form 
\fbox{$\bigstep{a}{\sigma}{\flagged{}}$}:
\begin{align*}
    &\bsrule{EA-Num}{}{\bigstep{\literalint}{\store}{\clean}} \qquad
    \bsrule{EA-Loc}{}{\bigstep{X}{\store}{\store(X)}}\qquad
    \bsrule{EA-Plus}
    {\bigstep{a_0}{\store}{\flagged[n_0][0]}
    \qquad \bigstep{a_1}{\store}{\flagged[n_1][1]}}
    {\bigstep{a_0+a_1}{\store}{\clean[(n_0+n_1)]}}\\
    &\bsrule{EA-Minus}
    {\bigstep{a_0}{\store}{\flagged[n_0][0]}
    \qquad \bigstep{a_1}{\store}{\flagged[n_1][1]}}
    {\bigstep{a_0-a_1}{\store}{\clean[(n_0-n_1)]}} \qquad
    \bsrule{EA-Times}
    {\bigstep{a_0}{\store}{\flagged[n_0][0]}
    \qquad \bigstep{a_1}{\store}{\flagged[n_1][1]}}
    {\bigstep{a_0 \times a_1}{\store}{\clean[(n_0 \times n_1)]}}\\
    &\bsrule{EA-Source}{\bigstep{a}{\store}{\flagged}}
    {\bigstep{\source}{\store}{\tracked}}
\end{align*}

The Judgment $\langle c, \sigma \rangle \downarrow \sigma'$ is replaced by 
$\bigstep{c}{\sigma}{\excstore'}$
where $\excstore{}: (\Loc \to \StoreDomain) \cup \{\exception\}$.
The state $\exception$ indicates that the program was aborted.
Programs are aborted when a dataflow-tracked value reaches a sink.
\\
Judgement \fbox{$\bigstep{c}{\sigma}{\excstore'}$}:
\begin{align*}
    &\bsrule{EC-SinkAbrt}{\bigstep{a}{\store}{\tracked{}}}
    {\bigstep{\defaultsink}{\store}{\exception}} \qquad
    \bsrule{EC-Sink}{\bigstep{a}{\store}{\clean{}}}
    {\bigstep{\defaultsink}{\store}{\store}} \qquad
    \bsrule{EC-Skip}{}{\bigstep{\skipcmd}{\store}{\store}}\\
    &\bsrule{EC-SeqAbrt}{\bigstep{c_0}{\sigma}{\exception}}
    {\bigstep{\defaultseq}{\store}{\exception}} \qquad
    \bsrule{EC-Seq}{\bigstep{c_0}{\sigma}{\store''} \qquad \bigstep{c_1}{\store''}{\excstore'}}
    {\bigstep{\defaultseq}{\store}{\excstore'}}\\
    &\bsrule{EC-IfT}{\bigstep{b}{\store}{\btrue}\qquad \bigstep{c_0}{\store}{\excstore'}}
    {\bigstep{\defaultif}{\store}{\excstore'}} \qquad
    \bsrule{EC-IfF}{\bigstep{b}{\store}{\bfalse}\qquad \bigstep{c_1}{\store}{\excstore'}}
    {\bigstep{\defaultif}{\store}{\excstore'}}\\
    &\bsrule{EC-WhileF}{\bigstep{b}{\sigma}{\bfalse}}
    {\bigstep{\defaultwhile}{\store}{\store}} \qquad
    \bsrule{EC-WhileTAbrt}{\bigstep{b}{\sigma}{\btrue} \qquad \bigstep{c_0}{\store}{\exception}}
    {\bigstep{\defaultwhile}{\store}{\exception}}\\
    &\bsrule{EC-WhileT}{\bigstep{b}{\sigma}{\btrue} \qquad \bigstep{c_0}{\store}{\store''}\qquad
    \bigstep{\defaultwhile}{\store''}{\excstore'}}
    {\bigstep{\defaultwhile}{\store}{\excstore'}}\\
    &\bsrule{EC-Assign}{\bigstep{a}{\store}{\flagged{}}}
    {\bigstep{\defaultstore}{\store}{\store{}[X \mapsto \flagged{}]}}
\end{align*}

The judgement operations for booleans are modified to ignore the tracked-flag and work
just as in IMP. This is analoguous to how $\textsc{EA-Plus}$ etc.\ work.

\subsection{Syntax of SSA-dIMP}
To facilitate the dataflow analysis, we describe a SSA form on dIMP with a concrete
syntax and semantics.
We do not describe how to transform a dIMP program to a SSA-dIMP program, but 
refer the reader to the literature, where different SSA construction algorithms are
described. (TODO cite). Furthermore, there are correctness proofs of SSA construction
available (TODO).

The syntax for boolean and arithmetic expressions in SSA-dIMP is the same as in dIMP.
However, the command syntax changes.
We now denote commands by $s$ (statement, or SSA-command) to highlight the differences
from the commands denoted by $c$ in dIMP.
\begin{align*}
    s \Coloneqq &\:\skipcmd \mid \defaultsink \mid \defaultstore \mid \defaultssaseq 
    \mid \defaultssaif\\
    &\mid \defaultssawhile% \\
    %\philist \Coloneqq &\:[] \mid \Phi_{node} : \philist\\
    %\Phi_{node} \Coloneqq &\:X \coloneqq \varphi(Y, Z)
\end{align*}
At every point where $\varphi$ nodes can placed in the control flow graph
of a dIMP program, the syntax of SSA-dIMP has an explicit list of $\varphi$ nodes.

A program in SSA-dIMP is only valid if it conforms to the SSA properties.
To enforce that, we describe a simple type system that describes the SSA properties.
Thus, every variable definition dominates all of the uses of the variable.
%, and the number of $\Phi$ nodes in the program is minimal.
The easiest way to achieve this is to run a SSA construction algorithm on a dIMP program.
It is a gap in the provided proofs that we do not show that every dIMP program
can be transformed in a valid SSA-dIMP program (especially, we omit showing that
$\Phi$ nodes will be placed exactly at the places they are allowed in the syntax
of SSA-dIMP).
TODO conjecture

\subsubsection*{The SSA property}
TODO subsection or something
To ensure that a program suffices the SSA property, i.e.\ that every definition dominates its use,
we require sets $\Gamma$, $\Delta$ of variable names fulfilling the following rules:

Judgement \fbox{$\typestmt{\Gamma}{a}$}:
\begin{align*}
    &\rule{SSA-Num}{}{\typestmt{\Gamma}{n}}\qquad
    \rule{SSA-Loc}{}{\typestmt{\Gamma}{X}}\text{($X \in \Gamma$)}\qquad
    \rule{SSA-Source}{\typestmt{\Gamma}{a}}{\typestmt{\Gamma}{\source{a}}}\\
    &\rule{SSA-Plus}{\typestmt{\Gamma}{a_0}\qquad\typestmt{\Gamma}{a_1}}
    {\typestmt{\Gamma}{a_0+a_1}}\quad
    \rule{SSA-Minus}{\typestmt{\Gamma}{a_0}\qquad\typestmt{\Gamma}{a_1}}
    {\typestmt{\Gamma}{a_0-a_1}}\quad
    \rule{SSA-Times}{\typestmt{\Gamma}{a_0}\qquad\typestmt{\Gamma}{a_1}}
    {\typestmt{\Gamma}{a_0 \times a_1}}
\end{align*}
Judgement \fbox{$\typestmt{\Gamma}{b}$}:
\begin{align*}
    &\rule{SSA-Eq}{\typestmt{\Gamma}{a_0}\qquad \typestmt{\Gamma}{a_1}}
    {\typestmt{\Gamma}{a_0 = a_1}}\qquad
    \rule{SSA-Leq}{\typestmt{\Gamma}{a_0}\qquad \typestmt{\Gamma}{a_1}}
    {\typestmt{\Gamma}{a_0 \leq a_1}}\qquad
    \rule{SSA-Neg}{\typestmt{\Gamma}{b}}
    {\typestmt{\Gamma}{\neg b}}\\
    &\rule{SSA-And}{\typestmt{\Gamma}{b_1}\qquad\typestmt{\Gamma}{b_2}}
    {\typestmt{\Gamma}{b_1 \land b_2}}\\
\end{align*}
Judgement \fbox{$\typestep{\Gamma;\Delta_0;\Delta_1}{\defaultphilist}{\Delta}$}:
\begin{align*}
    &\rule{SSA-$\varphi$}{}{\typestep{\Gamma;\Delta_0;\Delta_1}{\defaultphilistn}{\{X_1, \ldots, X_n\}}}\\
    &\text{(if for $i \in \{1, \ldots, n\}: X_i \notin \Gamma \land Y_i \in \Gamma \cup \Delta_0 \land Z_i \in \Gamma \cup \Delta_1
    \land \forall j \neq i: X_j \neq X_i$)}
\end{align*}
Judgement \fbox{$\typestep{\Gamma}{s}{\Delta}$}:
\begin{align*}
    &\rule{SSA-Sink}{\typestmt{\Gamma}{a}}{\typestep{\Gamma}{\sink{a}}{\emptyset}}\qquad
    \rule{SSA-Skip}{}{\typestep{\Gamma}{\skipcmd}{\emptyset}}\\
    &\rule{SSA-Assign}{\typestmt{\Gamma}{a}}{\typestep{\Gamma}{\defaultstore}{\{X\}}} \text{($X \notin \Gamma$)}\\
    &\rule{SSA-Seq}{\typestep{\Gamma}{s_0}{\Delta_0}\qquad
    \typestep{\Gamma \cup \Delta_0}{s_1}{\Delta_1}}{\typestep{\Gamma}{\defaultssaseq}{\Delta_0 \cup \Delta_1}}\\
    &\rule{SSA-If}{\typestmt{\Gamma}{b}\qquad\typestep{\Gamma}{s_0}{\Delta_0}\qquad \typestep{\Gamma}{s_1}{\Delta_1}\qquad
    \typestep{\Gamma;\Delta_0;\Delta_1}{\defaultphilist}{\Delta}}
    {\typestep{\Gamma}{\defaultssaif}{\Delta}}\\
    &\rule{SSA-While}{\typestep{\Gamma;\emptyset;\Delta_1}{\defaultphilist}{\Delta}\qquad
    \typestmt{\Gamma \cup \Delta}{b}\qquad\typestep{\Gamma \cup \Delta}{s_0}{\Delta_1}}
    {\typestep{\Gamma}{\defaultssawhile}{\Delta}}
\end{align*}

Note that by this definition, the first argument of $\varphi$ nodes for \textbf{if} commands
corresponds to the variable after executing the \btrue{} branch, and the second argument to that
of the variable after executing the \bfalse{} branch.
For \textbf{while} commands, the first argument to the $\varphi$ node corresponds to the value
before executing $s_0$ the first time, and the second argument corresponds to the value 
after executing the loop body.
The sets $\Gamma$ and $\Delta$ are natural outputs of an algorithm that converts a 
dIMP program into SSA-dIMP form.
They prove that a (not necessarily minimal) SSA form was indeed achieved.
TODO something about use of initial values, as they cannot have a definition

\subsubsection*{The Semantics of SSA-dIMP}
Stores are now partial functions $\store : \Sigma \partialf \Loc$ to indicate that
variables have to be defined before they can be read.
%Furthermore, when updating a store, we now use the syntax 
%$\store \dunion [X \mapsto \flagged{}]$ to indicate that the store $\store$ did not
%contain the variable $X$ before.

The judgment for booleans and arithmetic expressions does not change.
As before, we use $\excstore{}$ to denote partial functions
$\excstore{}: \Loc (\partialf \StoreDomain) \cup \{\exception\}$.
%where the evaluation of $\philist$ 
%is combined with the variables in the store $\sigma$.
% TODO cleanup
For $\Phi$ nodes, we introduce to judgements - one that evaluates them to the first argument,
and one that evalutes them to the second argument.
\\
Judgment \fbox{$\bigsteppl{\defaultphilist}{\store}{\store''}{\store'}$}:
\begin{align*}
    &\bsrule{E$\Phi_1$-Assign}{}
    {\bigsteppl{\defaultphilistn}{\store}{\store''}{\storeassign{\store}{X_1 \mapsto \store''(Y_1),\ldots X_n \mapsto \store''(Y_n)}}}\\
\end{align*}
Judgment \fbox{$\bigsteppr{\defaultphilist}{\store}{\store''}{\store'}$}:
\begin{align*}
    &\bsrule{E$\Phi_2$-Assign}{}
    {\bigsteppr{\defaultphilistn}{\store}{\store''}{\storeassign{\store}{X_1 \mapsto \store''(Z_1),\ldots X_n \mapsto \store''(Z_n)}}}\\
\end{align*}

\begin{landscape}
With $\excstore{}: (\Loc \partialf \StoreDomain) \cup \{\exception\}$ we have the following rules for the
 judgment \fbox{$\bigstep{c}{\store}{\excstore{}}$}:
\begin{align*}
    &\bsrule{EC-SinkAbrt}{\bigstep{a}{\store}{\tracked{}}}
    {\bigstep{\defaultsink}{\store}{\exception}} \qquad
    \bsrule{EC-Sink}{\bigstep{a}{\store}{\clean{}}}
    {\bigstep{\defaultsink}{\store}{\store}} \qquad
    \bsrule{EC-Skip}{}{\bigstep{\skipcmd}{\store}{\store}}\qquad
    \bsrule{EC-Seq}{\bigstep{s_0}{\sigma}{\store''} \qquad \bigstep{s_1}{\store''}{\excstore'}}
    {\bigstep{\defaultssaseq}{\store}{\excstore'}}\\
    &\bsrule{EC-SeqAbrt}{\bigstep{s_0}{\sigma}{\exception}}
    {\bigstep{\defaultssaseq}{\store}{\exception}} \qquad
    \bsrule{EC-IfTAbrt}{\bigstep{b}{\store}{\btrue}\qquad \bigstep{s_0}{\store}{\exception}}
    {\bigstep{\defaultssaif}{\store}{\exception}}\\
    &\bsrule{EC-IfFAbrt}{\bigstep{b}{\store}{\bfalse}\qquad \bigstep{s_1}{\store}{\exception}}
    {\bigstep{\defaultssaif}{\store}{\exception}}\qquad
    \bsrule{EC-Assign}{\bigstep{a}{\store}{\flagged{}}}
    {\bigstep{\defaultstore}{\store}{\storeassign{\store}{X \mapsto \flagged{}]}}}\\
    &\bsrule{EC-IfT}{\bigstep{b}{\store}{\btrue}\qquad \bigstep{s_0}{\store}{\store''}
    \qquad \bigsteppl{\defaultphilist}{\store}{\store''}{\store'}}
    {\bigstep{\defaultssaif}{\store}{\store'}}\\
    &\bsrule{EC-IfF}{\bigstep{b}{\store}{\bfalse}\qquad \bigstep{s_1}{\store}{\store''}
    \qquad \bigsteppr{\defaultphilist}{\store}{\store''}{\store'}}
    {\bigstep{\defaultssaif}{\store}{\store'}}\\
    &\bsrule{EC-While}{\bigsteppl{\defaultphilist}{\store}{\store}{\store''} \qquad
    \bigstepw{\defaultssawhile}{\store}{\store''}{\excstore'}}
    {\bigstep{\defaultssawhile}{\store}{\excstore'}}\\
\end{align*}

For while loops, we introduce a special judgment of the form \fbox{$\bigstepw{c}{\store}{\store''}{\excstore'{}}$}:
\begin{align*}
    &\bsrule{EW-WhileF}{\bigstep{b}{\store'}{\bfalse}}
    {\bigstepw{\defaultssawhile}{\store}{\store'}{\store'}}\qquad
    \bsrule{EW-WhileTAbrt}{\bigstep{b}{\store'}{\btrue} \qquad 
    \bigstep{s_0}{\store'}{\exception}}
    {\bigstepw{\defaultssawhile}{\store}{\store'}{\exception}}\\
    &\bsrule{EW-WhileT}{\bigstep{b}{\store''}{\btrue} \  
    \bigstep{s_0}{\store''}{\store'''} \  
    \bigsteppr{\defaultphilist}{\store}{\store'''}{\store''''} \ 
    \bigstepw{\defaultssawhile}{\store}{\store''''}{\excstore'}}
    {\bigstepw{\defaultssawhile}{\store}{\store''}{\excstore'}}\\
 \end{align*}
\end{landscape}
In order to correctly model the execution of $\varphi$ nodes for \textbf{while} commands,
a second judgment is set up.
The regular command judmenent only evaluates the first execution of the loop body
(if any), where the variables are populated by the values as defined before the loop.
Then it delegates to a special while-command-judgement that evaluates all other 
loop executions, evaluating the $\varphi$ nodes with the store of the previous loop 
iteration.

\subsubsection*{TODO BLA PRoofs about }
\begin{theorem}
    \label{thm:gamma-delta-disjoint}
    If $\typestep{\Gamma}{s}{\Delta}$ holds, then $\Gamma \cap \Delta = \emptyset$.
\end{theorem}
\begin{proof}
    Let $\D$ be a derivation of $\typestep{\Gamma}{s}{\Delta}$.
    We prove the theorem by induction over the derivation tree.\\
    \textbf{Case 1:}
    \begin{align*}
        \D = \rule{SSA-Skip}{}{\typestep{\Gamma}{\skipcmd}{\emptyset}}
    \end{align*}
    Obvious.\\
    \textbf{Case 2:}
    \begin{align*}
        \D = \rule{SSA-Sink}{\typestmt{\Gamma}{a}}{\typestep{\Gamma}{\defaultsink}{\emptyset}}
    \end{align*}
    Obvious.\\
    \textbf{Case 3:}
    \begin{align*}
        \D = \rule{SSA-Assign}{\typestmt{\Gamma}{a}}{\typestep{\Gamma}{\defaultstore}{\{X\}}}
    \end{align*}
    As the side condition for the rule is $X \notin \Gamma$, it follows immediately that 
    $\Gamma \cap \{X\} = \emptyset$ holds.\\
    \textbf{Case 4:}
    \begin{align*}
        \D = \rule{SSA-Seq}{\overset{\D_0}{\typestep{\Gamma}{s_0}{\Delta_0}}\qquad
        \overset{\D_1}{\typestep{\Gamma \cup \Delta_0}{s_1}{\Delta_1}}}{\typestep{\Gamma}{\defaultssaseq}{\Delta_0 \cup \Delta_1}}
    \end{align*}
    By IH on $\D_0$ we get that $\Gamma \cap \Delta_0 = \emptyset$ and by IH on 
    $\D_1$ we get that $\Gamma \cap \Delta_1 = \emptyset$.
    Thus we have $(\Delta_0 \cup \Delta_1) \cap \Gamma = \emptyset$.\\
    \textbf{Case 5:}
    \begin{align*}
        \D = \rule{SSA-If}{\typestmt{\Gamma}{b}\qquad
        \typestep{\Gamma}{s_0}{\Delta_0}\qquad \typestep{\Gamma}{s_1}{\Delta_1}\qquad
        \typestep{\Gamma;\Delta_0;\Delta_1}{\defaultphilist}{\Delta}}
        {\typestep{\Gamma}{\defaultssaif}{\Delta}}
    \end{align*}
    As is obvious from looking at the side condition of \textsc{SSA-$\varphi$} that requires all
    $X_i \notin \Gamma$, it holds that $\Delta = \{X_1, \ldots, X_n\} \cap \Gamma = \emptyset$.\\
    \textbf{Case 6:}
    \begin{align*}
        \D = \rule{SSA-While}{\typestep{\Gamma;\emptyset;\Delta_1}{\defaultphilist}{\Delta}\qquad
        \typestmt{\Gamma \cup \Delta}{b}\qquad\typestep{\Gamma \cup \Delta}{s_0}{\Delta_1}}
        {\typestep{\Gamma}{\defaultssawhile}{\Delta}}
    \end{align*}
        We conclude $\Gamma \cap \Delta = \emptyset$ by the same argument as in case 5.
\end{proof}

\begin{theorem}
    If $\typestep{\Gamma}{s}{\Delta}$, $\text{dom } \store = \Gamma$ and
    $\bigstep{s}{\store}{\store'}$, then $\store' = \store \dunion \store_0$
    and $\dom{\store_0} = \Delta$.
\end{theorem}
\begin{proof}
    Let $\D$ be a derivation of $\typestep{\Gamma}{s}{\Delta}$.
    Let $\E$ be a derivation of $\bigstep{s}{\store}{\store'}$.
    We prove the theorem by induction over the derivation tree.\\
    \textbf{Case 1:}
    \begin{align*}
        \D = \rule{SSA-Skip}{}{\typestep{\Gamma}{\skipcmd}{\emptyset}}
    \end{align*}
    Then we have $s = \skipcmd$,
    \begin{align*}
        \E = \rule{EC-Skip}{}{\bigstep{\skipcmd}{\store}{\store}}
    \end{align*}
    $\store' = \store$ and $\dom{\store_0} = \emptyset$.\\
    \textbf{Case 2:}
    \begin{align*}
        \D = \rule{SSA-Sink}{\typestmt{\Gamma}{a}}{\typestep{\Gamma}{\defaultsink}{\emptyset}}
    \end{align*}
    Then we have $s = \defaultsink$,
    \begin{align*}
        \E =     \bsrule{EC-Sink}{\bigstep{a}{\store}{\clean{}}}
        {\bigstep{\defaultsink}{\store}{\store}}    
    \end{align*}
    $\store' = \store$ and $\dom{\store_0} = \emptyset$.\\
    \textbf{Case 3:}
    \begin{align*}
        \D = \rule{SSA-Assign}{\typestmt{\Gamma}{a}}{\typestep{\Gamma}{\defaultstore}{\{X\}}}
    \end{align*}
    Then we have $s = \defaultstore$ and
    \begin{align*}
        \E = \bsrule{EC-Assign}{\bigstep{a}{\store}{\flagged{}}}
        {\bigstep{\defaultstore}{\store}{\storeassign{\store}{X \mapsto \flagged{}]}}}
    \end{align*}
    Furthermore, we obviously get $\dom{\store_0} = \{X\} = \Delta$.
    As the side-condition for \textsc{SSA-Assign} specifies that $X \notin \Gamma$
    and we have $\dom{\store} = \Gamma$ as assumption, we also have $X \notin \dom{\store}$,
    so the union is disjoint as postulated.\\
    \textbf{Case 4:}
    \begin{align*}
        \D = \rule{SSA-Seq}{\overset{\D_0}{\typestep{\Gamma}{s_0}{\Delta_0}}\qquad
        \overset{\D_1}{\typestep{\Gamma \cup \Delta_0}{s_1}{\Delta_1}}}{\typestep{\Gamma}{\defaultssaseq}{\Delta_0 \cup \Delta_1}}
    \end{align*}
    Then we have $s = \defaultssaseq$ and
    \begin{align*}
        \E =\bsrule{EC-Seq}{\overset{\E_0}{\bigstep{s_0}{\sigma}{\store''}} \qquad 
        \overset{\E_1}{\bigstep{s_1}{\store''}{\store'}}}
        {\bigstep{\defaultssaseq}{\store}{\store'}}
    \end{align*}
    By IH on $\D_0$ with $\E_0$ we get that $\sigma'' = \sigma \dunion \tilde{\store}''$ 
    with $\dom{\tilde{\store}''} = \Delta_0$.
    Thus, $\dom{\sigma''} = \Gamma \union \Delta_0$.
    Then, by IH on $\D_1$ with $\E_1$ we get that
    $$\sigma' = \sigma'' \dunion \tilde{\store}' = \sigma \dunion (\tilde{\store}'' \dunion \tilde{\store}')$$
    and $\dom{\tilde{\store}'} = \Delta_1$.
    Thus, we have $\sigma_0 = \tilde{\store}'' \dunion \tilde{\store}'$
    and $\dom{\sigma_0} = \Delta_0 \union \Delta_1$ as required.
    \autoref{thm:gamma-delta-disjoint} implies that all the unions are disjoint.\\
    \textbf{Case 5:}
    \begin{align*}
        \D = \rule{SSA-If}{\typestmt{\Gamma}{b}\qquad
        \typestep{\Gamma}{s_0}{\Delta_0}\qquad \typestep{\Gamma}{s_1}{\Delta_1}\qquad
        \typestep{\Gamma;\Delta_0;\Delta_1}{\defaultphilist}{\Delta}}
        {\typestep{\Gamma}{\defaultssaif}{\Delta}}
    \end{align*}
    Then we have $s = \defaultssaif$ and
    two subcases.\\
    \textbf{Subcase 1}:
    \begin{align*}
        \E = \rule{SSA-IfT}{\bigstep{b}{\store}{\btrue}\qquad \bigstep{s_0}{\store}{\store''}
        \qquad \overset{\E_0}{\bigsteppl{\defaultphilist}{\store}{\sigma''}{\sigma'}}}
        {\bigstep{\defaultssaif}{\store}{\store'}}
    \end{align*}
    Because there is only one rule for $\E_0$, we know that 
    $\store' = \store \dunion [X_i \mapsto \sigma''(Y_i)] = \store \dunion \sigma_0$ .
    Thus $\dom{\store'} = \dom{\store{}} \dunion \{X_1, \ldots, \X_n\}$.
    By the condition on \textsc{SSA-If} that $X_i \notin \Gamma$, we get that the union
    is disjoint.
    Furthermore note that $\Delta = \{X_1, \ldots, X_n\}$, proving $\dom{\store_0} = \Delta$.\\
    \textbf{Subcase 2}:
    \begin{align*}
        \E = \rule{SSA-IfF}{\bigstep{b}{\store}{\bfalse}\qquad \bigstep{s_1}{\store}{\store''}
        \qquad \overset{\E_0}{\bigsteppr{\defaultphilist}{\store}{\sigma''}{\sigma'}}}
        {\bigstep{\defaultssaif}{\store}{\store'}}
    \end{align*}
    This case is fully analoguous to subcase 1.\\
    \textbf{Case 6:}
    TODO add lemma about EW rules!
    \begin{align*}
        \D = \rule{SSA-While}{\typestep{\Gamma;\emptyset;\Delta_1}{\defaultphilist}{\Delta}\qquad
        \typestmt{\Gamma \cup \Delta}{b}\qquad\typestep{\Gamma \cup \Delta}{s_0}{\Delta_1}}
        {\typestep{\Gamma}{\defaultssawhile}{\Delta}}
    \end{align*}
    Then we have $s = \defaultssawhile$ and
    \begin{align*}
        \E = \bsrule{EC-While}{\bigsteppl{\defaultphilist}{\store}{\store}{\store''} \ 
        \overset{\E_0}{\bigstepw{\defaultssawhile}{\store}{\store''}{\store'}}}
        {\bigstep{\defaultssawhile}{\store}{\store'}}
    \end{align*}
    \textbf{Subcase 1:}
    We have $\E_0$ using \textsc{EW-WhileF}.
    Then $\store' = \store''$ and by an argument like in case 5 we prove the theorem,
    as we only need to reason about the $\varphi$ node assignment.\\
    \textbf{Subcase 2:}
    We have
    \begin{align*}
        \E_0 = \bsrule{EW-WhileT}{\bigstep{b}{\store''}{\btrue} \  
        \bigstep{s_0}{\store''}{\store'''} \  
        \bigsteppr{\defaultphilist}{\store}{\store'''}{\store''''} \ 
        \bigstepw{\defaultssawhile}{\store}{\store''''}{\store'}}
        {\bigstepw{\defaultssawhile}{\store}{\store''}{\store'}}
    \end{align*}

    TODO
\end{proof}
\subsection{Definition of Dataflow}
\begin{definition}[Program]
    A \emph{program} in dIMP is a command $c$.
    Complex programs are expressed by using the recursive nature of
    the definition of commands.
    Any program in dIMP can be transformed to be a program in SSA-dIMP.
\end{definition}

\begin{definition}[Initial Store]
    An \emph{initial store} is a store $\initialstore$ s.t.\ 
    $\forall X: \exists n^c: \initialstore(X) = \clean{}$.
    This means that in an initial store all values are flagged as being clean.
    We will implicitly denote inital stores by $\initialstore$.
\end{definition}

\begin{definition}[Dataflow]
    A tuple $(c, \initialstore)$ of a program $c$ and initial store $\initialstore{}$ 
    has \emph{dataflow from the source to a sink} if
    $\bigstep{c}{\initialstore{}}{\exception}$ holds.
    This means that the program aborts.
\end{definition}

\begin{definition}[Dataflow Algorithm]
    A \emph{dataflow algorithm} $\A(c)$ computes, given a program $c$,
    if there exists an initial store $\initialstore{}$ 
    such that $(c, \initialstore)$ has dataflow from the source to a sink.
\end{definition}

\begin{definition}[Soundness]
    A dataflow algorithm is \emph{sound} if for any tuple $(c, \initialstore)$ that
    has dataflow it holds that $\A(c) = \text{HAS\_FLOW}$.
\end{definition}

\begin{definition}[Completeness]
    A dataflow algorithm is \emph{complete} if $\A(c) = \text{HAS\_FLOW}$
    implies that there exists an initial store $\initialstore$
     such that $(c, \initialstore)$ has dataflow.
\end{definition}
\begin{definition}[False Positive]
    A program $c$ for which there exists no initial store $\initialstore$ such that 
    $(c, \initialstore)$ has dataflow, but for which $\A(c) = \text{HAS\_FLOW}$ holds
    is called a \emph{false positive} of the algorithm.
\end{definition}
\begin{remark}
    In general, it is impossible to construct a dataflow algorithm that is both 
    sound and complete.
    In practice, a dataflow algorithm will be neither sound nor complete.
    However, in the theoretical setting of this chapter, we are interested in 
    sound dataflow algorithms.
    The \emph{trivially sound dataflow algorithm} $\A_0(c) = \text{HAS\_FLOW}$ 
    is sound by definition, but not very interesting.
    We will not consider it further, but it is interesting to keep in mind,
    because it shows that just proving that a dataflow algorithm is sound does not
    mean it is useful.    
\end{remark}

\subsection{The Dataflow Graph}
To aid the construction of dataflow algorithms, we define the concept
of a dataflow graph.
The dataflow graph is computed upon an SSA form of the program.

\begin{definition}[Dataflow Graph]
    The \emph{node set} $V$ of a \emph{dataflow graph} $G(c)$
    is defined on the syntactic structure
    of the SSA-dIMP program $c$. It is a subset of the node set of the abstract syntax tree
    (every node knows about its location in the program text).
    Every (sub-)command of $c$, as well as any arithmetic expression $a$ in the 
    program is a node in the dataflow graph.
    Furthermore, every $\Phi$-node in the program is also a node in the dataflow
    graph.
    For example for
    $c = \textbf{skip}; (\textbf{skip}; x \coloneqq \bar{4} + y)$
    the node set consists of $c$ itself, the first \textbf{skip}, the command 
    $\textbf{skip}; x \coloneqq \bar{4} + y$, the second \textbf{skip} (that is different 
    from the first because it appears in another location), the command
    $x \coloneqq \bar{4} + y$, and the arithmetic expressions $\bar{4}+y$,
    $\bar{4}$ and $y$.

    We call a graph $G(c)$ a \emph{dataflow graph} for $c$ if, should an initial store
    $\initialstore{}$ exist such that $(c, \initialstore{})$ has dataflow,
    there exists a path from a node of the form $\defaultsource$ to a node of 
    the form $\defaultsink$ in $G$.
\end{definition}

\begin{definition}[Dataflow Algorithm]
    We define the algorithm $\A_{G}(c)$ that takes a dataflow graph $G(c)$
    and outputs HAS\_FLOW if there exists a path from a node of the form \defaultsource
    to a node of the form \defaultsink, and NO\_FLOW otherwise.
\end{definition}
\begin{remark}
    By the definition of a dataflow graph, $\A_G(c)$ is a sound dataflow algorithm.
\end{remark}

\subsection{A Dataflow Algorithm}
In this section we describe an algorithm $\B(c) = G$ that computes a 
graph $G$ for $c$. We then prove that this algorithm indeed
computes a dataflow graph and thus $\A_G(c)$ is a sound dataflow algorithm.

\subsubsection*{The Algorithm}
Given a program $c$ in SSA-dIMP, the algorithm computes
the edge set $E$ as a least fixed point iteration.
The following recursive rules are used:\\
\textbf{Base Case:} Start at an expression $\textbf{source }a_0$.
If it is used in either the context $X \coloneqq \textbf{source }a_0$
or $\textbf{sink }(\textbf{source }a_0)$ then add an edge from the node for the
source to the node of the store/sink.\\
\textbf{Recursive Case 1:} For any store node $X \coloneqq a_0$ or
$\Phi$ node $\phistore{X}{Y}{Z}$ with an incoming edge
in the dataflow graph, enumerate all uses (traverse
the def-use-chain of the SSA form). Add edges from the store (or $\Phi$-node)
to all reads of $X$.
Reads are either arithmetic expressions reading from that variable,
or $\Phi$-nodes where one of the two possible variables is $X$.\\
\textbf{Recursive Case 2:} For any variable read $Y$ with an incoming edge that appears in an 
assignment, i.e. in $X \coloneqq Y$, add an edge from $Y$ to the node of the assignment.
\textbf{Recursive Case 3:} For any sink node of the form $\sink{X}$,
where the node for the variable read $X$ has an incoming edge, connect 
the read of $X$ with an edge to the node of the sink.

The fixed point iteration stops eventually as the set of possible edges is
finite. Furthermore, if the program doesn't contain a \textbf{source} expression,
the edge set will be empty.

%TODO: Add a second pass, like in QL, that starts at the sink nodes and thus
%filters out edges that don't lead to any sink.
\iffalse
\subsubsection*{Soundness Proof}
\begin{theorem}
    \label{thm:df-soundness}
    The algorithm $\B(c) = G$ computes a dataflow graph, and thus 
    $\A_G(c)$ is a sound dataflow algorithm.
\end{theorem}

\begin{definition}[Store-Read Chain]
    {\color{red}TODO fix all the store indices to not overlap unless needed}
    Let $(c, \initialstore{})$ be a tuple that has dataflow.
    Let $\E$ be the bigstep derivation of
    $\bigstep{c}{\initialstore{}}{\exception}$.
    A \emph{store-read chain} in $\E$ is a list of $m$ tuples of the form
    \begin{equation*}
        (\E^1_i, \E^2_i)
    \end{equation*}
    with $i \in \{1, \ldots, m\}$.
    $\E^0_i$, $\E^1_i$, $\E^2_i$ and $\E^3_i$ are subderivations of $\E$.
    Furthermore, we require that the $\E^1_i$ are of one of the following forms:\\
    \textbf{Case 1.1:}\\
    \begin{align*}
        \E^1_i=\bsrule{EC-Assign}
        {\bigstepderiv{\E^0_i}{a_i}{\sigma_i}{\tracked{}}}
        {\bigstep{\storecmd{X_i}{a_i}}{\store_i}
        {\store_i \dunion [X_i \mapsto \tracked{}]}}
    \end{align*}
    \textbf{Case 1.2:}\\
    \begin{align*}
        \E^1_i=\bsrule{E$\Phi_1$-Assign}
        {\overset{\E^0_i}{\bigsteppl{\defaultphilist}{\store_i}{\store_i''}{\store_i'}}}
        {\bigsteppl{\phistore{X_i}{Y_i}{Z_i} : \defaultphilist}{\store_i}{\store_i''}{\storeassign{\store_i'}{X_i \mapsto \store_i''(Y_i)}}}\\
    \end{align*} and $\store_i''(Y_i) = \tracked{}$.\\
     \textbf{Case 1.3:}\\
     \begin{align*}
        \E^1_i=\bsrule{E$\Phi_2$-Assign}
        {\overset{\E^0_i}{\bigsteppr{\defaultphilist}{\store}{\store''}{\store'}}}
        {\bigsteppr{\phistore{X_i}{Y_i}{Z_i} : \defaultphilist}{\store}{\store''}{\storeassign{\store'}{X_i \mapsto \store''(Z_i)}}}\\
    \end{align*} and $\store''(Z_i) = \tracked{}$.\\
    We require that the derivations $\E^2_i$ are of one of the following forms:\\
    \textbf{Case 2.1:}\\
    \begin{align*}
        \E^2_i=\bsrule{EA-Loc}{}
        {\bigstep{X_i}{\store_i'}{\store_i'(X_i) = \tracked{}}}
    \end{align*}\\
    \textbf{Case 2.2:}\\
    \begin{align*}
        \E^2_i=\bsrule{E$\Phi_1$-Assign}
        {\overset{\E^3_i}{\bigsteppl{\defaultphilist}{\store}{\store''}{\store'}}}
        {\bigsteppl{\phistore{X_{i+1}}{X_i}{Z_{i+1}} : \defaultphilist}{\store}{\store''}{\storeassign{\store'}{X_{i+1} \mapsto \store''(X_i)}}}\\
    \end{align*} and $\store''(X_i) = \tracked{}$.\\
    \textbf{Case 2.3:}\\
    \begin{align*}
        \E^2_i=\bsrule{E$\Phi_2$-Assign}
        {\overset{\E^3_i}{\bigsteppr{\defaultphilist}{\store}{\store''}{\store'}}}
        {\bigsteppr{\phistore{X_{i+1}}{Y_{i+1}}{X_i} : \defaultphilist}{\store}{\store''}{\storeassign{\store'}{X_{i+1} \mapsto \store''(X_i)}}}\\
    \end{align*} and $\store''(X_i) = \tracked{}$.

    Thus, the first element in the tuple is the derivation of a store into the variable
    $X_i$, and the second element in the tuple is the derivation of some read of the 
    variable $X_i$.
    Furthermore, we require that the tuples are connected in the following way:
    
    For $i \in \{1, \ldots, m-1\}$ the read of $X_i$ in $\E^2_i$ is used in the store
    $\E^1_{i+1}$.
    Thus, if $\E^2_i$ uses \textsc{EA-Loc} (case 2.1), we require that $\E^1_{i+1}$ uses case 1.1,
    and that $\E^0_{i+1} = \E^2_i$.
    If $\E^2_i$ uses case 2.2 or 2.3 (i.e. the read of $X_i$ is in a $\Phi$-node), we require
    $\E^2_i = \E^1_{i+1}$.
    This is because the \textsc{E$\Phi$-Assign} rules represents both a read and a store
    in the same rule.
\end{definition}

\begin{definition}[Minimal Store-Read-Chain]
    %Let $m$ be the length of a store-read-chain.
    We define a partial order on store-read chains.
    Let $C_1 = [(\E^1_i, \E^2_i)]$ and $C_2 = [(\tilde{\E}^1_j, \tilde{\E}^2_j)]$ for $i \in \{1, \ldots, m\}$ and
    $j \in \{1, \ldots, \tilde{m}\}$ be two store-read-chains.
    Two store-read-chains are only comparable if they start with the same store and 
    end in the same read (i.e. $\E^1_0 = \tilde{\E}^1_0$ and $\E^2_m = \tilde{\E}^2_{\tilde{m}}$).
    Then $C_1 \leq C_2$ iff $m \leq \tilde{m}$, i.e.\ the store-read-chains are ordered by length.
    This is obviously a partial order on store-read chains.
    As lengths of store-read-chains are positive integers, a minimal element always exist.
\end{definition}

\begin{lemma}
    Let $(c, \initialstore{})$ be a tuple that has dataflow.
    Let $G$ be the graph computed by $\B$.
    Then for every minimal store-read-chain in $(c, \initialstore{})$ where $\E^1_1$ is a derivation of
    the expression $X_1 \coloneqq \source{a_0}$,
    there is a path from in $G$ from the first store to the last read in the chain.
    \label{lem:store-read-chain-path}
\end{lemma}
\begin{proof}
    We prove the lemma by induction over $m$.
    In the base case we have $m=1$, so there is one tuple in the store-read-chain.
    We know that the store is of type 1.1. By the recursive case 1 of the algorithm,
    there is an edge from the store to the read in $G$, as by the base case of the algorithm, there
    is an incoming edge from the source to the store.

    \emph{Inductive Case:} Let $C$ be a minimal store-read-chain of length $m+1$.
    By the induction hypothesis, we get a path from the first store to the read 
    with derivation $\E^2_m$.
    We continue with a case distinction to show that there is an edge from the read 
    represented by $\E^2_m$ to the store represented by $\E^1_{m+1}$:
    
    If $\E^2_m$ is of type 2.1, then we know that $\E^1_{m+1}$ is of type 1.1 and $\E^0_{m+1} = \E^2_m$ holds.
    Furthermore, by the induction hypothesis the read represented by $\E^2_m$ has an incoming edge.
    Thus, by $\E^0_{m+1} = \E^2_m$ and by the recursive case 2 of the algorithm, we have 
    an edge from the variable read to the store.
    If $\E^2_m$ is of type 2.2 or 2.3, we know that $\E^2_m = \E^1_{m+1}$ by definition.
    Thus, no edge from the store to the read is needed in $G$, as they are represented by the same node.

    Furthermore, we have an edge from the store represented by $\E^1_{m+1}$ to the read represented by 
    the derivation $\E^2_{m+1}$ by recursive case 1 of the algorithm,
    as the recursive case traverses all uses of the store $\E^1_{m+1}$, and either it is a regular variable read 
    (then $\E^2_{m+1}$ is of type 2.1), or it is $\Phi$-node (then $\E^2_{m+1}$ is of type 2.2 or 2.3).

    This concludes the proof of the lemma.
\end{proof}

\begin{lemma}
    Let $(c, \initialstore{})$ be a tuple that has dataflow.
    Then there exists a store-read-chain 
    from every expression of form $X \coloneqq \source{a_0}$ to every 
    derivation of form $\bsrule{EA-Loc}{}{\bigstep{Y}{\sigma}{\tracked{}}}$.
    \label{lem:store-read-chain-existence}
\end{lemma}
\begin{proof}
    TODO - statement is in my opinion quite obvious, a formal argument needs to be provided nonetheless.
    An inductive proof providing a construction of a (minimal) store-read-chain should be doable.
\end{proof}
    
\begin{proof}[Proof of~\autoref{thm:df-soundness}]
    Because $(c, \initialstore{})$ has dataflow, the program contains a $\sink{}$
    command that evaluates to $\exception$.
    We prove the theorem by case distinction over the structure of that $\sink{}$
    command.\\
    \textbf{Case 1:}
    The $\sink{}$ has form $\sink{(\source{ a_0})}$.
    Then, by the base case of the algorithm there is a path from a source node
    to a sink node in the graph.\\
    \textbf{Case 2:}
    The $\sink{}$ has form $\sink{X}$.
    As we know that $\bigstep{\sink{X}}{\sigma}{\exception}$ holds, we know that
    we have a bigstep derivation
    \begin{align*}
        \bsrule{EC-SinkAbrt}
        {\bsrule{EA-Loc}{}{\bigstep{X}{\sigma}{\tracked{}}}}
        {\bigstep{\sink{X}}{\sigma}{\exception}}
    \end{align*}
    Because $\sigma(X) = \tracked{}$, there exists a $\source{a_0}$ somewhere in the program.
    By~\autoref{lem:store-read-chain-existence} we get a (minimal) store-read-chain from the source 
    to the read of the variable in the sink.
    Thus, by the\~autoref{lem:store-read-chain-path} and the base case of the algorithm, 
    we get a path in $G$ from the source to the variable read.
    By the recursive case 3, we get an edge from the variable read to the sink.
    Thus, we have a path from a source to the sink
    This concludes the proof, as there are no other forms a sink command evaluating 
    to $\exception$ can take.
\end{proof}
\fi
\iffalse
\subsection{IGNORE: The Dataflow Graph}
To aid the construction of dataflow algorithms, we define the concept
of a dataflow graph.
Then we show that an algorithm that outputs HAS\_FLOW if 
there exists a path from the source 
node to a sink node in a dataflow graph %and NO\_FLOW otherwise
is a sound dataflow algorithm.
Furthermore, we introduce an algorithm to compute a dataflow graph.

The dataflow graph is computed upon an SSA form of the program.
Thus, it may contain assignments of the form $X \coloneqq \phi(a_1, a_2)$.
We will assume that for any given store $\sigma$ the derivation only contains 
the assignment with an evaluated $\phi$ function, as to not overload the notation.
Furthermore, the dataflow graph defined on an SSA form of the program can easily 
be transformed to a dataflow graph defined on the regular program.
The minimal dataflow graph is directed and may, depending on the 
program structure, contain cycles.

The \emph{node set} $V$ of the \emph{minimal dataflow graph} $G_\text{min}(c)$
is defined on the syntactic structure
of the SSA form of $c$. It is a subset of the node set of the abstract syntax tree
(every node knows about its location in the program text).
Every (sub-)command of $c$, as well as any arithmetic expression $a$ in the 
program is a node in the dataflow graph.
For example for
$c = \textbf{skip}; (\textbf{skip}; x \coloneqq \bar{4} + y)$
the node set consists of $c$ itself, the first \textbf{skip}, the command 
$\textbf{skip}; x \coloneqq \bar{4} + y$, the second \textbf{skip} (that is different 
from the first because it appears in another location), the command
$x \coloneqq \bar{4} + y$, and the arithmetic expressions $\bar{4}+y$,
$\bar{4}$ and $y$.


For every initial store $\sigma_0$, 
let $\E$ be a bigstep derivation of $\langle c, \sigma_0 \rangle \downarrow \sigma'$.
The edge set $E^0_{\sigma_0}$ contains the following edges:
\begin{enumerate}
    \item $\E$ contains a subderivation
    \begin{equation*}
        \E_1 = \textsc{EC-Assign1}\ddfrac{\overset{\E'}{\langle a, \sigma \rangle \downarrow \top_n}}
        {\langle X \coloneqq a, \sigma \rangle \downarrow
         \sigma[X \mapsto \top_n]}
    \end{equation*}
    Then there is an edge from $a$ to the node of $X \coloneqq a$.
    This case includes both outgoing edges from sources and variable reads
    of locations to which a tracked value was written to.
    \item $\E$ contains a subderivation
    \begin{equation*}
        \E_1 = \textsc{EC-Sink1}\ddfrac{\overset{\E'}{\langle a, \sigma \rangle \downarrow \top_n}}
        {\langle \textbf{sink }a, \sigma \rangle \downarrow
         \sigma}
    \end{equation*}
    Then there is an edge from $a$ to the node of $\textbf{sink }a$.
    \item
    $\E$ contains a subderivation
    \begin{equation*}
        \E_1 = \textsc{EA-Loc}\ddfrac{}
        {\langle X, \sigma \rangle \downarrow \top_n}
    \end{equation*}
    and a subderivation
    \begin{equation*}
        \E_2 = \textsc{EC-Assign1}\ddfrac{\overset{\E'}{\langle a, \sigma' \rangle \downarrow \top_n}}
        {\langle X \coloneqq a, \sigma' \rangle \downarrow
         \sigma'[X \mapsto \top_n]}
    \end{equation*}
    with $\sigma'(X) \neq \top_n$ (this prevents cyclic edges).
    Then there is an edge from the store $X \coloneqq a$ to the read node $X$ as 
    refered to by $\E_1$.\\
    Technical note 1: Here we exploit the fact that there is only one source ---
    else this case could introduce edges between unrelated paths if two sources for the same 
    $\sigma_0$ would evaluate to $\top_n$. Then the edge set would not be minimal
    anymore.\\
    Technical note 2: Because the program is in SSA, edges from a store to a read 
    are in execution-order. A read has an incoming edge from a store only if the 
    store happens before the read in the program.
\end{enumerate}

Based on that we define the (minimized) edge set $E^1_{\sigma_0}$
that contains an edge from $E^0_{\sigma_0}$ iff it is part of any path 
from a node of the form $\textbf{source }a_0$
to a node of the form $\textbf{sink }a_1$.

Then we define the \emph{edge set} $E$ of the minimal dataflow graph as
\begin{equation*}
    E = \bigcup_{\sigma_0} E^1_{\sigma_0}
\end{equation*}
Even though the union is over an infinite set, the result is finite
as $E \subseteq V \times V$ and $|V| < \infty$.

We call a graph a \emph{dataflow graph} if it has the node set as the minimal
dataflow graph and its edge set contains the edges of the minimal dataflow 
graph.

Let $\A_{G_\text{min}}(c)$ be the algorithm that takes the
minimal dataflow graph $G_\text{min}(c)$ and outputs HAS\_FLOW if there 
exists a node of the form 
$\textbf{source }a$ and a node $\textbf{sink }a$ such that there is a path in 
$G_\text{min}(c)$ from the source node to the sink node.
If no such path exists, it outputs NO\_FLOW.


\begin{theorem}
    \label{thm:min-dg}
    The algorithm $\A_{G_\text{min}}(c)$ is a sound dataflow algorithm.
\end{theorem}
In order to prove the theorem, we introduce the concept of store-read chains.

\begin{definition}[Store-Read Chain]
    Let $(c, \sigma_0)$ be a tuple that has dataflow.
    Let $\E$ be the bigstep derivation of
    $\langle c, \sigma_0 \rangle \downarrow \sigma'$.
    A \emph{store-read chain} in $\E$ is a list of $m$ tuples of the form
    $(\E^1_i=\ddfrac{\overset{\E^0_i}{\langle a_i, \sigma_i \rangle \downarrow \top_n}}
    {\langle X_i \coloneqq a_i, \sigma_i \rangle \downarrow \sigma_i[X_i \mapsto \top_n]},
    \E^2_i=\ddfrac{}{\langle X_i, \sigma_i' \rangle \downarrow \sigma_i'(X_i) = \top_n})$
    with $i \in \{1, \ldots, m\}$.
    $\E^0_i$, $\E^1_i$ and $\E^2_i$ are subderivations of $\E$.
    We require that the following holds:
    \begin{enumerate}
        \item $\sigma_i(X) \neq \top_n$ (so we have a first store). 
        \item For $i \in \{1, \ldots, m-1\}$ it holds that 
        $\E^0_{i+1}=\E^2_i$. This means that every store except the first one
        in the chain is connected via the variable it reads from to the previous 
        store in the chain.
        \item Repeating tuples are not allowed in the list, all tuples have to 
        be pairwise disjoint --- equality on subderivations of $\E$ includes
        placement in $\E$, not just equality of the subderivation trees.
    \end{enumerate}
\end{definition}

\begin{remark}
    By definition a read-store-chain does not contain any cycles.
    As the derivation $\E$ is finite, this implies that the set of 
    read-store chains is finite.
    By defining the obvious partial order induced by inclusion
    on the set of read-store-chains, we see that 
    every non-empty read-store-chain can be extended to a maximal 
    read-store-chain.
\end{remark}

\begin{lemma}
    \label{lem:chain-to-graph}
    Any store-read-chain induces a path in $E^0_{\sigma_0}$ from the node 
    of the first store in the chain to the node of the last read in the chain.
    Furthermore, there is an edge from the expression that the first store reads
    to the first store.
\end{lemma}
\begin{proof}
    We proof the lemma by induction over the length of the store-read chain.\\
    \textbf{Base Case (m=1):}
    There is by definition of $\E^0_{\sigma_0}$
    one edge of type (3) connecting the store and the read in the chain.
    Furthermore, there is an edge of type (1) connecting the read-from expression
    to the store.\\
    \textbf{Inductive Case:}
    Let $m \geq 1$ be fixed. By the IH, there is a path in $E^0_{\sigma_0}$
    corresponding to the chain of length $m-1$ from the first store in the chain 
    to the read of $X_{m-1}$ in the chain.
    By definition of the chain, we have an edge of type (1) connecting 
    the read of $X_{m-1}$ to the store of $X_m$.
    Furthermore, again by definition of $\E^0_{\sigma_0}$
    there is one edge of type (3) connecting the store of $X_m$ to the
    read of $X_m$.
\end{proof}

\begin{lemma}
    \label{lem:max-store-read}
    Any maximal store-read-chain contains a store from the source as first store.
\end{lemma}
\begin{proof}
    We look at the derivation $\E^0_1$.
    If $a = Y$ for some $Y \in \textbf{Loc}$, then the store-read-chain is not 
    maximal, because it can be extended at the front by a tuple with a store to and a
    read from $Y$.
    The only other possibility for an arithmetic expression $a$ to evaluate to 
    $\top_n$ is that it is of form $\textbf{source } a_0$.
    This concludes the proof.
\end{proof}


\begin{proof}[Proof Of~\autoref{thm:min-dg}]
    Let $\sigma_0$ be an initial store such that $(c, \sigma_0)$ has dataflow.
    If no such $\sigma_0$ exists, by the definition of soundness
    the proof is concluded.

    Let $\E$ be the bigstep derivation of
    $\langle c, \sigma_0 \rangle \downarrow \sigma'$.
    Because $(c, \sigma_0)$ has dataflow, there exists a derivation of the form 
    \begin{equation*}
        \E_1 = \textsc{EC-Sink1}\ddfrac
        {\overset{\E_2}{\langle a, \sigma \rangle \downarrow \top_n}}
        {\langle \textbf{sink } a, \sigma} \downarrow \sigma
    \end{equation*}
    There are two possibilities for the structure of a.\\
    \textbf{Case 1:} $a = \textbf{source }a_0$\\
    Then $E^0_{\sigma_0}$ contains an edge of type (2) that connects the source 
    to a sink. This constitues a path from the source to a sink.\\
    \textbf{Case 2:} $a = X$\\
    Then $X$ is a variable read that evaluates to $\top_n$.
    Because $\sigma_0(X) \neq \top_n$, there has to be a first store
    of $X$ in $\E$.
    The derivation of that store, together with the derivation of the read make 
    up a store-read chain of length one.
    This chain can be extended to a maximal chain, and 
    by~\autoref{lem:max-store-read}, we know it starts with a store reading the
    source.
    Furthermore, by~\autoref{lem:chain-to-graph} we get a path in $E^0_{\sigma_0}$
    from the source node to the read. The read is then connected by an edge
    of type (2) to the sink, thus we have a path from the source to a sink in 
    $E^0_{\sigma_0}$.

    In both cases we have shown that $E^0_{\sigma_0}$ contains a path from the 
    source to a sink. Thus, $E^1_{\sigma_0}$ and in turn $E$ contain a path
    from the source to a sink, and $\A_{G_\text{min}}(c) = \text{HAS\_FLOW}$.
    This concludes the proof.
\end{proof}

\begin{corollary}
    \label{cor:dg-sound}
    For any dataflow graph $G(c)$ the algorithm $\A_{G}(c)$ is a sound
     dataflow algorithm.
\end{corollary}
\begin{proof}
    Remember that any dataflow graph contains the minimal dataflow graph as 
    subgraph. Thus for every pair $(c, \sigma_0)$ that has dataflow
    there is a path from the source node to a sink node, 
    thus $\A$ reports flow by~\autoref{thm:min-dg}.
    It doesn't matter if $G$ has more edges for soundness.
\end{proof}

\begin{remark}
    The \emph{minimal} dataflow graph gets name by being the minimal dataflow graph
    among all dataflow graphs that are considered in this paper.
    It is not the smallest graph $G$ on the node set 
    such that $\A_G(c)$ is a sound dataflow algorithm,
    because $G_\text{min}$ contains back-edges for loops that would, strictly
    speaking, not be necessary to make $\A_G(c)$ a sound dataflow algorithm.
\end{remark}

\begin{remark}
    Note that not all (sound) dataflow algorithms have the form $\A_G(c)$.
    For example the trivially sound dataflow algorithm disagrees on the 
    program $\textbf{skip}$ with any dataflow algorithm of the form 
    $\A_G(c)$ - the trivially sound
    dataflow algorithm outputs HAS\_FLOW, whereas
    $A_G(\textbf{skip}) = \text{NO\_FLOW}$ because the node set contains
    neither source nor sink.
\end{remark}

\subsection{IGNORE: An Algorithm to Compute A Dataflow Graph}
In this section we describe an algorithm $\B(c) = G$ that computes a dataflow 
graph $G$. We then prove that this algorithm indeed
computes a dataflow graph, and, using~\autoref{cor:dg-sound},
$\A_G(c)$ is a sound dataflow algorithm.

\subsubsection*{The Algorithm}
Given a program $c$ in SSA form, the algorithm computes
the edge set $E$ as a least fixed point iteration.
The following recursive rules are used:\\
\textbf{Base Case:} Start at the expression $\textbf{source }a_0$.
If it is used in either the context $X \coloneqq \textbf{source }a_0$
or $\textbf{sink }(\textbf{source }a_0)$ then add an edge from the node for the
source to the node of the store.\\
\textbf{Recursive Case:} For any store node $X \coloneqq a_0$, enumerate all uses (traverse
the def-use-chain of the SSA form). Add edges to all uses that are themselves
either stores or \textbf{sink} commands of the form $\textbf{sink } X$.
Note that stores of form $Z \coloneqq \phi(X, Y)$ get an incoming edge from $X$,
if the definition of $X$ itself has an incoming edge.

The fixed point iteration stops eventually as the set of possible edges is
finite. Furthermore, if the program doesn't contain a \textbf{source} expression,
the edge set will be empty.

TODO: Add a second pass, like in QL, that starts at the sink nodes and thus
filters out edges that don't lead to any sink.

\subsubsection*{Soundness Proof}
\begin{theorem}
    The algorithm $\B(c) = G$ computes a dataflow graph, and thus 
    $\A_G(c)$ is a sound dataflow algorithm.
\end{theorem}
\begin{proof}
    Because of~\autoref{cor:dg-sound}, we only need to show that $\B(c)$ is 
    a dataflow graph.
    To do that, we show that for $G=(V, E)$, for all initial stores $\sigma_0$,
    all edges in $E^0_{\sigma_0}$ (as in the definition of the minimal dataflow
    graph) are contained in the set $E$.

    Let $\sigma_0$ be an initial store.
    Let $e$ be an edge in $E^0_{\sigma_0}$.\\
    \textbf{Case 1:} $e$ is an edge of type (1).\\
    Then the end node of $e$
    has form $X \coloneqq a$.
    Then there are two cases for the structure of $a$.\\
    \textbf{Subcase 1:} $a = \textbf{source }a_0$.
    Then there is a corresponding edge in $E$ per the base case of the recursive
    definition of $E$.\\
    \textbf{Subcase 2:} $a = Y$ for some variable $Y \neq X$ (remember, we have 
    a SSA form)
    TODO\\
    \textbf{Case 2:} $e$ is an edge of type (2).\\
    TODO\\
    \textbf{Case 3:} $e$ is an edge of type (3).\\
    TODO
\end{proof}


\subsection{IGNORE: A Path-Sensitive Dataflow Algorithm}
TODO: description of path-pruning, proof that the only pruned edges are not
in the definition of the minimal dataflow graph, thus it is still sound

%However, as a dataflow algorithm cannot be sound and complete at the same time,
%dataflow algorithms constitute an heuristic.
%The evaluation of heuristics in a theoretic setting is difficult.
%We do aim to show that 
% TODO: A_2 is better than A_1, as both sound+one has less FP
\fi