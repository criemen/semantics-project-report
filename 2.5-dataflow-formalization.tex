% !TEX root = main.tex
\section{Formalization of Dataflow with dIMP}

In this section, we are going to formalize what we mean by dataflow,
give a simple algorithm for computing dataflow and proving it is sound.
We will furthermore use this formalization to prove that our
path-sensitivity extension of the dataflow algorithm preserves soundness.

We use the toy language IMP as presented in~\cite{sat}.
The lecture notes themselves heavily borrow from~\cite{fsopl} for the
presentation and semantics of IMP.
We assume that the reader is familiar with the syntax and semantics of IMP
as presented in section 2.1 of~\cite{sat}.

\subsection{Syntax and Semantic of dIMP}
We extend the syntax of IMP a little bit
and call the resulting language dIMP (for dataflow IMP):
\begin{align*}
    &a \Coloneqq \dots | \textbf{source } a\\
    &c \Coloneqq \dots | \textbf{sink } a\\
\end{align*}
We define the set $\tilde{\textbf{Z}} = \textbf{Z} \cup \{\top_n | n \in \textbf{Z}\}$.
Stores are denoted with $\sigma \in \Sigma = \textbf{Loc} \to \tilde{\textbf{Z}}$.
Furthermore, only up to one expression of the form $\textbf{source } a$ is allowed
in a program.
This technical restriction makes the later definition of a minimal dataflow graph
easier. It could be lifted by indexing the $\top_n$ per source.

The intuition here is that for all $n \in \textbf{Z}$, $\top_n$
is a marker indicating that the value $n$ is tracked by the dataflow algorithm,
and all tracked values originate at the source (see the semantics below).
The command $\textbf{sink } a$ is a special marker for the dataflow sink.
It aborts the program via an exception if a tracked value reaches the sink.
This ensures termination for all programs that have dataflow.
In real programs, the dataflow source would likely be a procedure reading input,
i.e.\ from a terminal, and the dataflow sink would likely be a procedure 
printing the data to a terminal, or otherwise communicating with the outside world.
Note that stores can now contain both regular integers as variable values,
as well as dataflow-tracked values.

The judgements are modified as follows:

Judgment $\langle a, \sigma \rangle \downarrow n$ is extended by:
\begin{align*}
    \textsc{EA-Source}\ddfrac{\langle a, \sigma \rangle \downarrow n}
    {\langle \textbf{source }a, \sigma \rangle \downarrow \top_n}
    \qquad     
    \textsc{EA-Untrack}\ddfrac{\langle a, \sigma \rangle \downarrow \top_n}
    {\langle a, \sigma \rangle \downarrow n}
\end{align*}




The Judgment $\langle c, \sigma \rangle \downarrow \sigma'$ is replaced by 
$\langle c, (\sigma, s) \rangle \downarrow (\sigma', s')$, were $s, s' \in \{n, e\}$.
$s$ denotes an exception state, where $n$ indicates normal program execution and $e$
indicates that an exception happened.
Exceptions are thrown when a dataflow-tracked value reaches a sink, the program
is then aborted. The exception cannot be catched.
\begin{align*}
    &\textsc{EC-Sink1}
    \ddfrac{\langle a, \sigma \rangle\downarrow \top_n}
    {\langle \textbf{sink } a, (\sigma, s) \rangle \downarrow (\sigma, e)}\\
    &\textsc{EC-Sink2}\ddfrac{\langle a, \sigma\rangle \downarrow n}
    {\langle \textbf{sink } a, (\sigma, s) \rangle \downarrow (\sigma, s)}
    (\text{Only if $\langle a, \sigma\rangle$ doesn't evaluate to $\top_n$, too})\\
    &\textsc{EC-Exception}\ddfrac{}
    {\langle c, (\sigma, e) \rangle \downarrow (\sigma, e)}
\end{align*}

The rule \textsc{EC-Assign} is removed and replaced by:
\begin{align*}
    &\textsc{EC-Assign1}\ddfrac{\langle a, \sigma\rangle \downarrow \top_n}
    {\langle X \coloneqq a, (\sigma, n) \rangle \downarrow (\sigma[X \mapsto \top_n], n)}\\
    &\textsc{EC-Assign2}\ddfrac{\overset{\E_0}{\langle a, \sigma \rangle \downarrow n}}
    {\langle X \coloneqq a, (\sigma, n) \rangle \downarrow (\sigma[X \mapsto n], n)}
     (\text{Only if $\E_0$ doesn't use rule \textsc{EA-Untrack} as last rule})
\end{align*}
All other rules are modified to only apply when the exception state $s$ is $n$.
The exception state of $n$ is preserved by all commands not reproduced here.

Note that the side-conditions on \textsc{EC-Sink2} and \textsc{EC-Assign2}
ensure that all derivations are unique.
Furthermore, the rule \textsc{EA-Untrack} ensures that whenever the value of
a tracked arithmetic expression is used in a non-value-preserving operation,
the value is not tracked anymore.
This applies even to operations like addition with 0 or multiplication with 1,

\subsection{Syntax and Semantic of SSA-dIMP}
To facilitate the dataflow analysis, we describe a SSA form on dIMP with a concrete
syntax and semantic.
We do not describe how to transform a dIMP program to a SSA-dIMP program, but 
refer the reader to the literature, where different SSA construction algorithms are
described. (TODO cite). Furthermore, there are correctness proofs of SSA construction
available (TODO).

The syntax for boolean and arithmetic expressions in SSA-dIMP is the same as in dIMP.
However, the command syntax changes.
\begin{align*}
    c \Coloneqq &\textbf{skip} | X \coloneqq a| c_0; c_1 | (\textbf{if } b \textbf{ then }c_0
    \textbf{ else }c_1);[X = \Phi(Y, Z)] |\\
    &\textbf{while }[X = \Phi(Y, Z)]; b\textbf{ do } c_0 | \textbf{sink } a\\
\end{align*}
At every point where possibly $\Phi$ nodes are placed in the control flow graph
of a dIMP program, the syntax of SSA-dIMP has an explicit list of $\Phi$ nodes.
That list can be empty.

A program in SSA-dIMP is only valid if it conforms to the SSA properties.
Thus, every variable definition dominates all of the uses of the variable, and
the number of $\Phi$ nodes in the program is minimal.
The easiest way to achieve this is to run a SSA construction algorithm on a dIMP program.
It is a gap in the provided proofs that we do not show that every dIMP program
can be transformed in a valid SSA-dIMP program (especially, we omit showing that
$\Phi$ nodes will be placed exactly at the places they are allowed in the syntax
of SSA-dIMP).
We also require that the first argument for $\Phi$ nodes for \textbf{if} commands
corresponds to the variable after executing the \textbf{true} branch, and the second argument to that
of the variable after executing the \textbf{false} branch.
For \textbf{while} commands, the first argument to the $\Phi$ node is the value
before executing $c_0$ the first time, and the second argument is the value 
after executing the loop body.
TODO something about use of initial values, as they cannot have a definition

The only rules replaced from the dIMP semantics are the ones for \textbf{if} and 
\textbf{while}.
\begin{landscape}
    The new rules are the following:
    \begin{align*}
        &\textsc{EC-IfT}
        \ddfrac{\langle b, \sigma \rangle \downarrow \textbf{true}
        \qquad\langle c_0, (\sigma, n) \rangle \downarrow (\sigma', n)}
        {\langle (\textbf{if }b\textbf{ then } c_0 \textbf{ else }c_1);
        [X_1= \Phi(Y_1, Z_1), \ldots, X_n = \Phi(Y_n, Z_n)], (\sigma, n) \rangle
        \downarrow (\sigma'[X_1 \mapsto Y_1, \ldots, X_n \mapsto Y_n], n)}\\
        &\textsc{EC-IfF}
        \ddfrac{\langle b, \sigma \rangle \downarrow \textbf{false}
        \qquad\langle c_1, (\sigma, n) \rangle  \downarrow (\sigma', n)}
        {\langle (\textbf{if }b\textbf{ then } c_0 \textbf{ else }c_1);
        [X_1= \Phi(Y_1, Z_1), \ldots, X_n = \Phi(Y_n, Z_n)], (\sigma, n) \rangle
        \downarrow (\sigma'[X_1 \mapsto Z_1, \ldots, X_n \mapsto Z_n], n)}\\
        &\textsc{EC-WhileF0}\ddfrac
        {\langle b, \sigma[X_1 \mapsto Y_1, \ldots, X_n \mapsto Y_n] \rangle \downarrow \textbf{false}}
        {\langle \textbf{while }[X_1= \Phi(Y_1, Z_1), \ldots, X_n = \Phi(Y_n, Z_n)];
        b \textbf{ do } c_0, (\sigma, n) 
        \downarrow (\sigma[X_1 \mapsto Y_1, \ldots, X_n \mapsto Y_n], n)}\\
        &\textsc{EC-WhileT0}\ddfrac
        {\langle b, \sigma[X_1 \mapsto Y_1, \ldots, X_n \mapsto Y_n] \rangle \downarrow \textbf{true}
        \quad \langle c_0, (\sigma[X_1 \mapsto Y_1, \ldots, X_n \mapsto Y_n], n) \rangle \downarrow (\sigma'', n)
        \quad \overset{\E_0}{\textbf{while }[X_1= \Phi(Y_1, Z_1), \ldots];%, X_n = \Phi(Y_n, Z_n)];
        b \textbf{ do } c_0, (\sigma'', n) \rangle \downarrow (\sigma', n) }}
        {\langle \textbf{while }[X_1= \Phi(Y_1, Z_1), \ldots, X_n = \Phi(Y_n, Z_n)];
        b \textbf{ do } c_0, (\sigma, n) 
        \downarrow }\\
        &\text{(if $\E_0$ uses either \textsc{EC-WhileF+} or \textsc{EC-WhileT+} as last rule)}\\
        &\textsc{EC-WhileF+}\ddfrac
        {\langle b, \sigma[X_1 \mapsto Z_1, \ldots, X_n \mapsto Z_n] \rangle \downarrow \textbf{false}}
        {\langle \textbf{while }[X_1= \Phi(Y_1, Z_1), \ldots, X_n = \Phi(Y_n, Z_n)];
        b \textbf{ do } c_0, (\sigma, n) 
        \downarrow (\sigma[X_1 \mapsto Z_1, \ldots, X_n \mapsto Z_n], n)}\\
        &\textsc{EC-WhileT+}\ddfrac
        {\langle b, \sigma[X_1 \mapsto Z_1, \ldots, X_n \mapsto Z_n] \rangle \downarrow \textbf{true}
        \quad \langle c_0, (\sigma[X_1 \mapsto Z_1, \ldots, X_n \mapsto Z_n], n) \rangle \downarrow (\sigma'', n)
        \quad \overset{\E_0}{\textbf{while }[X_1= \Phi(Y_1, Z_1), \ldots];%, X_n = \Phi(Y_n, Z_n)];
        b \textbf{ do } c_0, (\sigma'', n) \rangle \downarrow (\sigma', n) }}
        {\langle \textbf{while }[X_1= \Phi(Y_1, Z_1), \ldots, X_n = \Phi(Y_n, Z_n)];
        b \textbf{ do } c_0, (\sigma, n) 
        \downarrow }\\
        &\text{(if $\E_0$ uses either \textsc{EC-WhileF+} or \textsc{EC-WhileT+} as last rule)}\\
    \end{align*}
    They ensure that the first execution of a while loop uses the variables as defined before
    the loop executes, and after that the variables are used that the loop body 
    defined.
    Unfortunately this makes the semantics more complicated.
\end{landscape}

\subsection{Definition of Dataflow}
\begin{definition}[Program]
    A \emph{program} in dIMP is a command $c$.
    Complex programs are expressed by using the recursive nature of
    the definition of commands.
\end{definition}

\begin{definition}[Initial Store]
    An \emph{initial store} is a store $\sigma_0$ s.t.\ 
    $\forall X: \exists n: \sigma_0(X) = n$.
    This means that an initial store is not allowed to contain dataflow tracking 
    markers of the form $\top_n$.
    We will implicitly denote inital stores by $\sigma_0$.
\end{definition}

\begin{definition}[Dataflow]
    A tuple $(c, \sigma_0)$ of a program $c$ and initial store $\sigma_0$ 
    has \emph{dataflow from the source to a sink} if
    $\langle c, (\sigma_0, n) \rangle \downarrow (\sigma', e)$ holds.
    This means that the program terminates with a dataflow exception.
\end{definition}

\begin{definition}[Dataflow Algorithm]
    A \emph{dataflow algorithm} $\A(c)$ computes, given a program $c$,
    if there exists an initial store $\sigma_0$ 
    such that $(c, \sigma_0)$ has dataflow from the source to a sink.
\end{definition}

\begin{definition}[Soundness]
    A dataflow algorithm is \emph{sound} if for any tuple $(c, \sigma_0)$ that
    has dataflow it holds that $\A(c) = \text{HAS\_FLOW}$.
\end{definition}

\begin{definition}[Completeness]
    A dataflow algorithm is \emph{complete} if $\A(c) = \text{HAS\_FLOW}$
    implies that there exists an initial store $\sigma_0$
     such that $(c, \sigma_0)$ has dataflow.
\end{definition}
\begin{definition}[False Positive]
    A program $c$ for which there exists no initial store $\sigma_0$ such that 
    $(c, \sigma_0)$ has dataflow, but for which $\A(c) = \text{HAS\_FLOW}$ holds
    is called a \emph{false positive} of the algorithm.
\end{definition}
\begin{remark}
    In general, it is impossible to construct a dataflow algorithm that is both 
    sound and complete.
    In practice, a dataflow algorithm will be neither sound nor complete.
    However, in the theoretical setting of this chapter, we are interested in 
    sound dataflow algorithms.
    The \emph{trivially sound dataflow algorithm} $\A_0(c) = \text{HAS\_FLOW}$ 
    is sound by definition, but not very interesting.
    We will not consider it further, but it is interesting to keep in mind,
    because it shows that just showing that a dataflow algorithm is sound does not
    mean it is useful.    
\end{remark}

\subsection{The Dataflow Graph}
To aid the construction of dataflow algorithms, we define the concept
of a dataflow graph.
Then we show that an algorithm that outputs HAS\_FLOW if 
there exists a path from the source 
node to a sink node in a dataflow graph %and NO\_FLOW otherwise
is a sound dataflow algorithm.
Furthermore, we introduce an algorithm to compute a dataflow graph.

The dataflow graph is computed upon an SSA form of the program.
Thus, it may contain assignments of the form $X \coloneqq \phi(a_1, a_2)$.
We will assume that for any given store $\sigma$ the derivation only contains 
the assignment with an evaluated $\phi$ function, as to not overload the notation.
Furthermore, the dataflow graph defined on an SSA form of the program can easily 
be transformed to a dataflow graph defined on the regular program.
The minimal dataflow graph is directed and may, depending on the 
program structure, contain cycles.

The \emph{node set} $V$ of the \emph{minimal dataflow graph} $G_\text{min}(c)$
is defined on the syntactic structure
of the SSA form of $c$. It is a subset of the node set of the abstract syntax tree
(every node knows about its location in the program text).
Every (sub-)command of $c$, as well as any arithmetic expression $a$ in the 
program is a node in the dataflow graph.
For example for
$c = \textbf{skip}; (\textbf{skip}; x \coloneqq \bar{4} + y)$
the node set consists of $c$ itself, the first \textbf{skip}, the command 
$\textbf{skip}; x \coloneqq \bar{4} + y$, the second \textbf{skip} (that is different 
from the first because it appears in another location), the command
$x \coloneqq \bar{4} + y$, and the arithmetic expressions $\bar{4}+y$,
$\bar{4}$ and $y$.


For every initial store $\sigma_0$, 
let $\E$ be a bigstep derivation of $\langle c, \sigma_0 \rangle \downarrow \sigma'$.
The edge set $E^0_{\sigma_0}$ contains the following edges:
\begin{enumerate}
    \item $\E$ contains a subderivation
    \begin{equation*}
        \E_1 = \textsc{EC-Assign1}\ddfrac{\overset{\E'}{\langle a, \sigma \rangle \downarrow \top_n}}
        {\langle X \coloneqq a, \sigma \rangle \downarrow
         \sigma[X \mapsto \top_n]}
    \end{equation*}
    Then there is an edge from $a$ to the node of $X \coloneqq a$.
    This case includes both outgoing edges from sources and variable reads
    of locations to which a tracked value was written to.
    \item $\E$ contains a subderivation
    \begin{equation*}
        \E_1 = \textsc{EC-Sink1}\ddfrac{\overset{\E'}{\langle a, \sigma \rangle \downarrow \top_n}}
        {\langle \textbf{sink }a, \sigma \rangle \downarrow
         \sigma}
    \end{equation*}
    Then there is an edge from $a$ to the node of $\textbf{sink }a$.
    \item
    $\E$ contains a subderivation
    \begin{equation*}
        \E_1 = \textsc{EA-Loc}\ddfrac{}
        {\langle X, \sigma \rangle \downarrow \top_n}
    \end{equation*}
    and a subderivation
    \begin{equation*}
        \E_2 = \textsc{EC-Assign1}\ddfrac{\overset{\E'}{\langle a, \sigma' \rangle \downarrow \top_n}}
        {\langle X \coloneqq a, \sigma' \rangle \downarrow
         \sigma'[X \mapsto \top_n]}
    \end{equation*}
    with $\sigma'(X) \neq \top_n$ (this prevents cyclic edges).
    Then there is an edge from the store $X \coloneqq a$ to the read node $X$ as 
    refered to by $\E_1$.\\
    Technical note 1: Here we exploit the fact that there is only one source ---
    else this case could introduce edges between unrelated paths if two sources for the same 
    $\sigma_0$ would evaluate to $\top_n$. Then the edge set would not be minimal
    anymore.\\
    Technical note 2: Because the program is in SSA, edges from a store to a read 
    are in execution-order. A read has an incoming edge from a store only if the 
    store happens before the read in the program.
\end{enumerate}

Based on that we define the (minimized) edge set $E^1_{\sigma_0}$
that contains an edge from $E^0_{\sigma_0}$ iff it is part of any path 
from a node of the form $\textbf{source }a_0$
to a node of the form $\textbf{sink }a_1$.

Then we define the \emph{edge set} $E$ of the minimal dataflow graph as
\begin{equation*}
    E = \bigcup_{\sigma_0} E^1_{\sigma_0}
\end{equation*}
Even though the union is over an infinite set, the result is finite
as $E \subseteq V \times V$ and $|V| < \infty$.

We call a graph a \emph{dataflow graph} if it has the node set as the minimal
dataflow graph and its edge set contains the edges of the minimal dataflow 
graph.

Let $\A_{G_\text{min}}(c)$ be the algorithm that takes the
minimal dataflow graph $G_\text{min}(c)$ and outputs HAS\_FLOW if there 
exists a node of the form 
$\textbf{source }a$ and a node $\textbf{sink }a$ such that there is a path in 
$G_\text{min}(c)$ from the source node to the sink node.
If no such path exists, it outputs NO\_FLOW.


\begin{theorem}
    \label{thm:min-dg}
    The algorithm $\A_{G_\text{min}}(c)$ is a sound dataflow algorithm.
\end{theorem}
In order to prove the theorem, we introduce the concept of store-read chains.

\begin{definition}[Store-Read Chain]
    Let $(c, \sigma_0)$ be a tuple that has dataflow.
    Let $\E$ be the bigstep derivation of
    $\langle c, \sigma_0 \rangle \downarrow \sigma'$.
    A \emph{store-read chain} in $\E$ is a list of $m$ tuples of the form
    $(\E^1_i=\ddfrac{\overset{\E^0_i}{\langle a_i, \sigma_i \rangle \downarrow \top_n}}
    {\langle X_i \coloneqq a_i, \sigma_i \rangle \downarrow \sigma_i[X_i \mapsto \top_n]},
    \E^2_i=\ddfrac{}{\langle X_i, \sigma_i' \rangle \downarrow \sigma_i'(X_i) = \top_n})$
    with $i \in \{1, \ldots, m\}$.
    $\E^0_i$, $\E^1_i$ and $\E^2_i$ are subderivations of $\E$.
    We require that the following holds:
    \begin{enumerate}
        \item $\sigma_i(X) \neq \top_n$ (so we have a first store). 
        \item For $i \in \{1, \ldots, m-1\}$ it holds that 
        $\E^0_{i+1}=\E^2_i$. This means that every store except the first one
        in the chain is connected via the variable it reads from to the previous 
        store in the chain.
        \item Repeating tuples are not allowed in the list, all tuples have to 
        be pairwise disjoint --- equality on subderivations of $\E$ includes
        placement in $\E$, not just equality of the subderivation trees.
    \end{enumerate}
\end{definition}

\begin{remark}
    By definition a read-store-chain does not contain any cycles.
    As the derivation $\E$ is finite, this implies that the set of 
    read-store chains is finite.
    By defining the obvious partial order induced by inclusion
    on the set of read-store-chains, we see that 
    every non-empty read-store-chain can be extended to a maximal 
    read-store-chain.
\end{remark}

\begin{lemma}
    \label{lem:chain-to-graph}
    Any store-read-chain induces a path in $E^0_{\sigma_0}$ from the node 
    of the first store in the chain to the node of the last read in the chain.
    Furthermore, there is an edge from the expression that the first store reads
    to the first store.
\end{lemma}
\begin{proof}
    We proof the lemma by induction over the length of the store-read chain.\\
    \textbf{Base Case (m=1):}
    There is by definition of $\E^0_{\sigma_0}$
    one edge of type (3) connecting the store and the read in the chain.
    Furthermore, there is an edge of type (1) connecting the read-from expression
    to the store.\\
    \textbf{Inductive Case:}
    Let $m \geq 1$ be fixed. By the IH, there is a path in $E^0_{\sigma_0}$
    corresponding to the chain of length $m-1$ from the first store in the chain 
    to the read of $X_{m-1}$ in the chain.
    By definition of the chain, we have an edge of type (1) connecting 
    the read of $X_{m-1}$ to the store of $X_m$.
    Furthermore, again by definition of $\E^0_{\sigma_0}$
    there is one edge of type (3) connecting the store of $X_m$ to the
    read of $X_m$.
\end{proof}

\begin{lemma}
    \label{lem:max-store-read}
    Any maximal store-read-chain contains a store from the source as first store.
\end{lemma}
\begin{proof}
    We look at the derivation $\E^0_1$.
    If $a = Y$ for some $Y \in \textbf{Loc}$, then the store-read-chain is not 
    maximal, because it can be extended at the front by a tuple with a store to and a
    read from $Y$.
    The only other possibility for an arithmetic expression $a$ to evaluate to 
    $\top_n$ is that it is of form $\textbf{source } a_0$.
    This concludes the proof.
\end{proof}


\begin{proof}[Proof Of~\autoref{thm:min-dg}]
    Let $\sigma_0$ be an initial store such that $(c, \sigma_0)$ has dataflow.
    If no such $\sigma_0$ exists, by the definition of soundness
    the proof is concluded.

    Let $\E$ be the bigstep derivation of
    $\langle c, \sigma_0 \rangle \downarrow \sigma'$.
    Because $(c, \sigma_0)$ has dataflow, there exists a derivation of the form 
    \begin{equation*}
        \E_1 = \textsc{EC-Sink1}\ddfrac
        {\overset{\E_2}{\langle a, \sigma \rangle \downarrow \top_n}}
        {\langle \textbf{sink } a, \sigma} \downarrow \sigma
    \end{equation*}
    There are two possibilities for the structure of a.\\
    \textbf{Case 1:} $a = \textbf{source }a_0$\\
    Then $E^0_{\sigma_0}$ contains an edge of type (2) that connects the source 
    to a sink. This constitues a path from the source to a sink.\\
    \textbf{Case 2:} $a = X$\\
    Then $X$ is a variable read that evaluates to $\top_n$.
    Because $\sigma_0(X) \neq \top_n$, there has to be a first store
    of $X$ in $\E$.
    The derivation of that store, together with the derivation of the read make 
    up a store-read chain of length one.
    This chain can be extended to a maximal chain, and 
    by~\autoref{lem:max-store-read}, we know it starts with a store reading the
    source.
    Furthermore, by~\autoref{lem:chain-to-graph} we get a path in $E^0_{\sigma_0}$
    from the source node to the read. The read is then connected by an edge
    of type (2) to the sink, thus we have a path from the source to a sink in 
    $E^0_{\sigma_0}$.

    In both cases we have shown that $E^0_{\sigma_0}$ contains a path from the 
    source to a sink. Thus, $E^1_{\sigma_0}$ and in turn $E$ contain a path
    from the source to a sink, and $\A_{G_\text{min}}(c) = \text{HAS\_FLOW}$.
    This concludes the proof.
\end{proof}

\begin{corollary}
    \label{cor:dg-sound}
    For any dataflow graph $G(c)$ the algorithm $\A_{G}(c)$ is a sound
     dataflow algorithm.
\end{corollary}
\begin{proof}
    Remember that any dataflow graph contains the minimal dataflow graph as 
    subgraph. Thus for every pair $(c, \sigma_0)$ that has dataflow
    there is a path from the source node to a sink node, 
    thus $\A$ reports flow by~\autoref{thm:min-dg}.
    It doesn't matter if $G$ has more edges for soundness.
\end{proof}

\begin{remark}
    The \emph{minimal} dataflow graph gets name by being the minimal dataflow graph
    among all dataflow graphs that are considered in this paper.
    It is not the smallest graph $G$ on the node set 
    such that $\A_G(c)$ is a sound dataflow algorithm,
    because $G_\text{min}$ contains back-edges for loops that would, strictly
    speaking, not be necessary to make $\A_G(c)$ a sound dataflow algorithm.
\end{remark}

\begin{remark}
    Note that not all (sound) dataflow algorithms have the form $\A_G(c)$.
    For example the trivially sound dataflow algorithm disagrees on the 
    program $\textbf{skip}$ with any dataflow algorithm of the form 
    $\A_G(c)$ - the trivially sound
    dataflow algorithm outputs HAS\_FLOW, whereas
    $A_G(\textbf{skip}) = \text{NO\_FLOW}$ because the node set contains
    neither source nor sink.
\end{remark}

\subsection{An Algorithm to Compute A Dataflow Graph}
In this section we describe an algorithm $\B(c) = G$ that computes a dataflow 
graph $G$. We then prove that this algorithm indeed
computes a dataflow graph, and, using~\autoref{cor:dg-sound},
$\A_G(c)$ is a sound dataflow algorithm.

\subsubsection*{The Algorithm}
Given a program $c$ in SSA form, the algorithm computes
the edge set $E$ as a least fixed point iteration.
The following recursive rules are used:\\
\textbf{Base Case:} Start at the expression $\textbf{source }a_0$.
If it is used in either the context $X \coloneqq \textbf{source }a_0$
or $\textbf{sink }(\textbf{source }a_0)$ then add an edge from the node for the
source to the node of the store.\\
\textbf{Recursive Case:} For any store node $X \coloneqq a_0$, enumerate all uses (traverse
the def-use-chain of the SSA form). Add edges to all uses that are themselves
either stores or \textbf{sink} commands of the form $\textbf{sink } X$.
Note that stores of form $Z \coloneqq \phi(X, Y)$ get an incoming edge from $X$,
if the definition of $X$ itself has an incoming edge.

The fixed point iteration stops eventually as the set of possible edges is
finite. Furthermore, if the program doesn't contain a \textbf{source} expression,
the edge set will be empty.

TODO: Add a second pass, like in QL, that starts at the sink nodes and thus
filters out edges that don't lead to any sink.

\subsubsection*{Soundness Proof}
\begin{theorem}
    The algorithm $\B(c) = G$ computes a dataflow graph, and thus 
    $\A_G(c)$ is a sound dataflow algorithm.
\end{theorem}
\begin{proof}
    Because of~\autoref{cor:dg-sound}, we only need to show that $\B(c)$ is 
    a dataflow graph.
    To do that, we show that for $G=(V, E)$, for all initial stores $\sigma_0$,
    all edges in $E^0_{\sigma_0}$ (as in the definition of the minimal dataflow
    graph) are contained in the set $E$.

    Let $\sigma_0$ be an initial store.
    Let $e$ be an edge in $E^0_{\sigma_0}$.\\
    \textbf{Case 1:} $e$ is an edge of type (1).\\
    Then the end node of $e$
    has form $X \coloneqq a$.
    Then there are two cases for the structure of $a$.\\
    \textbf{Subcase 1:} $a = \textbf{source }a_0$.
    Then there is a corresponding edge in $E$ per the base case of the recursive
    definition of $E$.\\
    \textbf{Subcase 2:} $a = Y$ for some variable $Y \neq X$ (remember, we have 
    a SSA form)
    TODO\\
    \textbf{Case 2:} $e$ is an edge of type (2).\\
    TODO\\
    \textbf{Case 3:} $e$ is an edge of type (3).\\
    TODO
\end{proof}


\subsection{A Path-Sensitive Dataflow Algorithm}
TODO: description of path-pruning, proof that the only pruned edges are not
in the definition of the minimal dataflow graph, thus it is still sound

%However, as a dataflow algorithm cannot be sound and complete at the same time,
%dataflow algorithms constitute an heuristic.
%The evaluation of heuristics in a theoretic setting is difficult.
%We do aim to show that 
% TODO: A_2 is better than A_1, as both sound+one has less FP
