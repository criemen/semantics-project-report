% !TEX root = main.tex
\section{Formalization of Dataflow with dIMP}

In this section, we are going to formalize what we mean by dataflow,
give a simple algorithm for computing dataflow and proving it is sound.
We will furthermore use this formalization to prove that our
path-sensitivity extension of the dataflow algorithm preserves soundness.

We use the toy language IMP as presented in~\cite{sat}.
The lecture notes themselves heavily borrow from~\cite{fsopl} for the
presentation and semantics of IMP.
We assume that the reader is familiar with the syntax and semantics of IMP
as presented in section 2.1 of~\cite{sat}.

\subsection{Syntax and Semantic of dIMP}
We extend the syntax of IMP a little bit
and call the resulting language dIMP (for dataflow IMP):
\begin{align*}
    &a \Coloneqq \dots | \textbf{source } a\\
    &c \Coloneqq \dots | \textbf{sink } a\\
\end{align*}
We define the set $\tilde{\textbf{Z}} = \textbf{Z} \cup \{\top_n | n \in \textbf{Z}\}$.
Stores are denoted with $\sigma \in \Sigma = \textbf{Loc} \to \tilde{\textbf{Z}}$.
Furthermore, only up to one expression of the form $\textbf{source } a$ is allowed
in a program.
This technical restriction makes the later definition of a minimal dataflow graph
easier. It could be lifted by indexing the $\top_n$ per source.

The intuition here is that for all $n \in \textbf{Z}$, $\top_n$
is a marker indicating that the value $n$ is tracked by the dataflow algorithm,
and all tracked values originate at the source (see the semantics below).
The command $\textbf{sink } a$ is a special marker for the dataflow sink.
In real programs, the dataflow source would likely be a procedure reading input,
i.e.\ from a terminal, and the dataflow sink would likely be a procedure 
printing the data to a terminal, or otherwise communicating with the outside world.
Note that stores can now contain both regular integers as variable values,
as well as dataflow-tracked values.

The judgements are extended as follows:

Judgment $\langle a, \sigma \rangle \downarrow n$:
\begin{align*}
    \textsc{EA-Source}\ddfrac{\langle a, \sigma \rangle \downarrow n}
    {\langle \textbf{source }a, \sigma \rangle \downarrow \top_n}
    \qquad     
    \textsc{EA-Untracking}\ddfrac{\langle a, \sigma \rangle \downarrow \top_n}
    {\langle a, \sigma \rangle \downarrow n}
\end{align*}




Judgment $\langle c, \sigma \rangle \downarrow \sigma'$:
\begin{align*}
    &\textsc{EC-Sink1}\ddfrac{\langle a, \sigma \rangle\downarrow \top_n}
    {\langle \textbf{sink } a, \sigma \rangle \downarrow \sigma}\\
    &\textsc{EC-Sink2}\ddfrac{\langle a, \sigma\rangle \downarrow n}
    {\langle \textbf{sink } a, \sigma \rangle \downarrow \sigma}
    (\text{Only if $\langle a, \sigma\rangle$ doesn't evaluate to $\top_n$, too})
\end{align*}

The rule \textsc{EC-Assign} is removed and replaced by:
\begin{align*}
    &\textsc{EC-Assign1}\ddfrac{\langle a, \sigma\rangle \downarrow \top_n}
    {\langle X \coloneqq a, \sigma \rangle \downarrow \sigma[X \mapsto \top_n]}\\
    &\textsc{EC-Assign2}\ddfrac{\langle a, \sigma \rangle \downarrow n}
    {\langle X \coloneqq a, \sigma \rangle \downarrow \sigma[X \mapsto n]}
     (\text{Only if $\langle a, \sigma\rangle$ doesn't evaluate to $\top_n$, too})
\end{align*}

Note that the side-conditions on \textsc{EC-Sink2} and \textsc{EC-Assign2}
ensure that all derivations are unique.
Furthermore, the rule \textsc{EA-Untracking} ensures that whenever the value of
a tracked arithmetic expression is used in a non-value-preserving operation,
the value is not tracked anymore.
This applies even to operations like addition with 0 or multiplication with 1,
that are value-preserving special cases of non-value-preserving operations.
The \textbf{sink} command has the same semantics as \textbf{skip} 
for program execution.
However, the difference will become obvious when defining dataflow.

\subsection{Definition of Dataflow}
\begin{definition}[Program]
    A \emph{program} in IMP is a command $c$.
    Complex programs are expressed by using the recursive nature of
    the definition of commands.
\end{definition}

\begin{definition}[Initial Store]
    An \emph{initial store} is a store $\sigma_0$ s.t.\ 
    $\forall X: \exists n: \sigma_0(X) = n$.
    This means that an initial store is not allowed to contain dataflow tracking 
    markers of the form $\top_n$.
    We will implicitly denote inital stores by $\sigma_0$.
\end{definition}

\begin{definition}[Dataflow]
    A tuple $(c, \sigma_0)$ of a program $c$ and initial store $\sigma_0$ 
    has \emph{dataflow from the source to a sink} if the derivation $\E$
    of $\langle c, \sigma_0 \rangle \downarrow \sigma'$ contains a 
    subderivation using rule \textsc{EC-Sink1}.
\end{definition}
\begin{remark}
    Note that a technicality of this definition is that only terminating programs
(with $\sigma_0$) can have dataflow. 
In a practical setting, this is not correct - a program could have
interesting dataflow from a source to a sink, and then enter an infinite loop,
However, there would be a (terminating) sub-program that we could analyze
(basically, a program that stops as soon as the sink with a tracked value
is encountered).
\end{remark}

\begin{definition}[Dataflow Algorithm]
    A \emph{dataflow algorithm} $\A(c)$ computes, given a program $c$,
    if there exists an initial store $\sigma_0$ 
    such that $(c, \sigma_0)$ has dataflow from the source to a sink.
\end{definition}

\begin{definition}[Soundness]
    A dataflow algorithm is \emph{sound} if for any tuple $(c, \sigma_0)$ that
    has dataflow it holds that $\A(c) = \text{HAS\_FLOW}$.
\end{definition}

\begin{definition}[Completeness]
    A dataflow algorithm is \emph{complete} if $\A(c) = \text{HAS\_FLOW}$
    implies that there exists an initial store $\sigma_0$
     such that $(c, \sigma_0)$ has dataflow.
\end{definition}
\begin{definition}[False Positive]
    A program $c$ for which there exists no initial store $\sigma_0$ such that 
    $(c, \sigma_0)$ has dataflow, but for which $\A(c) = \text{HAS\_FLOW}$ holds
    is called a \emph{false positive} of the algorithm.
\end{definition}
\begin{remark}
    In general, it is impossible to construct a dataflow algorithm that is both 
    sound and complete.
    In practice, a dataflow algorithm will be neither sound nor complete.
    However, in the theoretical setting of this chapter, we are interested in 
    sound dataflow algorithms.
    The \emph{trivially sound dataflow algorithm} $\A_0(c) = \text{HAS\_FLOW}$ 
    is sound by definition, but not very interesting.
    We will not consider it further, but it is interesting to keep in mind,
    because it shows that just showing that a dataflow algorithm is sound does not
    mean it is useful.    
\end{remark}

\subsection{The Dataflow Graph}
To aid the construction of dataflow algorithms, we define the concept
of a dataflow graph.
Then we show that an algorithm that outputs HAS\_FLOW if 
there exists a path from the source 
node to a sink node in a dataflow graph %and NO\_FLOW otherwise
is a sound dataflow algorithm.
Furthermore, we introduce an algorithm to compute a dataflow graph.

The dataflow graph is computed upon an SSA form of the program.
Thus, it may contain assignments of the form $X \coloneqq \phi(a_1, a_2)$.
We will assume that for any given store $\sigma$ the derivation only contains 
the assignment with an evaluated $\phi$ function, as to not overload the notation.
Furthermore, the dataflow graph defined on an SSA form of the program can easily 
be transformed to a dataflow graph defined on the regular program.
The minimal dataflow graph is directed and may, depending on the 
program structure, contain cycles.

The \emph{node set} $V$ of the \emph{minimal dataflow graph} $G_\text{min}(c)$
is defined on the syntactic structure
of the SSA form of $c$. It is a subset of the node set of the abstract syntax tree
(every node knows about its location in the program text).
Every (sub-)command of $c$, as well as any arithmetic expression $a$ in the 
program is a node in the dataflow graph.
For example for
$c = \textbf{skip}; (\textbf{skip}; x \coloneqq \bar{4} + y)$
the node set consists of $c$ itself, the first \textbf{skip}, the command 
$\textbf{skip}; x \coloneqq \bar{4} + y$, the second \textbf{skip} (that is different 
from the first because it appears in another location), the command
$x \coloneqq \bar{4} + y$, and the arithmetic expressions $\bar{4}+y$,
$\bar{4}$ and $y$.


For every initial store $\sigma_0$, 
let $\E$ be a bigstep derivation of $\langle c, \sigma_0 \rangle \downarrow \sigma'$.
The edge set $E^0_{\sigma_0}$ contains the following edges:
\begin{enumerate}
    \item $\E$ contains a subderivation
    \begin{equation*}
        \E_1 = \textsc{EC-Assign1}\ddfrac{\overset{\E'}{\langle a, \sigma \rangle \downarrow \top_n}}
        {\langle X \coloneqq a, \sigma \rangle \downarrow
         \sigma[X \mapsto \top_n]}
    \end{equation*}
    Then there is an edge from $a$ to the node of $X \coloneqq a$.
    This case includes both outgoing edges from sources and variable reads
    of locations to which a tracked value was written to.
    \item $\E$ contains a subderivation
    \begin{equation*}
        \E_1 = \textsc{EC-Sink1}\ddfrac{\overset{\E'}{\langle a, \sigma \rangle \downarrow \top_n}}
        {\langle \textbf{sink }a, \sigma \rangle \downarrow
         \sigma}
    \end{equation*}
    Then there is an edge from $a$ to the node of $\textbf{sink }a$.
    \item
    $\E$ contains a subderivation
    \begin{equation*}
        \E_1 = \textsc{EA-Loc}\ddfrac{}
        {\langle X, \sigma \rangle \downarrow \top_n}
    \end{equation*}
    and a subderivation
    \begin{equation*}
        \E_2 = \textsc{EC-Assign1}\ddfrac{\overset{\E'}{\langle a, \sigma' \rangle \downarrow \top_n}}
        {\langle X \coloneqq a, \sigma' \rangle \downarrow
         \sigma'[X \mapsto \top_n]}
    \end{equation*}
    with $\sigma'(X) \neq \top_n$ (this prevents cyclic edges).
    Then there is an edge from the store $X \coloneqq a$ to the read node $X$ as 
    refered to by $\E_1$.\\
    Technical note 1: Here we exploit the fact that there is only one source ---
    else this case could introduce edges between unrelated paths if two sources for the same 
    $\sigma_0$ would evaluate to $\top_n$. Then the edge set would not be minimal
    anymore.\\
    Technical note 2: Because the program is in SSA, edges from a store to a read 
    are in execution-order. A read has an incoming edge from a store only if the 
    store happens before the read in the program.
\end{enumerate}

Based on that we define the (minimized) edge set $E^1_{\sigma_0}$
that contains an edge from $E^0_{\sigma_0}$ iff it is part of any path 
from a node of the form $\textbf{source }a_0$
to a node of the form $\textbf{sink }a_1$.

Then we define the \emph{edge set} $E$ of the minimal dataflow graph as
\begin{equation*}
    E = \bigcup_{\sigma_0} E^1_{\sigma_0}
\end{equation*}
Even though the union is over an infinite set, the result is finite
as $E \subseteq V \times V$ and $|V| < \infty$.

We call a graph a \emph{dataflow graph} if it has the node set as the minimal
dataflow graph and its edge set contains the edges of the minimal dataflow 
graph.

Let $\A_{G_\text{min}}(c)$ be the algorithm that takes the
minimal dataflow graph $G_\text{min}(c)$ and outputs HAS\_FLOW if there 
exists a node of the form 
$\textbf{source }a$ and a node $\textbf{sink }a$ such that there is a path in 
$G_\text{min}(c)$ from the source node to the sink node.
If no such path exists, it outputs NO\_FLOW.


\begin{theorem}
    \label{thm:min-dg}
    The algorithm $\A_{G_\text{min}}(c)$ is a sound dataflow algorithm.
\end{theorem}
In order to prove the theorem, we introduce the concept of store-read chains.

\begin{definition}[Store-Read Chain]
    Let $(c, \sigma_0)$ be a tuple that has dataflow.
    Let $\E$ be the bigstep derivation of
    $\langle c, \sigma_0 \rangle \downarrow \sigma'$.
    A \emph{store-read chain} in $\E$ is a list of $m$ tuples of the form
    $(\E^1_i=\ddfrac{\overset{\E^0_i}{\langle a_i, \sigma_i \rangle \downarrow \top_n}}
    {\langle X_i \coloneqq a_i, \sigma_i \rangle \downarrow \sigma_i[X_i \mapsto \top_n]},
    \E^2_i=\ddfrac{}{\langle X_i, \sigma_i' \rangle \downarrow \sigma_i'(X_i) = \top_n})$
    with $i \in \{1, \ldots, m\}$.
    $\E^0_i$, $\E^1_i$ and $\E^2_i$ are subderivations of $\E$.
    We require that the following holds:
    \begin{enumerate}
        \item $\sigma_i(X) \neq \top_n$ (so we have a first store). 
        \item For $i \in \{1, \ldots, m-1\}$ it holds that 
        $\E^0_{i+1}=\E^2_i$. This means that every store except the first one
        in the chain is connected via the variable it reads from to the previous 
        store in the chain.
        \item Repeating tuples are not allowed in the list, all tuples have to 
        be pairwise disjoint --- equality on subderivations of $\E$ includes
        placement in $\E$, not just equality of the subderivation trees.
    \end{enumerate}
\end{definition}

\begin{remark}
    By definition a read-store-chain does not contain any cycles.
    As the derivation $\E$ is finite, this implies that the set of 
    read-store chains is finite.
    By defining the obvious partial order induced by inclusion
    on the set of read-store-chains, we see that 
    every non-empty read-store-chain can be extended to a maximal 
    read-store-chain.
\end{remark}

\begin{lemma}
    \label{lem:chain-to-graph}
    Any store-read-chain induces a path in $E^0_{\sigma_0}$ from the node 
    of the first store in the chain to the node of the last read in the chain.
    Furthermore, there is an edge from the expression that the first store reads
    to the first store.
\end{lemma}
\begin{proof}
    We proof the lemma by induction over the length of the store-read chain.\\
    \textbf{Base Case (m=1):}
    There is by definition of $\E^0_{\sigma_0}$
    one edge of type (3) connecting the store and the read in the chain.
    Furthermore, there is an edge of type (1) connecting the read-from expression
    to the store.\\
    \textbf{Inductive Case:}
    Let $m \geq 1$ be fixed. By the IH, there is a path in $E^0_{\sigma_0}$
    corresponding to the chain of length $m-1$ from the first store in the chain 
    to the read of $X_{m-1}$ in the chain.
    By definition of the chain, we have an edge of type (1) connecting 
    the read of $X_{m-1}$ to the store of $X_m$.
    Furthermore, again by definition of $\E^0_{\sigma_0}$
    there is one edge of type (3) connecting the store of $X_m$ to the
    read of $X_m$.
\end{proof}

\begin{lemma}
    \label{lem:max-store-read}
    Any maximal store-read-chain contains a store from the source as first store.
\end{lemma}
\begin{proof}
    We look at the derivation $\E^0_1$.
    If $a = Y$ for some $Y \in \textbf{Loc}$, then the store-read-chain is not 
    maximal, because it can be extended at the front by a tuple with a store to and a
    read from $Y$.
    The only other possibility for an arithmetic expression $a$ to evaluate to 
    $\top_n$ is that it is of form $\textbf{source } a_0$.
    This concludes the proof.
\end{proof}


\begin{proof}[Proof Of~\autoref{thm:min-dg}]
    Let $\sigma_0$ be an initial store such that $(c, \sigma_0)$ has dataflow.
    If no such $\sigma_0$ exists, by the definition of soundness
    the proof is concluded.

    Let $\E$ be the bigstep derivation of
    $\langle c, \sigma_0 \rangle \downarrow \sigma'$.
    Because $(c, \sigma_0)$ has dataflow, there exists a derivation of the form 
    \begin{equation*}
        \E_1 = \textsc{EC-Sink1}\ddfrac
        {\overset{\E_2}{\langle a, \sigma \rangle \downarrow \top_n}}
        {\langle \textbf{sink } a, \sigma} \downarrow \sigma
    \end{equation*}
    There are two possibilities for the structure of a.\\
    \textbf{Case 1:} $a = \textbf{source }a_0$\\
    Then $E^0_{\sigma_0}$ contains an edge of type (2) that connects the source 
    to a sink. This constitues a path from the source to a sink.\\
    \textbf{Case 2:} $a = X$\\
    Then $X$ is a variable read that evaluates to $\top_n$.
    Because $\sigma_0(X) \neq \top_n$, there has to be a first store
    of $X$ in $\E$.
    The derivation of that store, together with the derivation of the read make 
    up a store-read chain of length one.
    This chain can be extended to a maximal chain, and 
    by~\autoref{lem:max-store-read}, we know it starts with a store reading the
    source.
    Furthermore, by~\autoref{lem:chain-to-graph} we get a path in $E^0_{\sigma_0}$
    from the source node to the read. The read is then connected by an edge
    of type (2) to the sink, thus we have a path from the source to a sink in 
    $E^0_{\sigma_0}$.

    In both cases we have shown that $E^0_{\sigma_0}$ contains a path from the 
    source to a sink. Thus, $E^1_{\sigma_0}$ and in turn $E$ contain a path
    from the source to a sink, and $\A_{G_\text{min}}(c) = \text{HAS\_FLOW}$.
    This concludes the proof.
\end{proof}

\begin{corollary}
    \label{cor:dg-sound}
    For any dataflow graph $G(c)$ the algorithm $\A_{G}(c)$ is a sound
     dataflow algorithm.
\end{corollary}
\begin{proof}
    Remember that any dataflow graph contains the minimal dataflow graph as 
    subgraph. Thus for every pair $(c, \sigma_0)$ that has dataflow
    there is a path from the source node to a sink node, 
    thus $\A$ reports flow by~\autoref{thm:min-dg}.
    It doesn't matter if $G$ has more edges for soundness.
\end{proof}

\begin{remark}
    The \emph{minimal} dataflow graph gets name by being the minimal dataflow graph
    among all dataflow graphs that are considered in this paper.
    It is not the smallest graph $G$ on the node set 
    such that $\A_G(c)$ is a sound dataflow algorithm,
    because $G_\text{min}$ contains back-edges for loops that would, strictly
    speaking, not be necessary to make $\A_G(c)$ a sound dataflow algorithm.
\end{remark}

\begin{remark}
    Note that not all (sound) dataflow algorithms have the form $\A_G(c)$.
    For example the trivially sound dataflow algorithm disagrees on the 
    program $\textbf{skip}$ with any dataflow algorithm of the form 
    $\A_G(c)$ - the trivially sound
    dataflow algorithm outputs HAS\_FLOW, whereas
    $A_G(\textbf{skip}) = \text{NO\_FLOW}$ because the node set contains
    neither source nor sink.
\end{remark}

\subsection{An Algorithm to Compute A Dataflow Graph}
In this section we describe an algorithm $\B(c) = G$ that computes a dataflow 
graph $G$. We then prove that this algorithm indeed
computes a dataflow graph, and, using~\autoref{cor:dg-sound},
$\A_G(c)$ is a sound dataflow algorithm.

\subsubsection*{The Algorithm}
Given a program $c$ in SSA form, the algorithm computes
the edge set $E$ as a least fixed point iteration.
The following recursive rules are used:\\
\textbf{Base Case:} Start at the expression $\textbf{source }a_0$.
If it is used in either the context $X \coloneqq \textbf{source }a_0$
or $\textbf{sink }(\textbf{source }a_0)$ then add an edge from the node for the
source to the node of the store.\\
\textbf{Recursive Case:} For any store node $X \coloneqq a_0$, enumerate all uses (traverse
the def-use-chain of the SSA form). Add edges to all uses that are themselves
either stores or \textbf{sink} commands of the form $\textbf{sink } X$.
Note that stores of form $Z \coloneqq \phi(X, Y)$ get an incoming edge from $X$,
if the definition of $X$ itself has an incoming edge.

The fixed point iteration stops eventually as the set of possible edges is
finite. Furthermore, if the program doesn't contain a \textbf{source} expression,
the edge set will be empty.

TODO: Add a second pass, like in QL, that starts at the sink nodes and thus
filters out edges that don't lead to any sink.

\subsubsection*{Soundness Proof}
\begin{theorem}
    The algorithm $\B(c) = G$ computes a dataflow graph, and thus 
    $\A_G(c)$ is a sound dataflow algorithm.
\end{theorem}
\begin{proof}
    Because of~\autoref{cor:dg-sound}, we only need to show that $\B(c)$ is 
    a dataflow graph.
    To do that, we show that for $G=(V, E)$, for all initial stores $\sigma_0$,
    all edges in $E^0_{\sigma_0}$ (as in the definition of the minimal dataflow
    graph) are contained in the set $E$.

    Let $\sigma_0$ be an initial store.
    Let $e$ be an edge in $E^0_{\sigma_0}$.\\
    \textbf{Case 1:} $e$ is an edge of type (1).\\
    Then the end node of $e$
    has form $X \coloneqq a$.
    Then there are two cases for the structure of $a$.\\
    \textbf{Subcase 1:} $a = \textbf{source }a_0$.
    Then there is a corresponding edge in $E$ per the base case of the recursive
    definition of $E$.\\
    \textbf{Subcase 2:} $a = Y$ for some variable $Y \neq X$ (remember, we have 
    a SSA form)
\end{proof}


\subsection{A Path-Sensitive Dataflow Algorithm}
TODO: description of path-pruning, proof that the only pruned edges are not
in the definition of the minimal dataflow graph, thus it is still sound

%However, as a dataflow algorithm cannot be sound and complete at the same time,
%dataflow algorithms constitute an heuristic.
%The evaluation of heuristics in a theoretic setting is difficult.
%We do aim to show that 
% TODO: A_2 is better than A_1, as both sound+one has less FP
