% !TEX root = main.tex
\section{Description of the Dataflow Library}
Designing a dataflow library which works in theory is a task that is not too difficult.
However, designing a dataflow library which performs well on projects with millions of lines of code is 
considerably more difficult.
In this section, we will describe the dataflow library for the CodeQL platform % TODO check marketing material, is this correct?
for analyzing programs in Java.
We will describe the version without our contributions which are outlined in the next sections.

As the dataflow library is used for bug finding, it is neither sound nor complete.
In general, the dataflow library reports \emph{possible} dataflow, i.e.\ as soon
as there is a path in the control flow graph TODO flow is reported.
On the other hand, sometimes dataflow paths that do exist are not reported by the 
dataflow library because finding them would be prohibitive performance-wise.
These limitations will be explained later in more detail.
However, care is taken to be as precise as possible without sacrificing performance. 

The dataflow library tracks only the flow of exact values.
For example, dataflow of a string is interrupted when \java{toUpperCase()} is called.
However, using functions like \java{toUpperCase()} does not prevent XSS or SQL injection attacks.
To find such paths that do not interrupt flow when the content of a variable still depends 
heavily on the input data, the \texttt{TaintTracking} library exists.
It is built on top of the dataflow library and provides a generic interface to enable
finding of bug classes XSS, SQL injections or directory traversals.
While the taint tracking library is probably the biggest user of the dataflow library,
it is implemented strictly on top of the dataflow library.
Thus, it is not discussed here further, except to highlight the hooks in the dataflow 
library on top of which the taint tracking library is built.
Because of that design, all improvements made to the dataflow library immediately
also benefit taint tracking.

\subsection{Interface}
Before we delve into the internals of the dataflow library, we will describe the API 
of the dataflow library users will encounter. %, as well as general design principles.

\subsubsection*{Nodes}
The main concept used throughout the dataflow library is that of a \texttt{Node}.
Every element in the program holding data is a node.
The dataflow library is concerned with the task to identify 
under which circumstances there is flow between nodes.
The set of nodes and the set of dataflow steps between those nodes make up the 
\emph{dataflow graph}.

The type \texttt{Node} has several subtypes, corresponding to different kinds of nodes.
The most important subclass of \texttt{Node} is \texttt{ExprNode}, 
as every expression in the program corresponds to a \texttt{Node}.
Another important subclass is \texttt{ParameterNode}.
Both the implicit \texttt{this} parameter for methods and all explicit parameters
given to methods are introduced in a method via these nodes.
In a call, data flows from the node of the argument of the 
call to the corresponding parameter node in the callee.

Objects that potentially change through an operation are modelled
 via \texttt{PostUpdateNode}s.
Objects that have a method called on them (potentially mutating it),
field accesses on an object and parameters to method calls (the callee could mutate the object)
have two nodes for that expression - one so-called pre-update node, that represents 
the object before the operation, and a post-update node that represents 
the object after the operation.
This becomes more clear when looking at the statement \java{a.field = "string"}.
In this example, there exists an expression node for 
\java{"string"}, an expression node for the instance access \java{a.field},
an expression node for \java{a} and an expression node for the entire assignment.
However, none of these expression nodes capture the fact that the object \java{a}
is mutated by the assignment.
To represent that fact a \texttt{PostUpdateNode} for \java{a} is introduced,
represting that the assignment (potentially) mutates the object \java{a}.
The corresponding pre-update node is the expression node for \java{a}.

Other subtypes of the \texttt{Node} class exist as well, 
but are less important to the bigger picture of understanding the dataflow library.
This subtypes are more technical in nature and correspond for example 
to instance creation, implicit reads of the \texttt{this} parameter or
the desugaring of vararg method calls.

One peculiarity of the \texttt{Node} class is that the dataflow graph
is stored in separate predicates, not on the nodes making up the graph itself.
This means that the graph is stored in an adjacency matrix, where the
 adjacency matrix is encoded in the tuple set of a predicate.
Different kinds of dataflow graphs can be represented by different predicates,
using the same nodes.
% TODO elaborate

\subsubsection*{Configuration}
The \texttt{Configuration} is the main class a user interacts with.
A configuration defines which nodes are sources and sinks of dataflow.
Furthermore, a configuration can also specify additional steps which yield flow.
In the example, a configuration could add that a call to 
\texttt{toUpperCase()} on a string still provides dataflow.
On the other hand, configurations can also specify nodes which block flow.
In the example, this could be sanitizer methods which for example strip HTML tags.
This enables the user to filter out paths which are known to be safe.

The most important predicate on the \texttt{Configuration} class 
is the \texttt{hasFlow(Node source, Node sink)} predicate.
With that predicate users can query the dataflow relation.
It holds whenever there is dataflow starting at the given node \texttt{source} 
(which has to be a source) which flows to the given node \texttt{sink} (which has to be a sink).
Note that this interface does \textbf{not} provide information about specific
dataflow paths.
It also limits dataflow queries to dataflow between the (previously defined) sources and sinks.
This design limitation is intentional, because it allows performance optimizations
that make it feasible to compute whole-program dataflow.

The user can disable the use of field flow.
That improves performance and allows the dataflow library to explore all branches
from virtual dispatch.
If field flow is enabled, the user has to set a limit on the number of branches 
allowed for virtual dispatch in the presence of field flow.
Methods calls via virtual dispatch are only considered as flow targets if there are 
sufficiently few possible implementations to analyze.
This restriction is needed to prevent the combinatorial explosion stemming from
fully respecting virtual dispatch and field flow.
This is explained in more detail later.

\subsection{General Design}

In theory, implementing a dataflow algorithm in a logic programming language is very simple.
Given the \texttt{Node} concept discussed above, one only needs to provide a relation
\texttt{step(Node node1, Node node2)}.
This step relation holds if \texttt{node1} has dataflow in one step to \texttt{node2}
(local and global steps!).
Then, the transitive closure of \texttt{step} has all possible dataflow paths in the program.
These dataflow paths then can be filtered so that only paths which begin in a source and 
end in a sink are considered.

In practice however, this example wouldn't perform well at all.
First, note that the number of \texttt{Node}s grows roughly linear in the size of
the program.
Second, the transitive closure of the \texttt{step} relation enumerates all dataflow
paths in the program from one node to another.
It is defined on the Cartesian product of \texttt{Node} with itself that is roughly quadratic 
in the size of the program.
We would end up with a predicate that grows probably superlinear in the program size.
The CodeQL tools are capable of handling very large programs,
for which algorithms with superlinear behaviour typically don't scale,
so a more involved approach to compute dataflow is needed.

On a very high level, the algorithm used in the dataflow library is very similar 
to the algorithm sketched above.
The last step of it is indeed taking the transitive closure of a step-relation.
However, a lot of care is taken to define the step relation only on suitable 
nodes that will probably be part of a dataflow path from a source to a sink.
For example, nodes that cannot be reached from any source via a dataflow path 
are ignored.

\subsection{Description of the algorithm}
We will now present the details of how dataflow is computed for Java programs.
First, the algorithm runs multiple passes of finding candidate nodes that are part 
of dataflow paths.
Every pass refines the set of nodes by computing more accurate information about the 
possible dataflow.
Then the algorithm computes a step relation and takes the transitive closure of that.
The implementation of the algorithm is divided into a generic dataflow algorithm that is 
used for Java, C/C++ and C\# and a specific language-specific implementation of a common interface.
In this description, we are glossing over the separation of those two modules and 
describe the algorithm formed by the generic module together with the language-specific
module for Java.
The implementation of the dataflow library for Java encompasses roughly 4000 lines of
QL code. Describing every single detail of the implementation would be neither feasible nor 
useful. Thus, some details are omitted or the implementation of a particular step 
of the algorithm is only described on a very abstract level.
Other parts of the algorithm are described more in-depth.

\subsubsection*{Terminology}
Before we can start with the steps of the algorithm, we need to introduce some terminology.
The \emph{virtual dispatch} library provides functionality to resolve calls of 
methods through interfaces and class hierarchies.
For any given method call, it provides a list of all potential targets of that
call site.
With an optional provided call context, it can further narrow down the list of
possible method implementations.
\begin{listing}
    \begin{javacode}
        interface I {
            public void f();
        }
        class A implements I {
            @Override
            public void f() {
                // Implementation A
            }
        }
        class B implements I {
            @Override
            public void f() {
                // Implementation B
            }
        }
        class User {
            public void g(I i) {
                i.f();
            }
            public void h() {
                A a = new A();
                g(a);
            }
        }
    \end{javacode}
    \caption{Example code for virtual dispatch resolution}
    \label{lst:virtual-dispatch}
\end{listing}
In the example~\autoref{lst:virtual-dispatch}, the \java{interface I} defines a
 method \java{f()}, and the two 
classes \texttt{A} and \texttt{B} both implement the interface \texttt{I}.
Without call contexts, the virtual dispatch library would resolve the call 
of \java{f()} in the method \java{User.g(i)} to both 
implementations in \texttt{A} and \texttt{B}.
However, if call contexts are provided to the virtual dispatch library 
it is able to infer that when \java{User.g(i)} is invoked by the method 
\java{User.h()} it only can dispatch to the implementation of \texttt{f()} in 
\java{class A}.
This functionality is orthogonal to the dataflow library, and the implementation 
of virtual dispatch is not described.

A \emph{local} flow step is a dataflow step where data flows from one node to another
and both nodes are inside the same callable.
Before local flow can be computed, an SSA form is computed first.
Some examples of local flow steps are:
\begin{itemize}
    \item Data flows from the right-hand side of a variable assignment to a 
    first read of that variable
    \item Data flows from a variable read to an adjacent variable read (a use-use chain in SSA lingo).
    That means that both nodes read the same variable, and there is no other operation between the 
    reads, except possibly the presence of $\phi$-nodes.
    \item Data flows from a parameter node to a first read of the parameter
    \item Data flows from the inside of an expression in parentheses to the dataflow node
    representing the outer expression, i.e.\ in the expression \java{(var)} there is dataflow from the node 
    representing the variable read \java{var} to the dataflow node representing the entire expression \texttt{(var)}.
    \item Data flows from a post-update node that represents an object to an adjacent read 
    of that object
\end{itemize}
There are some more 
 % TODO check again, we're talking about single steps here, not entire graphs!
 % This might be a bit early here
Note that, unlike for global dataflow, the entire local dataflow graph for each function 
is computed.
This is necessary because the local dataflow is an important building block in computing
the global dataflow.
A priori it is not clear whether a local dataflow path is part of a (global) dataflow path
from a source to a sink, thus it is not possible to filter paths based on that condition.
As methods are much smaller in relation to the whole program, computing the whole local 
dataflow graph is still feasible and no combinatorial explosion occurs.

A \emph{jump} flow step is a dataflow step that jumps between callables 
in a way that ignores call contexts.
In Java, there are two possibilities for a jump step to occur.
\begin{itemize}
    \item \textbf{Static fields:} Data flows from an assignment to a static field to a read of that static field,
    if the field read is not determined by SSA.
    The field read is only determined by SSA, if a local value is assigned to the static field.
    However, in most cases the static field will be assigned to in one callable, and read 
    in another.
    \item \textbf{Closures:} Data flows from a use or a definition of a variable in the outer callable 
    to a use of a captured variable in the inner function.
\end{itemize}

An \emph{additional} (local or jump) flow step is a dataflow step that the user 
provides via the \texttt{Configuration}.
The prime example of additional flow steps are taint steps.
In Java, string concatenation is a taint step, but not a (regular) dataflow.
The object that flows from the source to the sink is changed by the concatenation,
but for example adding a newline to the end of a string makes it still dangerous
to use in a SQL query without further checks.
Note that an additional step (by definition) does not preserve the value of the
variable in question, as all value-preserving steps are already contained in the
dataflow library.

A \emph{barrier} is a node that prevents dataflow.
Barriers are user-specified via the \texttt{Configuration}.
They are used in the taint tracking library to model sanitizing steps.

% TODO remove this, check for other references to it
%An \emph{extended return} is a generalized return that representing dataflow out of a callable.
%It is either a return statement, which we call a \emph{value return} or a parameter update.
%Parameter updates are often called having an \emph{out parameter}.
%The programmer supplies the reference to an object in a parameter, and the method then
% TODO check what exactly out parameters are in Java, have a look at https://lgtm.com/query/4328056797028651291/
% Should be in fact vacuous

\subsubsection*{Field Flow and Access Paths}
The dataflow library has support for (an approximation of) storing the data in a
field of an object. The details of the approximation will be described later, but
we will introduce the concepts now.
Whenever data is stored in an object, we need to switch the data we are tracking ---
before a store, we track an object that originated at a source, after a store the object
containing the field needs to be tracked, together with the information in which
field the originally tracked data is stored in.
Conversely, when reading out of the field, we need to switch back from tracking
the containing object to tracking the data we were originally interested in.
Because objects can be stored in objects, just keeping track of the state 
direct tracking versus tracking a field stored in an object is not enough.
Instead, we keep track of a list of all the fields objects were stored into along 
the current path.
We call such a list an \emph{access path}.
This access path provides the needed context to track dataflow through fields.
The access path is empty if the data that originated from the source was not 
stored in any field.
The \emph{head} of the access path indicates which field was last stored into,
if the access path is non-empty.
The access path is treated as a stack-like datastructure.
At every field store, the field is pushed to the access path, thus the head is
replaced by the current context.
At every field read, we pop the head from the access path which restores the old context.
Using the access path, we can precisely track field flow.

\subsubsection*{Flow Through Methods}
As a preparatory step, the dataflow algorithm summarizes methods that at some point 
return one of their parameters to a single dataflow step.
This is independent of the configuration, and does not take into account
neither additional steps nor barriers.
Later steps in the algorithm offset these shortcomings.
The dataflow from the argument node to the node for the value returned by
the method is thus represented by only a single edge.
\begin{listing}[H]
    \begin{javacode}
        class A {
            String name;
        
            public void setName(String name) {
                this.name = name;
                return this;
            }
        }
    
        class B {
            public void caller() {
                A a = new A();
                a.setName("a name");
            }
        }
    \end{javacode}
    \caption{Example code for flow through method summarization}    
    \label{lst:flow-through-example}
\end{listing}
In the code example in~\autoref{lst:flow-through-example}, there is dataflow of the instance
of class \java{A} to the method \java{setName}, because the instance a 
is (implicitly) passed as \java{this} parameter.
Because of the return statement, there is dataflow back from \texttt{setName} to
\texttt{caller}.
After the summarization step, the dataflow in line 13 is described in a single step,
from the node representing \texttt{a} as implicit argument to the method call, to the node 
representing the expression \java{a.setName("a name")}.

This summarization helps the dataflow algorithm in several ways.
In the first passes of the dataflow algorithm, we do not keep track of call contexts
for method calls, as that would be too expensive.
However, that means that calls and returns are not matched up, and every return out of 
a method flows to any of the callsites.
For example, some programmers write their setters in a way that they always return \java{this},
as that allows chaining setters.
If one of these setters has 100 call-sites and we wouldn't match up the callers with the callees,
the dataflow analysis would be very imprecise.
Using method summaries is an inexpensive way
to rectify that problem, as they match up callers and callees.
It also ensures that any dataflow path first returns out of methods, and only
then enters other methods. As soon as a path starts entering methods, it can 
never encounter return steps anymore.
Furthermore, summarization provides shorter paths with fewer nodes involved.
This helps performance.

The summarization computes for every parameter a relation with all nodes 
that are reachable by taking local (value-preserving) dataflow steps from that parameter.
This means all dataflow nodes that receive that parameter are marked.
Then a \emph{parameter flows through a method} if the parameter reaches a 
dataflow node associated with a return statement.
This information is then used together with the virtual dispatch analysis
(not described here) to match up arguments to parameters, yielding a predicate 
that describes all arguments that flow through their method calls.
These predicates are mutually recursive.
This means that in the setting of an outer method that calls an inner 
method both methods are summarized if a parameter flows through both of them.

The summarization is run in two passes.
The first pass works exactly as described above.
Its output is then used to detect getter and setter methods.
This relies on the predicate that indicates which dataflow nodes in a method are
reachable from a parameter.

A \emph{getter} is a method where data flows from a parameter node 
(the implicit \java{this} argument to methoods) to a field read (or recursively another getter)
and then to a return statement.

A \emph{setter} is a method with (at least) two parameters,
where data flows from one parameter to a field store (or recursively another setter),
and the store updates a field in an object that was provided as a parameter as well.

The second pass enhances the detection of method calls where an argument flows through 
by also taking \emph{local store-read steps} into account.
These are stores of data into a field, that are read later, and both 
the store and the read happen in the same method.
This second pass relies on the first pass, as the store and read steps take the 
(previously computed) getters and setters into account.

\subsubsection*{Node Filtering --- Phase 1 forward}
Before computing a whole dataflow graph for a program,
the algorithm identifies several sets of candidate nodes that are possibly part of 
that dataflow graph.
These sets are restricted further and further with every filtering pass.
These sets are configuration-specific.
They take sources, sinks, barriers and additional steps from the configuration into 
account.
Note that in the description of the passes, \emph{candidate node} always refers
to being a candidate node of that pass, unless otherwise noted.

The first pass is done in the predicate \texttt{nodeCandFwd1}.
It uses a flooding-style algorithm to compute all nodes reachable from the source nodes
using dataflow steps, while ignoring all call contexts.
It is a huge overapproximation of the node set of the dataflow graph 
but, because of its simplicity, cheap to compute.
A \texttt{Node n} is a candidate node if it is not a barrier node and 
either of the following is true:
\begin{itemize}
    \item The node \texttt{n} is a source node.
    \item There exists a candidate node \texttt{mid} such that \texttt{n} is reachable with a single flow step,
    either a local step, a jump step or an additional step.
    \item There exists a candidate dataflow node \texttt{arg}, such that 
    \texttt{arg} is an argument in a call, and \texttt{n} is a dataflow node 
    representing the parameter in the callee.
    In this case, we say that we have dataflow \emph{into} a callable.
    \item The opposite case is that we have dataflow \emph{out} of a callable.
    That happens when the node \texttt{n} is a node representing a value returned by 
    a call, either directly or by being a \texttt{PostUpdateNode} that
    represents a parameter that is possibly modified by a call.
    Furthermore, there exists a candidate node that returns from the callee.
    \item The node \texttt{n} is a \texttt{PostUpdateNode}, representing the state of an 
    object immediately after a field store (either direct, or by using a setter).
    Furthermore, the data that is stored needs to be a candidate node already.
    \item The node \texttt{n} takes the result of a field read 
    (either directly, or via a getter), the node from which we read is already a
    candidate node \textbf{and} the field we read from is stored to 
    by a candidate node.
\end{itemize}
In the case of calls and returns, virtual dispatch is taken into account,
so calls branch out to all possible targets, and returns go back to all possible callers.

Field flow is only considered when the field flow is enabled in the configuration.
If field flow is enabled, we do not only track candidate nodes, as computed by 
\texttt{nodeCandFwd1}, but also candidate fields in which we possibly store as
part of a dataflow path.
These fields are tracked by the predicate \texttt{storeCandFwd1}, which is mututally 
recursive with \texttt{nodeCandFwd1}.
While we do not track any access paths in this first pass,
the candidate fields can be understood as (a superset of) the nodes that make up 
access paths, as only fields in which we store can be a part of an access path.

\subsubsection*{Node Filtering --- Phase 1 backwards}
The second pass is done in the predicate \texttt{nodeCand1}.
The second pass filters the results from the first pass by only keeping nodes
that also are reachable from the sinks when taking all the steps backwards.
This pattern is used for all the candidate node filtering steps --- first, we 
filter the node set starting from the sources and taking steps forward,
and then we filter that set further by starting with the sinks and taking steps backwards.

Like \texttt{nodeCandFwd1}, \texttt{nodeCand1} ignores all call contexts.
All cases are symmetric to the ones in \texttt{nodeCandFwd1}.
In detail, this means a \texttt{Node n} is a candidate node as considered by 
\texttt{nodeCand1} if it is contained in 
\texttt{nodeCandFwd1} and either of the following is true:
\begin{itemize}
    \item The node \texttt{n} is a sink node.
    \item There exists a candidate node \texttt{mid} such that \texttt{mid} is reachable with a single flow step,
    either a local step, a jump step or an additional step from the node \texttt{n}.
    \item Flow into a callable:
    There exists a candidate node \texttt{param} that is a parameter of a callable,
    such that \texttt{node} is a possible argument that gets passed to the callable.
    \item Flow out of a callable:
    There exists a candidate node that is a return statement, and \texttt{n} is 
    a node in a caller, receiving the returned value.
    \item The node \texttt{n} is an object from which a field \texttt{f} is read,
    where the value of the read flows to a candidate node, and \texttt{f}
    is a candidate field as identified by the predicate \texttt{storeCandFwd1}.
    \item The node \texttt{n} is stored into a field \texttt{f}, where the
    object after the store is already a candidate node, \texttt{f} is a
    candidate field as identified by the predicate \texttt{storeCandFwd1}
    \textbf{and} the field is read from by a candidate node.
\end{itemize}

Again, if field flow is enabled, we also track candidate fields that could make up 
access paths. This predicate is called \texttt{readCand1} and is a subset of 
\texttt{storeCandFwd1}.
It tracks the fields that are read from in a path from the sink to a source.

Both passes in phase 1 take virtual dispatch into account by dispatching to all possible callees
for each call site. Only later phases use a more advanced analysis to use the call context 
to restrict the virtual dispatch targets.
Note that no type checking is done in phase 1.
TODO elaborate on type checking

\subsubsection*{Node Filtering --- Phase 2 forward}
The second phase of node filtering works very similar to phase 1 --- it 
operates on all nodes which are present in \texttt{nodeCand1} and filters them further.
It is again implemented in a flooding-style algorithm with a forward and backward 
pass, like in phase 1.
However, is starts taking very simple call contexts and access paths 
into account to make the analysis more precise.
In phase 2 both the call context and the access path are approximated by booleans.
The boolean for the call context tracks whether the data flow entered through 
a parameter or no.
The boolean for the access path tracks whether the access path is empty or not.


The forward pass is implemented in the predicate \texttt{nodeCandFwd2}.
It is different in the following steps from phase 1:
It skips over methods where the argument value 
flows through and uses the summarized dataflow instead.
To take additional non-value-preserving steps into account,
summaries are computed for methods which take at least one
non-value-preserving step as well.
Flow out of a callable is only taken into account when the call context 
indicates that we did not end up in the current node via a method call.
This excludes the non-summarized paths from the predicate.

Whenever we encounter a store, we simulate pushing to the head of the access path 
by setting the flag indicating that the access-path is non-empty.
Non-value-preserving steps are only allowed when the access path is empty.
When the access-path is non-empty, we are tracking an object in which the tracked 
data is stored. However, when a non-value-preserving step is applied to that object,
object identity does not hold anymore, and the value of the field with the tracked
data is lost.
Thus, we do not need to track it for dataflow anymore.

Whenever we encounter a read, we need to simulate a pop operation on the access path.
However, with the limited information we track, it is not possible to determine 
whether the access path is empty after one read or not.
Thus, we continue with both options considered possible.

Additionally, both reads and stores are only considered when they appear in the 
predicate \texttt{readCand1}.
Furthermore, just like in \texttt{nodeCandFwd1}, the predicate 
\texttt{storeCandFwd2} tracks all fields that are stored into in the dataflow
covered by this pass.
As we only consider fields from \texttt{readCand1}, it is a further refinement 
of that set.

Whenever we encounter flow into or out of callables while having a non-empty 
access path \textbf{and} we encounter virtual dispatch,
we check that the branch limit holds.
Virtual dispatch targets which branch out a lot are ignored when tracking objects 
into which we stored earlier. What exactly a lot means is determined by the 
configuration.
This pruning obviously eliminates real dataflow paths from the results, but is needed
to make the analysis feasible.

\subsubsection*{Node Filtering --- Phase 2 backwards}
The exact same filtering as done in phase 2 forwards from the sources to the sinks 
is afterwards applied going backwards from the sinks to the sources.
The predicate for this is \texttt{nodeCand2}.
Furthermore, it also refines the potential fields making up access paths even further,
only recording fields that are present in \texttt{storeCandFwd2} and that are 
also read from in the flow covered by \texttt{nodeCand2}.
The fields are stored in the predicate \texttt{readCand2}.


\subsubsection*{Local Flow Summarization --- The Bigstep Local Flow Relation}
Before filtering the node set even further, the local flow steps are summarized 
into the so-called bigstep relation.

The local flow step relation contains very detailed dataflow information.
For example, the flow from a parameter to a method call 
is in general not a single step.
Indeed, every use of the parameter along the control flow graph until the method 
call is a part of the dataflow.
Having a such refined model of the dataflow is good for preciseness, but 
reporting that dataflow path to the user would not be useful.
Furthermore, it incurs quite a performance overhead, as a lot of nodes and edges 
are needed to model local dataflow on that level.
To address those problems, a coarser version of the 
local dataflow step relation is computed.

However, a simple transitive closure over the step relation would lose too much 
information, as it would skip over interesting local flow steps,
like storing the data in a field, or calling methods with the information as 
argument.

The \emph{local flow bigstep relation} strikes a balance between the refined 
local flow step relation and its transitive closure.
It conflates steps until it reaches nodes that are considered to be interesting.
The intent is to skip over irrelevant steps for the programmer, while still 
reporting enough steps in the dataflow graph that the programmer can understand 
how the dataflow is happening.
It thus stops conflating steps when it encounters casts, flow into or out of callables,
jump steps, calls to methods where the argument value flows through (see above), field 
stores and reads as well as when it reaches a sink. 
Cast nodes are important stopping points, because in later steps the dataflow 
algorithm actually prunes path based on the type information to rule out 
paths where the type is not compatible.
The bigstep relation also keeps track if non-value-preserving steps have been used 
in the summarization or not, because as explained above, those steps are only allowed 
when the current access path is empty.

\subsubsection*{Node Filtering --- Phase 3 forward}
The next filtering step filters the candidate nodes from \texttt{nodeCand2}
and the candidate fields for the access paths from \texttt{readCand2} even further.
It is implemented in the predicate \texttt{flowCandFwd}, that stores the candidate nodes,
and in the predicate \texttt{consCandFwd} that tracks the fields that potentially 
make up the access path.

The main differences to \texttt{nodeCand2} are that the bigstep relation is 
used for local flow (thus reducing the number of nodes needed), 
a more precise approximation of the access path than just a boolean,
and type-checking along the considered paths.
Call contexts are still approximated by only a boolean that indicates whether 
in the current context, dataflow came from a method parameter.

To approximate the access path, we now store the front of the access path.
The front of an access path is either Nil if the access path is empty,
or the head of the access path (a field).
This is clearly more accurate than the boolean that just stores whether the 
access path is empty.
This approximation relies on the observation made by Semmle engineers that
in most programs the current head of the access path is a
good predictor for the next element of the access path.
Furthermore, given the current head of an non-empty access path, we get the 
type of the currently-tracked object for free.
Thus, by tracking type information when the access path is empty, we can type-check
the whole path quite cheaply.

The access path handling is implemented in the following way:
Sources enforce that the access path front is Nil.
Furthermore, non-value-preserving steps can only be used in contexts with
a Nil access path front.
Again, like in phase 2, we ignore calls to methods that branch out a lot when the
access path front is Nil.

When encountering a field store, we simulate a push to the access path by replacing 
the current access path front with a new one that uses the stored-to field 
as head.
This operation stores exactly the current content.

However, when encountering a field read, we need to simulate a pop operation.
As we throw away the information about the remainder of the access path as soon as 
we encounter a store, we have no reliable way to know what to set the access path front 
to.
Thus, we explore all possible options and consider all access path fronts that could fit.
This becomes a bit more clear when looking at the following example:

\begin{listing}[H]
    \begin{javacode}
        class A {
            public String stringField;
        }
        class A1  extends A {
            public String f1;
        }
        class A2 extends A{
            public String f2;
        }
        class O1 {
            public Object fo1;
        }
        class O2 {
            public Object fo2;
        }

        class Flow {
            // flow enters via the parameter source, access path front is Nil, 
            // type is String
            public void flowMethod(String source) {
                A1 a1 = new A1();
                // field store, access path front changes to the head for 
                // the field f1 in class A1
                // the new type of the object we're tracking is A1
                a1.f1 = source;
                O1 o1 = new O1();
                // field store, access path front changes to the head for
                // field a1, class O1
                // the new type we're tracking is O1
                o1.a1 = a1;
                // field read, access path front needs to be restored.
                // two options, either O1.a1, or O2.a2
                // the new type we're tracking is A
                A1 a1New = o.a1;

            }         
            public void flowMethod2(String source) {
                A2 a2 = new A2();
                // field store, access path front changes to the head for 
                // the field f2 in class A2
                // the new type of the object we're tracking is A2
                a2.f2 = source;
                O2 o2 = new O2();
                // field store, access path front changes to the head for
                // field a1, class O1
                // the new type we're tracking is O1
                o2.a2 = a2;
                // field read, access path front needs to be restored.
                // two options, either O1.a1, or O2.a2
                // the new type we're tracking is A
                A2 a2New = o.a2;

            }


        }
    \end{javacode}
\end{listing}

The example assumes that at some other point in the program, as part of some dataflow 
from a source to a sink we encounter a store and a read in the field \texttt{a2} of the class 
\texttt{O2}.

Types along dataflow paths are mostly checked by the compiler already, it is 
only a few places where the final type check is either deferred to the runtime 
(casts) or the dataflow analysis could get confused.
Thus, we prune using the recorded type information only at nodes that represent 
casts, parameters and returns.
These nodes are only candidate nodes if their type 
matches the type recorded in the access path front.

\subsubsection*{Node Filtering --- Phase 3 backwards}
This pass is TODO symmetric to the forwards pass.
\subsubsection*{Node Filtering --- Phase 4 forward}
Full call contexts, even better access path approximation 
\subsubsection*{Node Filtering --- Phase 4 backwards}

\subsubsection*{Dataflow Graph Computation}


% TODO: why localFlowStep flows through casts?
% simpleLocalFlowStep (Util): Follows SSA adjacent uses and def-first-use.
%   Data also flows from assignments to asignees (Check word??)
%   interestingly enough, also through casts
% TODO for the purpose of dataflow analysis, all numeric types are considered the same,
% so all numeric conversions are assumed to prevent dataflow
% TODO find out when we use call contexts to help Virutal Dispatch
% Question: compatibleTypes in DataFlowPrivate is always already called with erased representations,
% and then calls getErasedRepr again?
% TODO: local data flow: We have flow from PostUpdateNodes to the next adjacent read,
% but we also have flow from the pre-update node to the next adjacent read?
% Isn't that (more or less) two paths for the same?

% flowthrough matches callsites with returns (flow summary)

% start of considered nodes: nodeCandFwd1(node, config) -  simple local flow 
% node is reachable via dataflow from the source,
% including local flow, field stores, reads, flows into callables via parameters
% and flow out of callables via modified parameters (takes VD into account)
% doesn't take call contexts into account

% then nodeCand1 filters nodeCandFwd1 by just keeping all nodes which are
% reachable from a sink (through the same steps as before)

% nodeCand2: very simple call contexts, TODO find out exact differences to Cand1
% nodeCand:=nodeCand2 excluding call contexts

% bigstep relation: localFlowBigStep

% flowCandFwd0 something something accesspathfront, starts again with flooding
% from the source with all nodeCand2's
% flowCand0 filters flowCandFwd from the sink backwards to the source

% flowCandFwd = flowCandFwd0 AND CastingNode compatible type check

% flowCand = flowCand0 AND flowCandFwd
% flowCand0 uses flowCand and flowCandFwd, so mutual recursion, goes backwards from sink

% TODO: Sections about casts


% what is a node?
% user interface